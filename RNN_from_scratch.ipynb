{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow [here](https://github.com/pangolulu/rnn-from-scratch) - RNN and [here](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow [here](http://nicodjimenez.github.io/2014/08/08/lstm.html) - LSTM and [here](https://github.com/nicodjimenez/lstm). And mostly [here](http://blog.varunajayasiri.com/numpy_lstm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending forward\n",
    "\n",
    "At each step in an RNN, the cells take in a state $S$, a weight matrix $W$ multiplied by $S$, an input $X$, an weight matrix $U$. The RNN layer has a certain \"breadth\", that determines both the second dimensions of these weight matrices. \n",
    "\n",
    "The input $X$ will have the dimensions of the number of words, num_words, and $U$ will be used to transform this into the hidden layer, so $U$ will have dimension (num_words, num_hidden).\n",
    "\n",
    "The state for a given layer will be a vector of its number of hidden units.\n",
    "\n",
    "Each layer will have a state from the prior time step. This state can just be initialized to be all zeros.\n",
    "\n",
    "This state is then multiplied by the weight matrix $W$. This weight matrix has dimension (hidden_dim, hidden_dim). \n",
    "\n",
    "These two vectors, $WS$ and $XU$ are then added. Then, they are multiplied by a third weight matrix $V$ to get the output state for the layer.\n",
    "\n",
    "This state $SV$ is then used in the next layer as the layer's output $O$.\n",
    "\n",
    "So - this is key - at each time step, each layer in the RNN returns a state $S$ and an output $O$ that is actually passed onto the next layer.\n",
    "\n",
    "In addition, it produces $V$, which has dimension (num_hidden, num_words), that produces an output softmax $O$ over the number of words. \n",
    "\n",
    "When figuring out how backpropogation works, we can assume that we get from the layer above how this state $S$ and this output $O$ are affecting the loss; from this, we'll need to figure out how to update the weights $V$, $U$, and $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once backpropagation is complete, each layer sends down to the layer below it a gradient for how much the prior state affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self, W, X):\n",
    "        return np.dot(X, W)\n",
    "\n",
    "    def backward(self, W, X, dZ):\n",
    "        dW = np.dot(np.transpose(X), dZ)\n",
    "        dX = np.dot(dZ, np.transpose(W))\n",
    "        return dW, dX\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, X, b):\n",
    "        return X + b\n",
    "\n",
    "    def backward(self, X, b, dZ):\n",
    "        dX = dZ * np.ones_like(X)\n",
    "        db = np.dot(np.ones((1, dZ.shape[0]), dtype=np.float64), dZ)\n",
    "        return db, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        # x = 10000 x 1\n",
    "        # prev_s = 100 x 1\n",
    "        # U = 10000 x 100\n",
    "        # W = 100 x 100\n",
    "        # V = 100 x 100\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "        \n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        # Each forward pass receives 4 things: \n",
    "        # previous state\n",
    "        # U\n",
    "        # V\n",
    "        # W\n",
    "        # In principle, each of these quantities affects the output *of this layer* in some way\n",
    "        # And each of these should then be updated by a function of how they affect the output\n",
    "        # of this layer and of the gradient this layer receives \n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        # V = 100 x 100\n",
    "        # sv = 100\n",
    "        # dsv = np.dot(dmulv, np.transpose(V))\n",
    "        # diff_s -> received from the layer above. Both of these are the same shape as S\n",
    "        ds = dsv + diff_s\n",
    "        # dadd ->  sending back through activation\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        # sending backward again...\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        # dW, \n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTMs, there's a \"concatenation\" step: the $X$ values, with dimensions of the number of words in the vocabulary, and the $S$ values, with dimensions the number of hidden values in the state, are concatenated together before being multiplied by a matrix. Note that this is equivalent to the two values being each multiplied by their own matrices and then the results being concatenated together: this does not \"weight\" the words more than the state, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with regular RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs look the same as regular RNNs from the outside: each cell outputs the results of its operations forward both to the next layer, and back to the current layer; this output (often denoted $h_t$) has dimension equal to the dimension of the LSTM layer.\n",
    "\n",
    "The difference is in how the weights are updated inside the layer. LSTM layers maintain a \"cell state\" that stores information in a persistent manner. As they receive both real world data and the output from the prior layer, they perform a series of operations that ultimately result in outputting a vector of hidden states, but do so by operating on the cell state. Let's take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a vocabulary of 10,000 words, and an LSTM layer with 100 nodes. What actually goes on in the forward pass?\n",
    "\n",
    "First: the input vector of length 10,000 is passed in, along with the cell hidden state of length 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_t$ and $h_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, these are concatenated into a vector of length 10,100, and multiplied together and fed through a sigmoid to create a forget gate $f_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_t = \\sigma(W_f * [X_t, h_{t-1}])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similarily, the input and new candidate cell states are created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$i_t = \\sigma(W_i * [X_t, h_{t-1}])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C^{new}_t = tanh(W_C * [X_t, h_{t-1}])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell state is updated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ C_t = f_t * C_{t-1} + i_t * C^{new}_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another output is created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o_t = \\sigma(W_o * [X_t, h_{t-1}])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is used to update the hidden state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = o_t * tanh(C_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all of these W matrices are 10,100 x 100. $C_t$ and all the other vectors are of dimension 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we are outputing a softmax layer over the words, we must output a layer of length 10,000. We'll call this $V_t$. If"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ V_t = W_V * h_t $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then $W_V$ has dimension 100 x 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now is: how does backpropagation work?\n",
    "\n",
    "To start: in backpropagation for LSTMs, we'll receive:\n",
    "\n",
    "* The gradient for the cell state $C$. \n",
    "* The gradient for the output $V$.\n",
    "* The gradient for the hidden state $h$.\n",
    "\n",
    "Using these, we can trace backwards through the LSTM and get the gradients for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update all the gradients in the following order:\n",
    "\n",
    "1. $V_{grad}$ (from $y$) \n",
    "1. $Wv_{grad}$ (from $V_{grad}$ and $h_t$)\n",
    "1. $h_{grad}$ (from $W_grad$ and $h_{grad}$)\n",
    "1. $o_{grad}$ (from $h_{grad}$ and $C_t$)\n",
    "1. $Wo_{grad}$ (from $o_{grad}$ and $Z_t$)\n",
    "1. $C_{grad}$ (from $h_{grad}$ and $C_t$ (input), and $o_t$)\n",
    "1. $C^{new}_{grad}$ (from $i$ and $C_{grad}$ and $C^{new}_t$)\n",
    "1. $Wc_{grad}$ (from $C^{new}_{grad}$ and $Z_t$)\n",
    "1. $i_{grad}$ (from $C_{grad}$ and $C^{new}_t$)\n",
    "1. $Wi_{grad}$ (from $i_{grad}$ and $Z_t$)\n",
    "1. $f_{grad}$ (from $C_{grad}$ and $C^{new}_t$)\n",
    "1. $Wf_{grad}$ (from $f_{grad}$ and $Z_t$)\n",
    "1. $Z_{grad}$ (from the four weight matrices, and $f_t$, $i_t$, $C^{new}_t$, $o_t$)\n",
    "1. $C^{in}_{grad}$ (from $f_t$ and $C_{grad}$)\n",
    "\n",
    "Goal: get to $C^{in}_{grad}$ and $h^{in}_{grad}$. Remember that $h^{in}$ is just a part of $Z$, which is multiplied by several inputs throughout the internals of the LSTM cell. So, if we get $Z_{grad}$, then we can extract $h_{grad}$ and $x_{grad}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "\n",
    "    # 1\n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    # 2\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    # 3\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "\n",
    "    # 4\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    \n",
    "    # 5\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    # 6\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "\n",
    "    # 7\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "\n",
    "    # 8\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    # 9\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    \n",
    "    # 10\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    # 11    \n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "\n",
    "    # 12\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    # 13\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    \n",
    "    # 14\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dx_prev = dz[H_size:, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev, dx_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "The backpropogation through time algorithm works the same way for RNNs as for LSTMs. \n",
    "\n",
    "The difference is that regular RNN cells compute and send backwards a hidden state over time.\n",
    "\n",
    "LSTM cells compute (and send backwards) both a hidden state and a cell state. \n",
    "\n",
    "In both of these, to do backpropagation properly, we must keep track of the hidden states and cell states over time. The length that we keep track of is equal to the length of the input sequences we feed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: for a given cell, a forward pass should take in\n",
    "\n",
    "* $X_t$ (of dimension e.g. 10,000) \n",
    "* $h_{t-1}$ (of dimension 100)\n",
    "* $C_{t-1}$ (of dimension 100)\n",
    "\n",
    "For the layer, it should output:\n",
    "\n",
    "* $y_t$, the prediction for that layer at time $t$ (of dimension 10,000).\n",
    "* $h_t$, the output for that layer at time $t$ (of dimension 100).\n",
    "* $C_t$, the cell state for that layer at time $t$ (of dimension 10,100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "\n",
    "Concatenate $X_t$ and $h_{t-1}$.\n",
    "\n",
    "`z = np.row_stack((h_prev, x))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "\n",
    "Compute $f_t = \\sigma(W_f * [X_t, h_{t-1}])$.\n",
    "\n",
    "`f = sigmoid(np.dot(p.W_f.v, z) + p.b_f)`\n",
    "\n",
    "Note: $W_f$ has dimension 10,100 x 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "\n",
    "Compute $i_t = \\sigma(W_i * [X_t, h_{t-1}])$.\n",
    "\n",
    "`i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)`\n",
    "\n",
    "Note: $W_i$ has dimension 10,100 x 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:\n",
    "\n",
    "Compute $\\tilde{C_t} = tanh(W_c * [X_t, h_{t-1}])$\n",
    "\n",
    "`C_tilde = tanh(np.dot(p.W_C.v, z) + p.b_C.v)`\n",
    "\n",
    "Note: $W_c$ has dimension 10,100 x 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "\n",
    "Compute $ C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} $\n",
    "\n",
    "`C = f * C_prev + i * C_tilde`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "\n",
    "Compute $o_t = \\sigma(W_o * [X_t, h_{t-1}])$\n",
    "\n",
    "`o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)`\n",
    "\n",
    "Note: $W_o$ has dimension 10,100 x 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "\n",
    "Compute $h_t = o_t * tanh(C_t)$\n",
    "\n",
    "`h = o * tanh(C)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8:\n",
    "\n",
    "Compute $ V_t = W_V * h_t $ \n",
    "\n",
    "`v = np.dot(p.W_v.v, h) + p.b_v.v`\n",
    "\n",
    "Note: $W_v$ has dimension 100 x 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9:\n",
    "\n",
    "Compute $ Y_t = softmax(V_t) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    \n",
    "    # Step 1\n",
    "    z = np.row_stack((h_prev, x))\n",
    "    \n",
    "    # Step 2\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    \n",
    "    # Step 3\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    \n",
    "    # Step 4\n",
    "    C_tilde = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    # Step 5 \n",
    "    C = f * C_prev + i * C_tilde\n",
    "\n",
    "    # Step 6\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "\n",
    "    # Step 7\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    # Step 8\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "\n",
    "    # Step 9\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each LSTM layer on the backward pass, at each time step, will take in:\n",
    "\n",
    "The gradients for\n",
    "\n",
    "* $y_t$ (of dimension e.g. 10,000) \n",
    "* $h_t$ (of dimension 100)\n",
    "* $C_t$ (of dimension 100)\n",
    "\n",
    "That is, the partial derivatives of these with respect to the loss.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{Y}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{H}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{C}}$$\n",
    "\n",
    "Note: for the last character in the sequence, these last two have to be initialized to 0.\n",
    "\n",
    "For the layer, it should output the gradients for\n",
    "\n",
    "* $x_t$, the prediction for that layer at time $t$ (of dimension 10,000).\n",
    "* $h_{t_1}$, the output for that layer at time $t$ (of dimension 100).\n",
    "* $C_{t-1}$, the cell state for that layer at time $t$ (of dimension 10,100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target` is a one hot encoded vector of output.\n",
    "`y` is the predicted output.\n",
    "\n",
    "The loss is a function of `y-target`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = softmax(v) $.\n",
    "\n",
    "A softmax layer has no gradient, so:\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{V}} = -(y -  y^{hat})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in our basic neural net, when we did $ W * B = C $, and we were calculating $ \\frac{\\partial{L}}{\\partial{W}} $ and $ \\frac{\\partial{L}}{\\partial{B}} $, using $ \\frac{\\partial{L}}{\\partial{C}} $, it turned out that:\n",
    "\n",
    "$$ \\frac{\\partial{C}}{\\partial{W}} = B^T $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial{C}}{\\partial{B}} = W^T $$\n",
    "\n",
    "So that, for example, because of the chain rule,\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{C}} * \\frac{\\partial{C}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{C}} * B^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, since $ V_t = W_v * h_t $, \n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{V}} * H^T $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{H}} = \\frac{\\partial{L}}{\\partial{V}} * W_v^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the current value of $ \\frac{\\partial{L}}{\\partial{H}} $, we add the gradient from the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{L}}{\\partial{H}} = \\frac{\\partial{L}}{\\partial{H}} + H_{grad} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Next we want to compute\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{O}} $$\n",
    "\n",
    "We have both $h_t = o_t * tanh(C_t)$\n",
    "\n",
    "and by the logic presented for step 2, we have:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{O}} = \\frac{\\partial{L}}{\\partial{H}} * tanh(C_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5 \n",
    "\n",
    "Call $ W_o * [X_t, h_{t-1}] = p$. To continue \"moving down the chain\", we want to compute\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{P}} $$ \n",
    "\n",
    "We know that since $ o_t = \\sigma(p) $, so the derivative of this function is $ \\sigma(p) * (1 - \\sigma(p)) $. Thus\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{P}} = \\frac{\\partial{L}}{\\partial{O}} * \\sigma(p) * (1 - \\sigma(p)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Similarly to step 2, we have\n",
    "\n",
    "$ W_o * [X_t, h_{t-1}] = p $\n",
    "\n",
    "We'll see this quantity $[X_t, h_{t-1}]$ quite a bit, so we'll call it $z_t$.\n",
    "\n",
    "Since \n",
    "\n",
    "$ W_o * z_t = p $\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{z_t}} = \\frac{\\partial{L}}{\\partial{P}} * W_o^T $$\n",
    "\n",
    "(though as we shall see, this is just one component of $\\frac{\\partial{L}}{\\partial{z_t}}$) - and also:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_o}} = \\frac{\\partial{L}}{\\partial{P}} * z^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "\n",
    "Next we want to compute $$ \\frac{\\partial{L}}{\\partial{C_t}} $$, and we have \n",
    "\n",
    "$h_t = o_t * tanh(C_t)$\n",
    "\n",
    "First, we receive $C_{grad}$ from the layer above, so we can start by initializing\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_t}} = C_{grad} $$\n",
    "\n",
    "First, set $ tanh(C_t) = D_t $. Then, using exactly the same logic as in Step 2:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{D_t}} = \\frac{\\partial{L}}{\\partial{H_t}} * o_t $$.\n",
    "\n",
    "Then, since $ D_t = tanh(C_t) $, the derivative of this will be the derivative of the `tanh` function evaluated at $tanh(C_t)$. So, applying the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_t}} = \\frac{\\partial{L}}{\\partial{D_t}} * \\frac{\\partial{D_t}}{\\partial{C_t}} = \\frac{\\partial{L}}{\\partial{H_t}} * o_t * tanh'(tanh(C_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Next we want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{C_t}}} $$\n",
    "\n",
    "Since $C_t = \\tilde{C_t} * i_t$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$ \\frac{\\partial{L}}{\\partial{\\tilde{C_t}}} = \\frac{\\partial{L}}{\\partial{C}} * i_t $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.5\n",
    "\n",
    "Next we want to ultimately compute $$ \\frac{\\partial{L}}{\\partial{W_c}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ E_t = W_c * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{E_t}} $.\n",
    "\n",
    "We have $ \\tilde{C_t} = tanh(E_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{E_t}} = \\frac{\\partial{L}}{\\partial{C_t}} * tanh'(tanh(E_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "Next, we'll compute $$ \\frac{\\partial{L}}{\\partial{W_c}} $$. Similarly to Step 5, since:\n",
    "\n",
    "$$ E_t = W_c * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_c}} = \\frac{\\partial{L}}{\\partial{E_t}} * z^T $$\n",
    "\n",
    "(multiplying by the transpose of $z$ since these are again matrix multiplications), and \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z}} = \\frac{\\partial{L}}{\\partial{E_t}} * W_c^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "\n",
    "Step 9 follows Step 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{i_t}}} $$\n",
    "\n",
    "Since $C_t = \\tilde{C_t} * i_t$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$ \\frac{\\partial{L}}{\\partial{\\tilde{i_t}}} = \\frac{\\partial{L}}{\\partial{C}} * \\tilde{C_t} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9.5\n",
    "\n",
    "Similarly, step 9.5 follows Step 7.5. We ultimately want to compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ J_t = W_i * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{J_t}} $.\n",
    "\n",
    "We have $i_t  = \\sigma(J_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{J_t}} = \\frac{\\partial{L}}{\\partial{i_t}} * \\sigma(J_t) * (1-\\sigma(J_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10\n",
    "\n",
    "Step 10 is identical to Step 8: we can now compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "Similarly to Step 5, since:\n",
    "\n",
    "$$ J_t = W_i * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_i}} = \\frac{\\partial{L}}{\\partial{J_t}} * z^T $$\n",
    "\n",
    "and, adding to rolling sum of $ \\frac{\\partial{L}}{\\partial{Z}} $, \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{J_t}} * W_i^t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11\n",
    "\n",
    "Step 11 follows Step 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{f_t}}} $$\n",
    "\n",
    "Since $ C_t = \\tilde{f_t} * C_{prev}$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{f_t}} = \\frac{\\partial{L}}{\\partial{C_t}} * C_{prev} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11.5\n",
    "\n",
    "Similarly, step 11.5 follows step 7.5. We want to ultimately compute $$ \\frac{\\partial{L}}{\\partial{W_f}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ G_t = W_f * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{G_t}} $.\n",
    "\n",
    "We have $ f_t = \\sigma(G_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{G_t}} = \\frac{\\partial{L}}{\\partial{f_t}} * \\sigma(G_t) * (1-\\sigma(G_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12\n",
    "\n",
    "Step 12 is identical to Steps 8 and 10: we can now compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "\n",
    "Similarly to Step 5, since:\n",
    "\n",
    "$$ G_t = W_f * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_f}} = \\frac{\\partial{L}}{\\partial{G_t}} * z^T $$\n",
    "\n",
    "and, adding to rolling sum of $ \\frac{\\partial{L}}{\\partial{Z_t}} $, \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{G_t}} * W_f^t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13\n",
    "\n",
    "Step 13 follows steps 11 and 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{C_{prev}}} $$\n",
    "\n",
    "Since $ C_t = f_t * C_{prev}$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_{prev}}} = \\frac{\\partial{L}}{\\partial{C_t}} * f_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14\n",
    "\n",
    "Luckily, this isn't really a step, since we've alread computed the four components of \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{G_t}} * W_f^t + \\frac{\\partial{L}}{\\partial{J_t}} * W_i^t + \\frac{\\partial{L}}{\\partial{E_t}} * W_c^T + \\frac{\\partial{L}}{\\partial{P}} * W_o^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15\n",
    "\n",
    "On the forward pass, $Z_t$ was simply made up of $X_t$ and $h_{t-1}$ concatenated. So, we can select the appropriate elements of $ \\frac{\\partial{L}}{\\partial{Z_t}} $ to make up $ \\frac{\\partial{L}}{\\partial{X_t}} $ and $ \\frac{\\partial{L}}{\\partial{h_{t-1}}} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(Y_grad, self.H.T)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(LSTM_params.W_V_diff.T, Y_grad)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.O_int_diff, self.Z.T)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.C_prop_int_diff, self.Z.T)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.I_int_diff, self.Z.T)\n",
    "        self.Z_diff += np.dot(self.I_int, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.F_int, self.Z.T)        \n",
    "        self.Z_diff += np.dot(self.F_int, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z[:self.vocab_size, :]\n",
    "        H_grad = self.Z[self.vocab_size:, :]        \n",
    "\n",
    "        return H_grad, C_grad, X_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "\n",
    "    # 1\n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    # 2\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    # 3\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "\n",
    "    # 4\n",
    "    do = dh * tanh(C)\n",
    "    \n",
    "    # 4.5\n",
    "    dPt = dsigmoid(o) * do\n",
    "    \n",
    "    # 5\n",
    "    p.W_o.d += np.dot(dPt, z.T)\n",
    "    p.b_o.d += dPt\n",
    "\n",
    "    # 6\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "\n",
    "    # 7\n",
    "    dC_bar = dC * i\n",
    "    \n",
    "    # 7.5\n",
    "    dEt = dtanh(C_bar) * dC_bar\n",
    "\n",
    "    # 8\n",
    "    p.W_C.d += np.dot(dEt, z.T)\n",
    "    p.b_C.d += dEt\n",
    "\n",
    "    # 9\n",
    "    di = dC * C_bar\n",
    "    \n",
    "    # 9.5\n",
    "    dJt = dsigmoid(i) * di\n",
    "    \n",
    "    # 10\n",
    "    p.W_i.d += np.dot(dJt, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    # 11\n",
    "    df = dC * C_prev\n",
    "    \n",
    "    # 11.5\n",
    "    dGt = dsigmoid(f) * df\n",
    "\n",
    "    # 12\n",
    "    p.W_f.d += np.dot(dGt, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    # 13\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    # 14\n",
    "    dz = (np.dot(p.W_f.v.T, dGt)\n",
    "         + np.dot(p.W_i.v.T, dJt)\n",
    "         + np.dot(p.W_C.v.T, dEt)\n",
    "         + np.dot(p.W_o.v.T, d))\n",
    "    \n",
    "    # 15\n",
    "    X_ = dz[:self.vocab_size, :]\n",
    "    dx_prev = dz[:self.vocab_size, :]\n",
    "    \n",
    "    return dh_prev, dC_prev, dx_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a series of text. We'll break it into 15 character sequences, and learn to predict the next character in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do CNNs and RNNs from scratch - I have the code done for both.\n",
    "\n",
    "End goal:\n",
    "\n",
    "## CNNs:\n",
    "\n",
    "* Explanation of forward propagation, backpropagation with low level code, visuals, and math where necessary.\n",
    "* Need to fit this in with a whole neural net architecture. \n",
    "\n",
    "## RNNs:\n",
    "\n",
    "* Add visuals to explanation of forward, backward propagation.\n",
    "* Hardest part (part I'm avoiding): fitting in this RNN explanation into a neural net framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can modify this to be about LSTMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "        \n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff_s\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we would have `class LSTMLayer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a function that runs one training example (or batch of examples) through the network.\n",
    "\n",
    "Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    assert len(x) == len(y)\n",
    "    output = Softmax()\n",
    "    layers = self.forward_propagation(x)\n",
    "    dU = np.zeros(self.U.shape)\n",
    "    dV = np.zeros(self.V.shape)\n",
    "    dW = np.zeros(self.W.shape)\n",
    "\n",
    "    T = len(layers)\n",
    "    prev_s_t = np.zeros(self.hidden_dim)\n",
    "    diff_s = np.zeros(self.hidden_dim)\n",
    "    for t in range(0, T):\n",
    "        dmulv = output.diff(layers[t].mulv, y[t])\n",
    "        input = np.zeros(self.word_dim)\n",
    "        input[x[t]] = 1\n",
    "        dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "        prev_s_t = layers[t].s\n",
    "        dmulv = np.zeros(self.word_dim)\n",
    "        for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[x[i]] = 1\n",
    "            prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "            dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "            dU_t += dU_i\n",
    "            dW_t += dW_i\n",
    "        dV += dV_t\n",
    "        dU += dU_t\n",
    "        dW += dW_t\n",
    "    return (dU, dW, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(self, x, y, learning_rate):\n",
    "    dU, dW, dV = self.bptt(x, y)\n",
    "    self.U -= learning_rate * dU\n",
    "    self.V -= learning_rate * dV\n",
    "    self.W -= learning_rate * dW\n",
    "   \n",
    "def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    num_examples_seen = 0\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = self.calculate_total_loss(X, Y)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(Y)):\n",
    "            self.sgd_step(X[i], Y[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here: `sgd_step` does two things: \n",
    "\n",
    "1. Runs data through the network\n",
    "2. Updates the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to copy the `rnn-from-scratch` repo [here](https://github.com/pangolulu/rnn-from-scratch), but modify it to work with LSTMs following [here](http://blog.varunajayasiri.com/numpy_lstm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want a structure that defines a series of operations and knows how to send values through the structure appropriately. \n",
    "\n",
    "What would that look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# will have a number of nodes equal to the length of the input sequence\n",
    "class LSTM_Node:\n",
    "    ...\n",
    "    def __init__(self, x_dim, h_dim)\n",
    "    # store state as dictionary\n",
    "    # initialize params object\n",
    "\n",
    "    def forward(self, x_input, h_input, c_input):\n",
    "        ...\n",
    "        return x_out, h_out, c_out\n",
    "    \n",
    "    def forward(self, x_input, h_input, c_input):\n",
    "        ...\n",
    "        return x_out, h_out, c_out        \n",
    "```\n",
    "\n",
    "```python\n",
    "# store all the params\n",
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        # initialize params, diffs to correct shapes\n",
    "        \n",
    "\n",
    "    def update_params(self):\n",
    "\n",
    "            \n",
    "```\n",
    "\n",
    "```python\n",
    "class LSTM_Model:\n",
    "    def __init__(self, sequence_length):\n",
    "        # Initialize sequence of LSTM nodes\n",
    "        # Learning rate etc.\n",
    "        \n",
    "    def forward_pass(self, x_batch):\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        for node in nodes:\n",
    "            x_in, h_in, c_in = node.forward(x_in, h_in, c_in)\n",
    "            # Nodes are storing all the \"h\" and \"c\" information\n",
    "        return x_out\n",
    "    \n",
    "    def loss(self, prediction, y_batch):\n",
    "        return (prediction - y_batch) ** 2\n",
    "\n",
    "    def loss_gradient(self, prediction, y_batch):\n",
    "        y_batch = np.zeros_like(prediction)\n",
    "        y_batch = 2 * (prediction - y_batch)\n",
    "        return y_batch\n",
    "                \n",
    "    def backward_pass(self, loss_diff):\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        for node in nodes:\n",
    "            loss_diff, h_diff, c_diff = node.forward(loss_diff, h_diff, c_diff)\n",
    "            # Nodes are storing all the \"h\" and \"c\" information       \n",
    "        \n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "Key part:\n",
    "\n",
    "\n",
    "x_input = seq (len = 15)\n",
    "y_output = seq (len = 15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: \n",
    "\n",
    "Same \"forward-loss-backward\" API as before!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "list(range(10, 5, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - y * y\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(num_hidden, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_H = np.zeros_like(hidden_size)\n",
    "        self.start_C = np.zeros_like(hidden_size)\n",
    "        self.params = LSTM_Params(vocab_size, hidden_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x_batch, first_iter=False):\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        batch_size = x_batch.size[0]\n",
    "        x_out = x_batch\n",
    "        if first_iter:\n",
    "            h_in = np.zeros_like(num_hidden)\n",
    "            c_in = np.zeros_like(num_hidden)\n",
    "        else:\n",
    "            h_in = self.nodes[0].H\n",
    "            c_in = self.nodes[0].C            \n",
    "        for node in self.nodes:\n",
    "            x_out, h_in, c_in = node.forward(x_out, h_in, c_in, self.params)\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        loss = (y_batch - prediction) ** 2\n",
    "        y_grad = -1.0 * (y_batch - prediction)\n",
    "        return y_grad\n",
    "            \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        Implements the \"Backpropagation Through Time\" algorithm.\n",
    "        For each step in the sequence T moving backwards, backpropagate through some number\n",
    "        K nodes\n",
    "        \n",
    "        '''\n",
    "        # Initialize h_grad and c_grad\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        H_grad = np.zeros_like(self.hidden_size)\n",
    "        C_grad = np.zeros_like(self.hidden_size)\n",
    "        T = self.sequence_length - 1\n",
    "        K = 5 # BPTT length\n",
    "        \n",
    "        # BPTT\n",
    "        num_iterations = T - K\n",
    "        for n in range(num_iterations):\n",
    "            for t in range(T-n, T-K-n, -1):\n",
    "                Y_grad, H_grad, C_grad = \\\n",
    "                    self.nodes[t-n].backward(self, Y_grad, H_grad, C_grad, LSTM_params)\n",
    "        \n",
    "#         self.start_H = H_grad\n",
    "#         self.start_C = C_grad\n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x, y, learning_rate):\n",
    "        prediction = self.forward(x, first_iter=True)\n",
    "        loss, loss_gradient = self.loss(x, y)\n",
    "        self.backward(x, y)\n",
    "        self.params.update_params(self.learning_rate)\n",
    "        return\n",
    "\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, epochs):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, num_hidden, vocab_size):\n",
    "        self.state = LSTM_State()\n",
    "        \n",
    "    def forward(self, X_input, H_input, C_input, LSTM_params):\n",
    "\n",
    "        self.Z = np.row_stack((X_input, H_input))\n",
    "\n",
    "        # Step 2\n",
    "        self.F_int = np.dot(self.Z, LSTM_params.W_f)\n",
    "        self.F = sigmoid(self.F_int)\n",
    "\n",
    "        # Step 3\n",
    "        self.I_int = np.dot(self.Z, LSTM_params.W_i)\n",
    "        self.I = sigmoid(self.I_int)\n",
    "\n",
    "        # Step 4\n",
    "        self.C_prop_int = np.dot(self.Z, LSTM_params.W_c)\n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "\n",
    "        # Step 5 \n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "        self.C_out = self.F * C_input + self.I * self.C_prop\n",
    "\n",
    "        # Step 6\n",
    "        self.O_int = np.dot(self.Z, LSTM_params.W_O)\n",
    "        self.O = sigmoid(self.O_int)\n",
    "\n",
    "        # Step 7\n",
    "        self.C_act = tanh(self.C_out)\n",
    "        self.H_out = self.O * self.C_act\n",
    "\n",
    "        # Step 8\n",
    "        self.X_out = np.dot(self.H, LSTM_params.W_v)\n",
    "\n",
    "        return self.X_out, self.H_out, self.C_out\n",
    "\n",
    "\n",
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(Y_grad, self.H.T)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(LSTM_params.W_V_diff.T, Y_grad)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.O_int_diff, self.Z.T)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.C_prop_int_diff, self.Z.T)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.I_int_diff, self.Z.T)\n",
    "        self.Z_diff += np.dot(self.I_int, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.F_int, self.Z.T)        \n",
    "        self.Z_diff += np.dot(self.F_int, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z[:self.vocab_size, :]\n",
    "        H_grad = self.Z[self.vocab_size:, :]        \n",
    "\n",
    "        return H_grad, C_grad, X_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_F = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_I = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_C = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_O = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_V = np.random.normal(size=(self.hidden_size, self.vocab_size))\n",
    "        \n",
    "        self.W_F_diff = np.zeros_like(self.W_F)\n",
    "        self.W_I_diff = np.zeros_like(self.W_I)\n",
    "        self.W_C_diff = np.zeros_like(self.W_C)\n",
    "        self.W_O_diff = np.zeros_like(self.W_O)\n",
    "        self.W_V_diff = np.zeros_like(self.W_V)\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.W_F -= learning_rate * self.W_F_diff\n",
    "        self.W_I -= learning_rate * self.W_I_diff\n",
    "        self.W_C -= learning_rate * self.W_C_diff\n",
    "        self.W_O -= learning_rate * self.W_O_diff\n",
    "        self.W_V -= learning_rate * self.W_V_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

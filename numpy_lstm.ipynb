{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "#         import pdb; pdb.set_trace()\n",
    "        # The 0 included only because y_s is 2 dimensional (since we are using batch size 1)\n",
    "\n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dc from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115868c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " st weaved in your majesty happ\n",
      "Will deathing souler likema,\n",
      "But nem hath the rinith\n",
      "ar avokerd:\n",
      "Unleminged and soul this by took I\n",
      "stay, Thereking our moussipy, since be bear of And us seel thou heart \n",
      "----\n",
      "iter 230386, loss 31.864526\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "            \n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "            \n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient of each paramter matrix/vector at `num_checks` individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

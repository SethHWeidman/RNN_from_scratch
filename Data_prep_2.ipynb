{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ([char_to_idx[ch] \n",
    "           for ch in data[pointer: pointer + seq_len]])\n",
    "targets = ([char_to_idx[ch] \n",
    "            for ch in data[pointer + 1: pointer + seq_len + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_model_input(sequence, vocab_size):\n",
    "    x = {}\n",
    "    for t in range(len(sequence)):\n",
    "        x[t] = np.zeros((1, vocab_size))\n",
    "        x[t][0, inputs[t]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sequence_to_model_input(inputs, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will output a 62 x 1 output. We _want_ it to output 0s for everything, but 1s for the correct target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    \n",
    "    exp = np.exp(shiftx)\n",
    "    \n",
    "    sum_exp = np.sum(np.exp(shiftx), axis=1)\n",
    "    new_matrix = exp / sum_exp[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1122822b0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112110ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot([-np.log(1-x) for x in np.arange(0, 1, 0.01)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.11111111,  1.25      ,  1.42857143, -5.        ,  1.11111111],\n",
       "       [ 1.11111111,  1.25      , -3.33333333,  1.25      ,  1.11111111]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[0,0,0,1,0], [0,0,1,0,0]])\n",
    "pred = np.array([[0.1,0.2,0.3,0.2,0.1], [0.1,0.2,0.3,0.2,0.1]])\n",
    "\n",
    "cross_entropy_deriv(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(prediction, y):\n",
    "    return 0.5 * (prediction - y) ** 2\n",
    "\n",
    "def cross_entropy(prediction, y):\n",
    "    return np.array([-yi * np.log(predi) - (1-yi) * np.log(1-predi) for yi, predi in zip(y, prediction)])\n",
    "\n",
    "def cross_entropy_deriv(prediction, y):\n",
    "    return np.array([-yi / predi + (1-yi) / (1-predi) for yi, predi in zip(y, prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: break batches up into individual words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[]])\n",
    "b = np.array([[1,2]])\n",
    "np.append(a, b)\n",
    "# c = np.array([3,4])\n",
    "# a.append(b)\n",
    "# a.append(c)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_H = np.zeros(hidden_size)\n",
    "        self.start_C = np.zeros(hidden_size)\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch, first_iter=False):\n",
    "\n",
    "        batch_size = x_batch.shape[0]\n",
    "        if first_iter:\n",
    "            h_in = np.zeros((1, self.hidden_size))\n",
    "            c_in = np.zeros((1, self.hidden_size))\n",
    "        else:\n",
    "            h_in = self.nodes[0].H_out\n",
    "            c_in = self.nodes[0].C_out\n",
    "        x_batch_out = np.array([[]])\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            x_in = np.array(x_batch[i], ndmin=2)\n",
    "\n",
    "            x_out, h_in, c_in = node.forward(x_in, h_in, c_in, self.params)\n",
    "\n",
    "            if x_batch_out.shape[1] == 0:\n",
    "                x_batch_out = np.append(x_batch_out, x_out, axis=1)\n",
    "            else:\n",
    "                x_batch_out = np.append(x_batch_out, x_out, axis=0)\n",
    "                \n",
    "        return x_batch_out\n",
    "    \n",
    "    def loss(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return cross_entropy(prediction, y_batch)\n",
    "\n",
    "    \n",
    "    def loss_gradient(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return cross_entropy_deriv(prediction, y_batch)\n",
    "            \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        Implements the \"Backpropagation Through Time\" algorithm.\n",
    "        For each step in the sequence T moving backwards, backpropagate through some number\n",
    "        K nodes\n",
    "        \n",
    "        '''\n",
    "        # Initialize h_grad and c_grad\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "#         batch_size = loss_grad.shape[0]\n",
    "        \n",
    "        H_grad = np.zeros((1, self.hidden_size))\n",
    "        C_grad = np.zeros((1, self.hidden_size))\n",
    "        Y_grad = loss_grad\n",
    "        \n",
    "        T = self.sequence_length - 1\n",
    "        K = 20 # BPTT length\n",
    "        \n",
    "        # BPTT\n",
    "        num_iterations = T - K\n",
    "\n",
    "#         for n in range(num_iterations):\n",
    "        for t in range(T, T-K, -1):\n",
    "#             import pdb; pdb.set_trace()\n",
    "            Y_grad_in = np.array(Y_grad[T-t], ndmin=2)\n",
    "            Y_grad_out, H_grad, C_grad = \\\n",
    "                self.nodes[T-t].backward(Y_grad_in, H_grad, C_grad, self.params)\n",
    "                    \n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x_seq, y_seq, first_iter):\n",
    "        prediction = self.forward(x_seq, first_iter=first_iter)\n",
    "        p_softmax = softmax(prediction)\n",
    "        loss_gradient = self.loss_gradient(p_softmax, y_seq)\n",
    "\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.backward(loss_gradient)\n",
    "        self.params.update_params(self.learning_rate)\n",
    "        self.params.clear_gradients()        \n",
    "\n",
    "        \n",
    "    def loss_batch(self, x, y):\n",
    "        prediction = self.forward(x, first_iter=False)\n",
    "        p_softmax = softmax(prediction)\n",
    "        return np.sum(self.loss(p_softmax, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, X_input, H_input, C_input, LSTM_params):\n",
    "\n",
    "        self.Z = np.column_stack((X_input, H_input))\n",
    "\n",
    "        # Step 2\n",
    "        self.F_int = np.dot(self.Z, LSTM_params.W_F)\n",
    "        self.F = sigmoid(self.F_int)\n",
    "\n",
    "        # Step 3\n",
    "        self.I_int = np.dot(self.Z, LSTM_params.W_I)\n",
    "        self.I = sigmoid(self.I_int)\n",
    "\n",
    "        # Step 4\n",
    "        self.C_prop_int = np.dot(self.Z, LSTM_params.W_C)\n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "\n",
    "        # Step 5 \n",
    "        self.C_out = self.F * C_input + self.I * self.C_prop\n",
    "\n",
    "        # Step 6\n",
    "        self.O_int = np.dot(self.Z, LSTM_params.W_O)\n",
    "        self.O = sigmoid(self.O_int)\n",
    "\n",
    "        # Step 7\n",
    "        self.C_act = tanh(self.C_out)\n",
    "        self.H_out = self.O * self.C_act\n",
    "\n",
    "        # Step 8\n",
    "        self.X_out = np.dot(self.H_out, LSTM_params.W_V)\n",
    "\n",
    "        return self.X_out, self.H_out, self.C_out\n",
    "\n",
    "\n",
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(self.H_out.T, Y_grad)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(Y_grad, LSTM_params.W_V_diff.T)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.Z.T, self.O_int_diff)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(self.C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.Z.T, self.C_prop_int_diff)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.Z.T, self.I_int_diff)\n",
    "        self.Z_diff += np.dot(self.I_int_diff, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C_out\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.Z.T, self.F_int_diff)        \n",
    "        self.Z_diff += np.dot(self.F_int_diff, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z_diff[:, :self.vocab_size]\n",
    "        H_grad = self.Z_diff[:, self.vocab_size:,]        \n",
    "\n",
    "        return X_grad, H_grad, C_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "np.clip(a, -0.5, 0.5, out=np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_F = np.random.normal(size=(self.stack_size, self.hidden_size), loc=0, scale=0.1)\n",
    "        self.W_I = np.random.normal(size=(self.stack_size, self.hidden_size), loc=0, scale=0.1)\n",
    "        self.W_C = np.random.normal(size=(self.stack_size, self.hidden_size), loc=0, scale=0.1)\n",
    "        self.W_O = np.random.normal(size=(self.stack_size, self.hidden_size), loc=0, scale=0.1)\n",
    "        self.W_V = np.random.normal(size=(self.hidden_size, self.vocab_size), loc=0, scale=0.1)\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        self.W_F\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        self.W_F_diff = np.clip(self.W_F_diff, -1, 1)\n",
    "        self.W_I_diff = np.clip(self.W_I_diff, -1, 1)\n",
    "        self.W_C_diff = np.clip(self.W_C_diff, -1, 1)\n",
    "        self.W_O_diff = np.clip(self.W_O_diff, -1, 1)\n",
    "        self.W_V_diff = np.clip(self.W_V_diff, -1, 1)        \n",
    "\n",
    "        \n",
    "    def update_params(self, learning_rate):\n",
    "        self.clip_gradients()\n",
    "        self.W_F -= learning_rate * self.W_F_diff\n",
    "        self.W_I -= learning_rate * self.W_I_diff\n",
    "        self.W_C -= learning_rate * self.W_C_diff\n",
    "        self.W_O -= learning_rate * self.W_O_diff\n",
    "        self.W_V -= learning_rate * self.W_V_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - x * x\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    \n",
    "    def __init__(self, text_data, model):\n",
    "        self.data = text_data\n",
    "        self.model = model\n",
    "        self.chars = list(set(data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        \n",
    "        \n",
    "    def generate_sequences(self, start_pos, seq_length):\n",
    "        input_sequence = ([self.char_to_idx[ch] \n",
    "                           for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        target_sequence = ([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos+1:start_pos + seq_length+1]])\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "    def sequence_to_model_input(self, sequence, vocab_size):\n",
    "        out_batch = np.zeros((len(sequence), vocab_size))\n",
    "        for i, el in enumerate(sequence):\n",
    "            out_batch[i, el] = 1        \n",
    "        return out_batch\n",
    "    \n",
    "    def generate_batch(self, start_pos):\n",
    "        input_sequence, target_sequence = self.generate_sequences(start_pos, self.model.sequence_length)\n",
    "        return self.sequence_to_model_input(input_sequence, self.vocab_size), \\\n",
    "            self.sequence_to_model_input(target_sequence, self.vocab_size) \n",
    "    \n",
    "    def train(self, steps, check_every):\n",
    "        start_pos = 0\n",
    "        iterations = 0\n",
    "        while iterations < steps:\n",
    "            \n",
    "            x_batch, y_batch = self.generate_batch(start_pos)\n",
    "            first_iter = True if iterations == 0 else False\n",
    "            self.model.single_step(x_batch, y_batch, first_iter)\n",
    "            if iterations % check_every == 0:\n",
    "                print(\"Loss\", self.model.loss_batch(x_batch, y_batch))\n",
    "                print(iterations)\n",
    "            \n",
    "            start_pos += self.model.sequence_length\n",
    "            iterations += 1\n",
    "            if start_pos + self.model.sequence_length > len(data):\n",
    "                start_pos = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) // 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'invalid': 'raise', 'over': 'raise', 'under': 'raise'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 102.04147078591829\n",
      "0\n",
      "Loss 102.36886783682459\n",
      "1000\n",
      "Loss 102.35129719393359\n",
      "2000\n",
      "Loss 102.37662478089648\n",
      "3000\n",
      "Loss 102.3380308470633\n",
      "4000\n",
      "Loss 102.37473811781393\n",
      "5000\n",
      "Loss 102.0211819028246\n",
      "6000\n",
      "Loss 102.20791163702413\n",
      "7000\n",
      "Loss 102.39789625275345\n",
      "8000\n",
      "Loss 102.08138166870896\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "mod = LSTM_Model(sequence_length=20, vocab_size=62, hidden_size=100, learning_rate=0.01)\n",
    "character_generator = Character_generator(data, mod)\n",
    "character_generator.train(10000, check_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.10750961208381"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(character_generator.model.params.W_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement AdaGrad\n",
    "\n",
    "Investigate exploding values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget Stanford CS231: reproduce the results from the `numpy_LSTM` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, h_prev, C_prev, LSTM_Params):\n",
    "\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.v = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "#         self.y = np.exp(self.v) / np.sum(np.exp(self.v))\n",
    "        \n",
    "        return self.v, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "\n",
    "        assert self.z.shape == (1, self.vocab_size + self.hidden_size)\n",
    "        assert self.v.shape == (1, self.vocab_size)\n",
    "#         assert self.y.shape == (1, self.vocab_size)\n",
    "    \n",
    "#         for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "#             assert param.shape == (1, self.hidden_size)\n",
    "\n",
    "\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_batch_in):\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "\n",
    "        num_chars = x_batch_in.shape[0]\n",
    "        \n",
    "        x_batch_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "            x_in = np.array(x_batch_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_batch_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "#         print(\"x_batch_out\", type(x_batch_out))\n",
    "        return x_batch_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "            # Backward pass\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_softmax(input_array):\n",
    "    \n",
    "    a_exp = np.exp(input_array)\n",
    "    row_sums = a_exp.sum(axis=1)\n",
    "    new_matrix = a_exp / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, num_layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch_in):\n",
    "#         print(x_batch_in)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "\n",
    "            x_batch_in = layer.forward(x_batch_in)\n",
    "                \n",
    "        return x_batch_in\n",
    "\n",
    "    def _generate_one_hot_array(self, sequence):\n",
    "        '''\n",
    "        param sequence - sequence of indices of characters of length sequence_length.\n",
    "        return batch - numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.vocab_size))\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "\n",
    "        return np.array(batch)\n",
    "    \n",
    "    def loss(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        \n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        \n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "    \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            # Backward pass\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        \n",
    "        x_batch_in = self._generate_one_hot_array(inputs)\n",
    "        \n",
    "        x_batch_out = self.forward(x_batch_in)\n",
    "        \n",
    "        x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "        \n",
    "        loss = self.loss(x_softmax, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_softmax, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_in = self.model._generate_one_hot_array([input_char])\n",
    "            \n",
    "            x_batch_out = sample_model.forward(x_batch_in)\n",
    "        \n",
    "            x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, num_iterations):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs, targets = self._generate_inputs_targets(start_pos)\n",
    "            loss = self.model.single_step(inputs, targets)\n",
    "\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca16c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my haprend befole,\n",
      "Se am conged.\n",
      "\n",
      "DwErRE:\n",
      "Masle, an. Loun whak your at goole, thinge talr king thry, Iall am thee Seshertioo.\n",
      "\n",
      "DONINDTO:\n",
      "A all sam our my surpores to ky, a anony, Loedse sron,\n",
      "shes fo\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.01, sequence_length=25, num_layers=2)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.train(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

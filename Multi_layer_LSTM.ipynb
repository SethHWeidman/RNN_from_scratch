{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * ya\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, h_prev, C_prev, LSTM_Params):\n",
    "\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.v = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        self.y = np.exp(self.v) / np.sum(np.exp(self.v))\n",
    "        \n",
    "        return self.y, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "\n",
    "        assert self.z.shape == (1, self.vocab_size + self.hidden_size)\n",
    "        assert self.v.shape == (1, self.vocab_size)\n",
    "        assert self.y.shape == (1, self.vocab_size)\n",
    "    \n",
    "#         for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "#             assert param.shape == (1, self.hidden_size)\n",
    "\n",
    "\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_batch_in):\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_batch_in.shape[0]\n",
    "        \n",
    "        x_batch_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "            x_in = np.array(x_batch_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_batch_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_batch_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "            # Backward pass\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.num_layers = layers\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(self.num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        num_chars = len(inputs)\n",
    "        \n",
    "        x_batch_in = np.zeros((num_chars, self.vocab_size))\n",
    "        for t in range(num_chars):\n",
    "            x_batch_in[t, inputs[t]] = 1 # Input character\n",
    "        \n",
    "        for layer in self.layers:\n",
    "#             print(\"Forward through Layer\", i)\n",
    "            x_batch_in = layer.forward(x_batch_in)\n",
    "                \n",
    "        return x_batch_in\n",
    "\n",
    "    def loss(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "    \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            # Backward pass\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        x_batch_out = self.forward(inputs)\n",
    "        \n",
    "        loss = self.loss(x_batch_out, inputs, targets)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_batch_out, inputs, targets)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_out = sample_model.forward([input_char])\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_batch_out.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    def single_step(self):\n",
    "   \n",
    "        inputs, targets = self._generate_inputs_targets(self.start_pos)\n",
    "        \n",
    "        loss = self.model.single_step(inputs, targets)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_loop(self, num_iterations):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if self.start_pos + self.sequence_length > len(self.data):\n",
    "                self.start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            loss = self.single_step()\n",
    "            ##\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            self.start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            self.start_pos += self.sequence_length\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD0CAYAAABkZrYBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl4VNX9x/H3ZCOJhH1fgywHFEQWAZFNQdm0rbRaBa1LrUq1otYFMVRbUGht1YprVRREf1pRtIoIgoisyi4IHBAICILsENaQ5ffHLJk7SxIgyTCTz+t5eJ67nDv3XIXvnDn3nO9x5efnIyIisSEu0hUQEZGSo6AuIhJDFNRFRGKIgrqISAxRUBcRiSEJkbipMaYCcBGwA8iNRB1ERKJQPFAXWGytPRGqQESCOu6APjdC9xYRiXbdgXmhTkQqqO8AePvtt6lTp06EqiAiEl127tzJkCFDwBNDQ4lUUM8FqFOnDg0aNIhQFUREolbYbmu9KBURiSEK6iIiMURBXUQkhiioi4jEEAV1EZEYoqAuIhJDoi6oL87cR/rwqWTuORLpqoiInHWiLqh/uGwbAAs27o1wTUREzj5RF9TBBUCeVmwSEQlS5IxSY0w88CpggHzgTuA48KZnfzVwl7U2z++aFGASUAvIAm6y1u4uiQrHuWM6CukiIsGK01K/CsBaewmQATwBPA1kWGu74246/zLgmqHAKs/5iZ7rSqbCLndU19qqIiLBigzq1tqPgNs9u42BA0AHYI7n2DSgT8Bl3YDPCzl/2jwxnbw8BXURkUDF6lO31uYYYyYA44C3AZe11htVs4DKAZdUAg4Wcv60+VrqJfWBIiIxpNgvSq21NwEtcPevp/idSsPdevd3yHM83PnT5mupK6qLiAQpMqgbY240xjzi2T0K5AFLjDG9PMf6E7zgxXxgQCHnT5sL9amLiIRTnHzqHwJvGGO+BhKBe4G1wKvGmCTP9mQAY8wM4ErgJWCCMWYekA0MLqkKe1vqiukiIsGKDOrW2iPAtSFO9QxR9grPZjZwzZlVLbSj2TkAzN+4hz/0OLc0biEiErWibvLR2h1ZAHxVMsPeRURiStQFde/kIxERCRZ1QV1ERMKLuqCeEB91VRYRKTNRFyGv7dgQgJ4taka4JiIiZ5+oC+qpSfEApCTGR7gmIiJnn6gL6vGeN6U5mlIqIhIk6oK6V25eXtGFRETKmegN6mqoi4gEid6grpa6iEiQqA3qOWqqi4gEid6grhelIiJBojeo56r7RUQkUNQG9Vzl3hURCRK1QV196iIiwaIuqHsb6OpTFxEJFnVB3StPQV1EJEjUBvWTGqcuIhIkaoN6rvrURUSCRG1QV5+6iEgwBXURkRgSvUFdk49ERIIkFFXAGJMIjAfSgQrAaGAwUMdTJB1YZK29zu8aF7AN2OA5tNBa+0iJ1Rq11EVEQikyqAM3AHuttTcaY6oBK6y1jQCMMVWB2cB9Adc0BZZZa68q0dr6UVAXEQlWnKD+PjDZs+0CcvzO/RUYZ63dEXBNB6C+MWY2cAy4z1prz7Sybu5gnqugLiISpMg+dWvtYWttljEmDXdwzwAwxtQCegNvhrhsBzDGWnsp8CQwqcRq7KGgLiISrFgvSo0xDXF3s7xlrX3Hc/g3wDvW2twQlywBPgaw1s4D6nn62UVEpBQVGdSNMbWBGcDD1trxfqf6ANPCXPYYcK/n+rbAj9ZaNa1FREpZcfrURwBVgZHGmJGeY/0BA2zyL2iMmQFcCYwFJhljBuLug7+5pCosIiLhFRnUrbXDgGEhTp0fouwVns1sYOCZVU1ERE5V1E4+EhGRYArqIiIxREFdRCSGRF1Q19KkIiLhRV1QFxGR8BTURURiSFQHda1TKiLiFNVBXZkaRUScojqoK6mXiIhTVAf1bK1+JCLiENVB/fjJUAkiRUTKr6gO6kezFdRFRPxFXVD370U/mp0TtpyISHkUdUHd3zG11EVEHKI7qKtPXUTEIaqDuvrURUScojqoq/tFRMQpqoO6WuoiIk5RHdQPnzgZ6SqIiJxVojqo7z+qoC4i4i/qgrr/Ihn7j2RHriIiImehhKIKGGMSgfFAOlABGA38CHwKbPAUe8la+57fNSnAJKAWkAXcZK3dXZIVd7lg/1EFdRERf8Vpqd8A7LXWdgf6Ac8DHYCnrbW9PH/eC7hmKLDKc81EIKMkKw1QLTWJ/UfU/SIi4q/IljrwPjDZs+0CcnAHdWOM+SXu1vq91tosv2u6Af/wbE8DRpZMdQtUSU1kn1rqIiIORbbUrbWHrbVZxpg03ME9A/gWeNBa2wPYBDwWcFkl4KBnOwuoXHJVdqt2ThIHFNRFRByK9aLUGNMQmA28Za19B5hirV3qOT0FaBdwySEgzbOdBhwogbo6VElNYv/Rk+Tna6EMERGvIoO6MaY2MAN42Fo73nN4ujGmk2e7N7A04LL5wADPdn9gbgnU1aFaahK5efkcOq5MjSIiXsXpUx8BVAVGGmO8feP3A88YY04CO4HbAYwxM4ArgZeACcaYeUA2MLikK171nCTAPayxckpiSX+8iEhUKjKoW2uHAcNCnLokRNkrPJvZwDVnVrXCVTvHHcj3HjlBeo1zSvNWIiJRI+omH9WomERCnIvz67nfvW7bfyzCNRIROXsUp/vlrNKpSTWWZlxOhcQ4XC7I3HM00lUSETlrRF1Qd7lcVE51d73UqZTMlr1HIlwjEZGzR9R1v/hrXD2VLfvUUhcR8YruoF7tHLXURUT8RHdQr5HKnsPZHD6hseoiIhDlQT29unsoo1rrIiJuUR3Uq3smIB3UYhkiIkCUB/UKifEAnMjJi3BNRETODtEd1BPc1T+RowWoRUQgZoK6WuoiIhDtQd3b/XJSQV1EBKI9qKv7RUTEISaC+nG11EVEgCgP6qlJ7tQ1WZp8JCICRHlQj49zkZacwKFjGqcuIgJRHtQBqqQmagFqERGPqA/qlVMSOaiWuogIEANBvUpKEvuVJkBEBIiBoF6/SgpblVNdRASIgaDeok4a+45ksyvreKSrIiIScVEf1JvW9KbfVWtdRKTQNUqNMYnAeCAdqACMBrYC44Bc4ATwO2vtzwHXLQMOeXY3W2tvKdlqF2hQNRWADT8f5qL0aqV1GxGRqFDUwtM3AHuttTcaY6oBK4DNwJ+stSuMMXcADwP3ey8wxiQDLmttr1Kqs0ODqikALN+6n8GdG5XFLUVEzlpFBfX3gcmebReQA1xnrd3hd31gZ3ZbINUYM8NzfoS1dlEJ1TdIcmI8lVMS2XlIfeoiIoUGdWvtYQBjTBru4J7hDejGmK7A3UCPgMuOAv8EXgOaA9OMMcZaW2pz+atXTGLuhj2l9fEiIlGjyBelxpiGwGzgLWvtO55jvwVeBgZaa3cHXLIemGStzbfWrgf2AnVLttpO7RpWBeD4SWVrFJHyrdCgboypDcwAHrbWjvccuwF3C72XtXZTiMtuBf7lKVsPqATsCFGuxPQyNQHI1ALUIlLOFdWnPgKoCow0xowE4oHWwBbgQ2MMwBxr7WPGmIlABvA68KYxZh6QD9xaml0vAE1rVgRgzU+HaFmnUmneSkTkrFZUn/owYFhxPsha+zu/3cFnUqlTda5nrPqEhVsY1L5BWd5aROSsEvWTj8A9AgZg5Y8HIlwTEZHIiomgDlAp2f2jIz8/P8I1ERGJnJgJ6vdf3gKAPYeVW11Eyq+YCeqNq7v71dfsOFRESRGR2BVDQd2dA+bx/30f4ZqIiEROzAT1c2tWJC05gV2HjqtfXUTKrZgJ6gD39WnBkexcth84FumqiIhEREwF9RppFQDo9vfZEa6JiEhkxFRQ73d+nUhXQUQkomIqqCclFDxOTm5eBGsiIhIZMRXUAYb1bg7Acs0uFZFyKOaC+m86uHO//OfrUAkkRURiW8wF9YbV3OPVv1jzM6u3H4xwbUREylbMBXV/V46bF+kqiIiUqZgM6pvHDPBtL9ioZe5EpPyIyaDucrl824Nf/SaCNRERKVsxGdQB1o3q59vef0SZG0WkfIjZoJ6cGM9F6e4Fqa9/dVGEayMiUjZiNqgDPD+4PQDrdmaxT611ESkHilp4OqrVrpTs224/6guualuPzXsO89dfnE+HxtUiWDMRkdIR0y11gJu7pvu2P1n5E6u3H+L+/66MXIVEREpRkS11Y0wiMB5IByoAo4E1wJtAPrAauMtam+d3TQowCagFZAE3WWt3l3Ddi+XxX5zPt5v3OVZEOpqdG4mqiIiUuuK01G8A9lpruwP9gOeBp4EMzzEX8MuAa4YCqzznJwIZJVflU/fZsO50blLQ3bI76wS5eVpIQ0RiT3GC+vvASM+2C8gBOgBzPMemAX0CrukGfF7I+TI36bbOLM3o4wvuTUd8xpqftJ6piMSWIoO6tfawtTbLGJMGTMbd6nZZa71N3SygcsBllYCDhZwvc4nxcVSvWIFx17fzHRvw3FymrdoRwVqJiJSsYr0oNcY0BGYDb1lr3wH8k5WnAYF5bg95joc7HzG1KiXzx15NfftD317Gqm1K/CUisaHIoG6MqQ3MAB621o73HF5ujOnl2e4PzA24bD4woJDzEfVQv5YsH3m5b/+q5+cxeek2vrK7IlgrEZEzV5xx6iOAqsBIY4y3b30Y8JwxJglYi7tbBmPMDOBK4CVggjFmHpANDC7pip+pquckOfYfeN89zPGOHufyyIBWkaiSiMgZc+Xnl/0oEGNMOrB51qxZNGjQoMzv73U0O4cH3/+OqX796vWrpDB/+GVBZZdt3U/zWhVJS04syyqKiPhs27aN3r17AzSx1maGKhPzk48Kk5qUwAtD2juObT9wjDHT1jqOzba7GPTiAto8PoOv1+/W4hsictYq10Hda+5Dlzr2X5mziVvfXExeXj7pw6dyyxuLfed+N/5brhw3j+wcLWwtImcfBXXcS+Bljh1Ii9oVfce+XLeLc0d8Fvaa6/6zkJ0Hj5dF9UREik1B3c+bt3QKe+6R/i0d+8u2HqDLmFmkD5/K/3271XEu6/jJUqmfiEhRFNT91KuSQubYgbwU0M8OMKRL47DXPfLhKtZ6csukD59Km8dn8PGK7aVWTxGRcGI69e7pau7XDTPlj12ZtXYXFSsksPHJAeTm5WNGTiNw0FD/fzuH4k9btZPuzWtSLWDopIhIaVJLPYRmtdJ4/Krz+Oye7rRrVJUH+hoA4uNcJCXEOZKDhfP59ztpP+oLjp88/YyQR07k8Pq8zeTk6qWsiBSPgnoYN1/ShPPqVQp57tnftuOGLo14sK/hpovDd8sAfBaQW2bT7sPc9fYyjhWR/veZL9Zz/mPTGfXpGpo9Oo2DR9VPLyJFU/fLaahTOZnRv2rj2/9u+0GWby1Ib3NOUjxHPEE763iO7/iwd5fz8YqfALiqbT36ta4T9h7/nrXBsT/gubkhJ0WJiPhTS70EfHBnV5ZmFGQXXpJxObd1awLAY//7HoDV2w/6AjrAnZOWciqzebucW72EaisisUxBvQTExbmoXrEC/7qmLa/f1JGUpHhG+OWPWfDDHq4cNy/oulVhZqZ6++EbV0/1ZZT8YNk2fjpwrBRqLyKxREG9BP26QwN6t6oNuAN96/ruPvnBr30Tsvwvnp8f1Fo/eOwkLUe61xcZ2KYuD/UrGB/fdeyXHDmR4yi/69Bx0odPJX34VE7qhapIuaegXoqG9W4RdGz+8Mt45cYOvn37c5bj/I/7jvq2h/rlfff6z9ebfNvb9h+l05OzfPu3TVhyRvUVkeinoF6KLmnm7Acfd3076ldJ4XJPax5wvGCdvHSbr5vmj72a+jJCrnzsCl8Z/xeoswPW8p6z3r0//4c9HM12tuhFpHxQUC9FqUkJvHHLRTzcryVPXN2aq9rWA9xdM1/c1wNwz0b1enH2D77tazo29G1XTkkkc+xA3/59760AYORHqwFIjHf5zqUPn8qQ177hnv9b7jv26Xc/ccnYL8nTYtsiMU9BvZRdamoxtFdThnR2jmdvWrOiY//4yVw27Tni229S45ygz7q2ozv3/JTl20kfPtV3fP3o/kFlZ64tWMXp7neWs/3AMWas+fn0HkJEooaCeoTExbm4oIF7Pe7jJ3Pp7Nc3PvHW0InFzq8Xev1ul8sV8vhrczfxzjcFycbunLSUf8/cELKsiMQGBfUIGtSuPgAPTf6Og8cKZoz2aFEzdPn29WnbwBnYX77BnXzs0z91Cyo/eupaRkxZ5Tj2zMz1bAh4OSsisUNBPYIaVksF4H8rCyYlDWgTfpZpWnIiH99dELy/eqAX/VrXBaB1/cpkjh3IyseuYPVf+xZ63wcmf1es+uXn55OrfniRqKKgHkE9A1rkN3ZpzHPXtSvyus1jBrBuVD/SQ/S7V05JpGKF4OwPT/3mAt/2yh8PBJ0Ppfe/5tB0xGd8sHRbkWW7PDmL+z0vcM/U5KXb+EL9/yKnRUE9ghLinf/5a6VVCDoWisvlIjkxvtAyHwzt6ti/pmND3wia1KSCa3s9NZsX/EbdACzcuJf04VN9L27//P5KZttdvmGSx0/mMmFBJidycsnPz2fljwfYeeg4Hy7fzuvzNp92Zsr8fPfygQ+8v5I/TFzC0i37T+tzRMozJfSKMFM7zTcB6cYiMj6eig6Nq5I5diBvLdpC92Y1fMdTEuM5mp3Lo1NW0aZ+ZTL3HuWp6ZaX52ykT6va/OmyZlz/6qKgz/Ou0/rln3vyypxNvLfkR1ZuO8CHy5yLgYz6dA2jPl3jGIJZXHsOZzv2f/3SAtaP7k9SgtoeIsVVrKBujOkM/N1a28sY8y7g7fhNBxZZa6/zK+sCtgHeYRYLrbWPlFyVY8sbt1zEZ6t2cFv3c0vl828MWLHpmKcV/fY3gUvw5TBl+XamLC98xabL/jXHtx0Y0EPZefA4176ykAoJcfyqXX3uurRZ2LKLM/cFHdtz+AT1qqQUeR8RcSuyCWSMeQh4DUgGsNZeZ63tBVwNHADuC7ikKbDMWtvL80cBvRD1qqSUWkAPZcofuxZdyM/9lwenOjgVXcbMYuu+o2zYdZinpltO5ITumsnLy+ePby8D4I2bL/Id33ko9OLeX9ldnqUDp59R/URiTXF+124EBoU4/ldgnLV2R8DxDkB9Y8xsY8xnxhhzppWUktOuUVUqJRf9A+2CBpWZcGsn2jasUuzP7uS3ItQdby1xTJDyMhmfh7z23BGf+bYvbVmLy89zp1K46fVvQ5a/2dMd5J+vXkSKEdSttR8AjmV3jDG1gN7AmyEu2QGMsdZeCjwJTDrzakpJ+u7xvrz1+9ATnAB+eKI//7u7Gz1b1KRni5q8ectFjvwzoaQlJ/DfOy72dfdM/z786JXC0hW0rJMGwF+uPA+ArBM5LNi4p9B7HzmRQ3ZO3hktHSgSK073DdRvgHestaH+FS0BPgaw1s4D6nn62eUs0rFxNXq3rMUnd3djwfDLsKP7kTGwFdd3ahQ0AqeXqUXllETHsTt6nMvkOy/2zYq9rGUtAEb9qnWR9/7FC87c8su2Foxy+fxed06cBlUL+tEHv+pMXTz60zWO/fMfm06LjGm+lMUAObl5pA+fyqRFW4qsj0gsOd3RL32A0WHOPQbsBf5hjGkL/Git1QyWs0xKUjyv+/VdA0X27X981yVUTU2iVqUKviGV13RsyHfbDnLdRY3CXrfq8St4ec5GXpi9EYDV2w85zg96cUHQNS6Xi8evOo/HP3EH8BM5uVRIiCc/P5/X5m0Oe69lW/fTvlFVtnhSGGd8tJobupTcqCKRs93pttQNsMlxwJgZxpgkYCzQ0xgzB3gauPmMaihnjbYNq9CoeqpjjPyQTo345O5uXNw09HJ7SzL6kJacyIN9W7JuVL9CP//DgJe4N1/SxLdtMj5n0aa9HPLrQ58QIkeO9wvijfkFgf+TlT+x5/CJQu8tEiuK1VK31mYCXfz2zw9Rxtvpmg2c+iBliUpxcS7aBOSjWfnYFTw0eSUvDulAfFxBz5v/l8HurBPUTKvApt2HfcfaN6oa9Pn9W9dh2uqdAFz3n0Xc26c5AM/8ti3d/Mbf+wt8QfsnTxri0xk7LxJtNKtDSlzllEReubGjI6B7pXgC+/NfbiA7J88x7j2Ul27o4Nh/1pNlsmPjasTHuZh5f09m/bknNSomlVDt4Vh2brFyz2fn5PHQ5JUlel+RM6WgLmVq9gO9AJiwcAtv+b3ELKxr5vpOwf313glJzWpVpGnNiizJuJybu6Y7yrSpHzpVcTjeNAWt/vK5Y4il1w+7DrN1b8Fygy0ypvHfJdto8/h0doUZT18cq7cfxGRMo9VfPucvH68mO0drzcrpU1CXMlWncrJve/v+Y77twnLZjBnUBju6IOifV7dSyF8Bjw5s5dhftf0gmWMHkuZJcLbj4LGga7yOn8ylySPOQH7wqGMkL32enkOPp2YHjcHPOp7jWCv2VF05bh4nPIF84sItDP+g8Cya2/YfLfS8lG8K6hIx4z0vM8+vV6nIshUS4skcO5DMsQP5bFj3kGUS4+NY/Ggf3/4HQy8GCpYGXLhxLwAfr9iOyZjmmN26JDM4edhX63cxa617vL1/CuJwY/BnrytYbeqXL8yn37Nfs3r7wSKfLdCXdlfYc+nDp9Lt77OZuDDzlD9XygcFdSlzT17dxrE/9Z7QQfp01EyrwP/uvoQHrmhBh8buGa6DO7u7b454+qyHvbuCEzl5zF63my17j/DTgWPc8Po3QZ817N0V/H7CEr7dvI9WI0PPhPXnTcz2ld3Fyh8PsG5nFleOm8cPu8IvSrLmp0NBx9o2KHoW76crAydyi7gpS6OUuU5Ngke5lKQLGlThAr/A6O3yGfnRat9i3eBe3i+Uh/oZ/vG59e1f+8rCsPe6uWs6by7IBGDstHVs+PkwHyxz5p9/b/GPPDrwvKBr9x/JZsBzcwH3ZK7cPPcY/HDdRFnHC7qDsk6ETo+Qn5/Pydx8ZbYsx/R/Xspcg6qpvu0Vf7m81O8XatGQcH53cWP+2Ct8JslAQ3s15YcnChb+DgzoAK/O3RxyNI31W1ZwcOdGZHhSI6z/+TArQixk8olf63ztjkOkD58a9AXwxNS1tMiYRn6+5vuVVwrqUuaSE+MZ2qsp7/yhM1VSS24o4pnKGNiKv/2y8DQHD/UzPNSvIEdd7UrJxVrYZNI3wekKvC9H29SvTOPqzlWsfvXC/KClBJMT3fdpVK3gS/HiMV86+vK9s22vDjFLV8oHBXWJiIf7taRr09CTh0rDvIcvZVD7+mHPp1dPdaRJ+OK+HkGrRwH8sVezkC354f1bFnr/v3z8vWP/wNFsbhrvzkA5ZlDBO4a5D13q2/YP1idycrn/v+4x8a6AgT+3vLk46H4rfjzA4//7Pui4xD4FdSkXGlRN5elrL/Ttf/3gpWQMbMWiR3oDcGfPpo7yzWun0aFxVe7p7Z7B+vIN7YPSGNT1G565ym+Uy9hBbdjwRH8yxw4MG+y9s1wBjCczJbgXI/f+Erht4hKmf++eTTv1u4Kul19eGPzl9MOuw0FdMW8uyFQ3TDmkoC7lysP9WnJjl8Y08rTM61RO5ocn+nNdiAlOAMN6N2f6vT3o17quI43BspGX88X9PX37w/sVBO/rOjUi0dMlc7tf6//gsYIXnef5DeNMDOi+GdC6rm/7jreWknX8pK+VDnBnz3MdC4kALNi4hyGvBo/g+XFf+LH5EpsU1KVcGdqraVB64ML6xOPjXI6WtFe1c5IcL2AbVkul7/m1+fd1FzrKxcW5fKtHLd5csFzfK3Pc+fC+uK9H0Gen1zjHN2EKoM3jM6ia6k59vG5UP1KTEri0ZS3WjernG4v/l4+/9y0U/u7tXejtSYW8ee+RsM9WHCdycnlv8dZipU04VW/M30z68KkhF1OR06egLlJCXrmxY8iuEW8L/7aJSwC45Y2C1Zya1w7+wgCY/WAvx/7+oyfp0aKmY+ZtcmJ80Jj2OBd0blKN0Ve7v7i27it69um6nYf4++fraPFo8KiZZ77YwMMfrOLTVTtYumWf4+XtkTDDKovj8Ikc/vpJQV78UOvT+tt16Dg/FuNZROPURUpdh8YF3Tb5+fnMtruLvKZGxQpsHjPAkbrg6/XB1wX+ysjLd+eir52WTFJCHFv9Wuonc/OCunoWbNzjWISkySOfObJZvjzHnQP/Hs87gErJCY70xwPa1OHFIe6ka7l5+XQZM4vxN10UlLkz0N6AVMjXvLyQTU8OIC5E+ofcvHxfGgZl2iyaWuoipSwlKZ4qnu4T74tPwHcsHJfLxfOD2/n2QyU2CycuzkWjaqm+lvoHS7fR/NFpQV0dgatKgbsVHTic0utQwJqwn60qeJ6mIz5jd9YJrnp+HuMLWcgE4IH3g7Nb7jkSOuf9szPXF/pZ4qSgLlIG+p5XB4A7Jy3zHfswxJDJQFdeUM+3fWfP0CtTzXmwFz1a1Aw63qhaKls8WSUfDpEk7MDR7JCf1/qx6dzz7nK2HyjeS9a3v9kS9GXxt4AlB/1t3H2YxZ5cO/4pIwITqHmN+/KHIussBdT9IlIGujWvwXtLfvTtf//XvpxTzJmuRXU5NK5+DhNv7eSeZeo3iemnA8dYtzOLP0xcQo5fy3vHwWPUrZzCtv3hg/bU73Y4hlEW5tEpq0MeP5adS0pScPbN3n459Ad3bkRacgJ/+r/lTP9+Z9h3DL56rdrBr9s3cLxbOJGTyx1vLeWe3s1DLrRS3qilLlIGrmpbz7Ff3IB+KlrVreQIot70xF+scWaVvHjMl7y1aAt/83tR2alJNcdMWX+/6dCA12/q6FvgBOCbEb1Jr54asrzX7W8tKfR8l3PdCdcaez7nnzOK7mZ5dMpqxwLjAEsz9/OV3R1yrdvySEFdpIz5pwcuTf+6tm3YcyM/Ws23nhEnD/Y1vHpjx7BlN+w6TO9WtTl2siBVce1Kycz0G6fvNaBNHd/23A17gs77j65593b3cMzW9QpeqnpfzAYKzJ+/3j9vzmsF7wVCZb0sbxTURcrI2r/1Y9qw7tRMq1Am9zMBXRm/9eSVD3Rz13QqpyZyS9cmXNW2HgPb1HWc94bTS5q5Fxf/5zXuL4vAkTef39udFwa357vHr/Adm7bK2YWVkiKUAAALDUlEQVTjHc3ToGqK75j/iJex09Yx0++XxVXj5gFwbccGjs+54pmvmbQoOJ/OgOfmsmjT3pDPWV4oqIuUkZSkeFrVLXpBkJLicrnYPGYAN3Rxj5qZv3EPH911iaNMyzppvq6glKR4xl3fjheGtPedf/f2LkzxpEfwJh1LCbFKVev6lWhZpxIul4tKyQWjev7r9x7B37O/dU7S8s/WedvEJSzf6n6R6k2/sGr7QTo1qea4JuOj1Wz4OThXvf8Io/KoWEHdGNPZGPOVZ7udMWa7MeYrz5/fBpRNMcZ8YIyZa4z5zBgT/FpeRMqEy+Xi9u7uvDYTbu3EhQ2r8PWDBUnDPvlTt5DXPdjX8NrvOtLl3Oq4PBnE7u3dnN93a0Kf82r5yi16pDfPD27Hp39yLnQyzbM6lf+Y/JzcgrVX2zZ0TpoKzNZ59YsLHCNdxt98Ef+942JeGNzeUe7yZ74G4IErWviOvTE/M+Qz+Tt+Mpf2o75g5Eerz2h92bNRkUHdGPMQ8BrgzV7UAXjaWtvL8+e9gEuGAqustd2BiUBGSVZYRE5No+qpZI4dSNOaFX37FTyLaARORvK669Jm9DmvtuNYrUrJjLzyPCokFLTU61ROdgy79PJ2/XRKL2hdv/hVQX95qPvOfehS0pILXiDfNsH9ovVXF9ajVpo7/Ay8oC4LH7ks6NrUpAQ2jxkQ8lkC5eTm0fOp2ew7ks1bi7bQ6clZQTNpdxw8FjRBKpys4yeLnBFblorTUt8IDPLb7wAMNMZ8bYx53RgTOAapG+B9PT0NKJu3QiJSbDPv78nHAV0xJcnbT/5t5j6On8xlz+ETPP2Fe3TLH7o3CXlNw2qprHq8r29/yRZ3F8wdARk061ZOoU8r5xfO59/v9P2iKMp32w/y8yFnwN5z2Dn+/eIxX9LxiZlMWb6Nd77ZWujntXl8Bte8vBC7M/yyhV5rdxzitbmbilXP01VkULfWfgD4zwr4FnjQWtsD2AQ8FnBJJcCbhzQLKHy+sIiUuYbVUoO6QErL4sx9jJ22zrcfamk/f9PvdSY5818UxOuJq51J2V6+wZ2qoGtT98tc/6X/Ar3gN5nJa/OegnQK3lZ3fj7c995KRkxZxbqdh1i17aAvAZk346Z/zvu+z37tyMQZytUvzmf01LWlOkrndAbLTrHWetfamgKMCzh/CPC23tOA4HW5RCTm9WlVi5lrd/HszA0s9bS6iyMwK2aoMf21KyXz9LVtubhpdepWLhhJc32nRizYuJcdB4+Tlpzom+n6zYje1K7k7sKZ5ReInx/cjrvfWe5bhzZz7EBWbTtIoH7PznXsz1zzM38Okepg0+7DtAsxASo/P5//rfyJ4yfd7xUGPDe31PLYnM7ol+nGmE6e7d5A4Oq98wFv51Z/YC4iUu54W8+nEtC9vhnRu8gyg9o3cAR0gHpV3PuBY+T//nnBL4VmtSr6tnuZWo5yoz5dU2iKA6+dYV6ueruYAi3ctJdh767w7Y/1W+2qpJ1OS30oMM4YcxLYCdwOYIyZAVwJvARMMMbMA7KBwSVUVxGJIqHy1Be3dVq7UvJptWS9499HfbqG/q0LJkJ9uGw7T197Ifn5+Wzff4zuzWvw6u86OtINALxeRCIyr6em25DH527Yw4mcXMfLZIBNuwu6d4b3bxl2UZaSUKygbq3NBLp4tpcBQW9YrLXeGQfZwDUlVD8RiRH39WlRdKEzVLNiwcSurmO/dJzbe/gEz8xcz7GTuazdkRUU0AN9cV8Pvtm8j4yPQue28XphcHta169Ez6e+AmDF1gN0Pre67/yyrfsdnzGkc+kFdNDkIxEpRTPvL3jpeU/v4AW7S1qofOxeHUbPZNIi90iWC/1eEq8b1Y81f+sbVL557TSa1ChIkDbv4Usds2UBvh3Rm4EX1PVNzAL47X8WOcr456TJHDuQtOTCUy6fKQV1ESk1zWqlMWZQG2be36PYQw7P1Du3dS6yzH9u7ODbTk6MJzUpgQs8C3s8dtV5fOvp0z+3ZkGwblA1lUrJiYwYULAerX/Kh1BLE4I7IRoUnT+/pCj1roiUqlNZ3KMkdG1Wg8yxA30jXz4Y2pVfv+TM4BiqRf+fGzvy7uKt3Nw13fcFFPgiFuD2Hk2Jc7m48oJ6ji+qcGmDJy/dBsB7ngRmpU0tdRGJSWnJCdSvkkKHxlUdmTEvDDM+v07lZO7t0yLoF0Xj6qkMau9ce/a27udSp3Iyge7o4V7I5KQnJcJxv8yWoRYwLw0K6iISk5Zk9GH2A70AdzfJpN+7u2US40+tG2jOg5fy9LUXFl0Q8CYb+INnkfHA3O9lQUFdRGJShYR4khIKQpx3MY5B7RuEu+SMXeyZ0fqV3e3IJxOYBrk0qU9dRMoFd26ZK0p19En3ZjV82y/PKcjxMupXrUMVLxVqqYtIuVHawwkT4uO43JPd0juLddLvOwflgi9NCuoiIiVo3xFnxkfvilFlRUFdRKQE/eM3Fzj2y2p8vpeCuohICWpas6IvrfGGJ/qX+f31olREpIR9OLQrx07mhl1ZqjSppS4iUsLi41xUDJEHviwoqIuIxBAFdRGRGKKgLiISQxTURURiiIK6iEgMUVAXEYkhkRqnHg+wc+fOCN1eRCT6+MXMsAusRiqo1wUYMmRIhG4vIhLV6gIbQ52IVFBfDHQHdgC5RZQVERG3eNwBfXG4Ai7/RO4iIhLd9KJURCSGRFVCL2NMHPAi0BY4Adxmrf0hsrU6M8aYRGA8kA5UAEYDa4A3cS95uBq4y1qbZ4x5DBgI5AD3Wmu/NcY0C1W2jB/jtBhjagFLgctxP9ObxP4zPwL8AkjC/Xd5DjH+3J6/4xNw/x3PBf5ADP//NsZ0Bv5ure0Vru6n8pyhyhZ2/2hrqf8KSLbWXgwMB/4V4fqUhBuAvdba7kA/4HngaSDDc8wF/NIY0x7oCXQGrgNe8FwfVLaM639aPP/QXwGOeQ6Vh2fuBXQFLsH9XA0pB88NDAASrLVdgb8BTxCjz22MeQh4DUj2HDqj5yykbFjRFtS7AZ8DWGsXAR0jW50S8T4w0rPtwv1t3AF3Cw5gGtAH97PPsNbmW2u3AgnGmJphykaDfwIvAz959svDM/cFVgFTgE+ATykfz70e9zPEAZWAk8Tuc28EBvntn+lzhisbVrQF9UrAQb/9XGNMVHUhBbLWHrbWZhlj0oDJQAbgstZ632BnAZUJfnbv8VBlz2rGmJuB3dba6X6HY/qZPWrgbohcA9wJvA3ElYPnPoy762Ud8CrwHDH6/9ta+wHuLy2vM33OcGXDiragfghI89uPs9bmRKoyJcUY0xCYDbxlrX0H8O8vTAMOEPzs3uOhyp7tbgUuN8Z8BVwITARq+Z2PxWcG2AtMt9ZmW2stcBznP9BYfe77cD93C9zvwybgfqfgFavPDWf+bzlc2bCiLajPx90/hzGmC+6fslHNGFMbmAE8bK0d7zm83NP/CtAfmIv72fsaY+KMMY1wf6HtCVP2rGat7WGt7Wmt7QWsAH4HTIvlZ/aYB/QzxriMMfWAc4BZ5eC591PQ2twHJBLjf8f9nOlzhisbVrR1XUzB3cJbgLv/+ZYI16ckjACqAiONMd6+9WHAc8aYJGAtMNlam2uMmQssxP1lfJen7J+BV/3LlmntS07Qc8TaM1trPzXG9AC+peB5NhPjzw08A4z3PFMS7r/zS4j954Yz/HtdSNmwNPlIRCSGRFv3i4iIFEJBXUQkhiioi4jEEAV1EZEYoqAuIhJDFNRFRGKIgrqISAxRUBcRiSH/D5Ryc9DbMF3WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108913048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", but he was bressedis,\n",
      "Mares, it my host:\n",
      "Heth beeg your shatts blead was geveld, hear his eether petiness waw their pars, he's spard, your parits all me, than have Is, and I canny thep to thor is th\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.1, sequence_length=25, layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.training_loop(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

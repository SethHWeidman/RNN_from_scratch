{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classic blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a blog post from Chris Olah, a Thiel Fellow recipient who now works at Google, on LSTM networks. It has some very nice visuals explaining the internals of LSTM cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Models and Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence Models have become the state-of-the-art for machine translation. \n",
    "\n",
    "They involve taking in a sequence of text in one language, having one recurrent, LSTM-based neural network - the \"encoder\" - generate an intermediate \"vector\" representing the \"meaning\" of the sequence, and then having a second recurrent, LSTM-based network - the \"decoder\" - decode this into an output in another language.\n",
    "\n",
    "[Here](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) is the paper that introduced Sequence-to-Sequence models.\n",
    "\n",
    "[Here](https://www.tensorflow.org/tutorials/seq2seq) is a detailed tutorial on how to implement this in TensorFlow.\n",
    "\n",
    "This architecture the #1 real world application of recurrent neural networks today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Headline generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another application of sequence-to-sequence architectures is generating summaries of news articles from the articles themselves. One of my students did this for her passion project! \n",
    "\n",
    "[Here](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html) is a blog post from Google explaining the concept at a high level.\n",
    "\n",
    "[Here](https://github.com/hengluchang/deep-news-summarization) is a GitHub implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Handwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Graves, a leading neural net researcher, developed a technique to [generate handwriting](https://arxiv.org/abs/1308.0850) using an RNN with LSTM cells.\n",
    "\n",
    "In short, he used a deep RNN architecture, with gradient clipping and several of the optimization techniques discussed in the other notebook. The handwriting itself was encoded as:\n",
    "\n",
    "1. X and Y coordinates of the pen\n",
    "2. A one hot encoded version of the character being written, fed into the second layer of the network.\n",
    "\n",
    "See page 27 of the ArXiv paper for the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bleeding edge architecture that may seem like it has nothing to do with recurrent neural networks: however, it is an extension of the idea that when neural nets receive input, they can access a \"memory\" of what data points came before.\n",
    "\n",
    "[Here](https://arxiv.org/pdf/1410.5401.pdf) is the Neural Turing Machine paper showing that this architecture can learn what it means to copy and sort lists of information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

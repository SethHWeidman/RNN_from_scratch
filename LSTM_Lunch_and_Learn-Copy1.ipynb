{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lunch and Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your current understanding of RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_next_character.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_unrolling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is actually going on?\n",
    "\n",
    "### Example with sequence length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the backwards pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_in_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep learning?\n",
    "\n",
    "### What would an RNN with multiple layers look like?\n",
    "\n",
    "![](img/rnn_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the backward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_olah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Forward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Backward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    '''\n",
    "    A class that holds weight matrices along with their derivatives and momentum.\n",
    "    '''\n",
    "    def __init__(self, value):\n",
    "        '''\n",
    "        param value: numpy array, two dimensional, shape of the weight matrix\n",
    "        '''\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        '''\n",
    "        Resets the value of the derivative\n",
    "        '''\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        '''\n",
    "        Clips the derivative, setting its min value to -1 and its max value to 1.\n",
    "        '''\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to AdaGrad rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to stochastic gradient descent rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Params` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        The parameters to be used when updating the values in an LSTM_Layer, which is a layer\n",
    "        of LSTM cells stretched out over time.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the layer. This can be \n",
    "        different for each layer.\n",
    "        param vocab_size: int - the number of characters in the vocabulary that we are predicting\n",
    "        the next character of.\n",
    "        Note: the shape of these weight matrices assumes that the data will be fed in as \"rows\", \n",
    "        meaning that each data point will be represented by a numpy array of shape (1, vocab_size)\n",
    "        '''\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        '''\n",
    "        Biases always have the dimensions of the output of the transformation that they are being added to.\n",
    "        '''\n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        '''\n",
    "        Returns a list of all the parameters for easy iteration\n",
    "        '''\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        '''\n",
    "        Clears all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        '''\n",
    "        Clips all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        '''\n",
    "        Updates all the parameters according to the \"AdaGrad\" rule.\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node performing the role of the \"circles\" in the diagrams above.\n",
    "    It takes has two methods:\n",
    "    1. \"forward\" which takes in: \n",
    "        * An observation, \"X\"\n",
    "        * A hidden state from the prior time step, \"H_prev\"\n",
    "        * A cell state from the prior time step, \"C_prev\"\n",
    "        And returns:\n",
    "        * The output \"X_out\" to be passed on to the next layer.\n",
    "        * The output \"H\" to be passed into the next node as the hidden state\n",
    "        * The output \"C\" to be passed into the next node as the cell state\n",
    "    2. \"backward\" which takes in: \n",
    "        * The gradient of the output with respect to the loss, \"out_grad\"\n",
    "        * The gradient of the hidden state with respect to the loss, \"h_grad\"\n",
    "        * The gradient of the cell state with respect to the loss, \"c_grad\"\n",
    "        And returns:\n",
    "        * The gradient of the output of the same time step in the prior layer - the circle \"below\" in the \n",
    "        drawings above - \"dx_grad\"\n",
    "        * The gradient of the hidden state from the prior time step, \"dh_grad\" \n",
    "        * The gradient of the cell state from the prior time step, \"dc_grad\" \n",
    "    \n",
    "    It uses as parameters, not its own parameters, but the parameters from the LSTM_Layer that it is a part of.\n",
    "        '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, H_prev, C_prev, LSTM_Params):\n",
    "        '''\n",
    "        param x: numpy array of shape (1, vocab_size)\n",
    "        param H_prev: numpy array of shape (1, hidden_size)\n",
    "        param C_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.x_out: numpy array of shape (1, vocab_size)\n",
    "        return self.H: numpy array of shape (1, hidden_size)\n",
    "        return self.C: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, H_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.x_out = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.x_out, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_batch_in):\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_batch_in.shape[0]\n",
    "        \n",
    "        x_batch_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "            x_in = np.array(x_batch_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_batch_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_batch_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "            # Backward pass\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.num_layers = layers\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(self.num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        num_chars = len(inputs)\n",
    "        \n",
    "        x_batch_in = np.zeros((num_chars, self.vocab_size))\n",
    "        for t in range(num_chars):\n",
    "            x_batch_in[t, inputs[t]] = 1 # Input character\n",
    "        \n",
    "        for layer in self.layers:\n",
    "#             print(\"Forward through Layer\", i)\n",
    "            x_batch_in = layer.forward(x_batch_in)\n",
    "                \n",
    "        return x_batch_in\n",
    "\n",
    "    def loss(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "    \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            # Backward pass\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        x_batch_out = self.forward(inputs)\n",
    "        \n",
    "        loss = self.loss(x_batch_out, inputs, targets)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_batch_out, inputs, targets)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Character_generator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_out = sample_model.forward([input_char])\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_batch_out.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    def single_step(self):\n",
    "   \n",
    "        inputs, targets = self._generate_inputs_targets(self.start_pos)\n",
    "        \n",
    "        loss = self.model.single_step(inputs, targets)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_loop(self, num_iterations):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if self.start_pos + self.sequence_length > len(self.data):\n",
    "                self.start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            loss = self.single_step()\n",
    "            ##\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            self.start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            self.start_pos += self.sequence_length\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ec412b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities are not non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bde295646e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcharacter_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacter_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcharacter_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-072b2661a713>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, num_iterations)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n\u001b[0;32m---> 78\u001b[0;31m                                                  200)\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-072b2661a713>\u001b[0m in \u001b[0;36msample_output\u001b[0;34m(self, input_char, sample_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx_batch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0minput_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities are not non-negative"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.1, sequence_length=25, layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.training_loop(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

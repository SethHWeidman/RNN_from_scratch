{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Brief intro section on what LSTMs are\n",
    "* Go through coding up the math, step by step\n",
    "* Go through the theory of how data must flow through them\n",
    "* Go through the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New - with classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we design the classes for an RNN?\n",
    "\n",
    "Before we do this, we have to know what it is we are designing. Let's set out what we do know about RNNs.\n",
    "\n",
    "The goal of the model will be to predict the next character in a sequence. It will do this by taking as input a one-hot encoded version of the characters input sequence and comparing its predictions to a one hot encoded version of the characters in the output sequence, with all the characters shifted one forward. \n",
    "\n",
    "We know that these sequences will be fed into an \"RNN Layer\", one character at a time. Each time, the layers will feed back into themselves a \"cell state\" and a \"hidden state\", - a typical RNN would just feed back in a \"hidden state\", but when our cells are LSTM cells they feed back a \"cell state\" as well.\n",
    "\n",
    "![](img/Olah_RNNs.png)\n",
    "\n",
    "We can implement this as a series of nodes that maintain a hidden state and a cell state that get updated at each time step. \n",
    "\n",
    "The first node, the very first time the network is trained, will receive as information: \n",
    "\n",
    "* A one-hot encoded version of the first letter in the network\n",
    "* An initial hidden state, which can be initialized to all zeros\n",
    "* An initial cell state, which can also be initialized to all zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will return the hidden state and the cell state to be passed on to the next node - keeping in mind that what is really happening is that we are passing the hidden state and cell state back into the RNN layer itself. So, equivalently to what is drawn above, we could draw:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the RNN-specific stuff. The rest of the stuff is neural net specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LSTM_Param`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LSTM_Params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        self.clip_gradients()\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross entropy derivative, that doesn't work...\n",
    "def cross_entropy_deriv(prediction, y):\n",
    "    return np.array([-yi / predi + (1-yi) / (1-predi) for yi, predi in zip(y, prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the LSTM-specific stuff.\n",
    "\n",
    "We discussed above how the forward pass works. Now let's cover the tricky part: the backward pass, i.e. the \"Backpropagation through time\" algorithm.\n",
    "\n",
    "Conceptually, this algorithm works the same way as a normal algorithm used to train neural nets: you have a series of quantities - neurons, weights - that have been done in the forward pass, defined by equations. You want to compute the amount that changing each of these quantities affects the loss. We'll go into the details of how to do this within each node, but at the level of the entire model:\n",
    "\n",
    "* In the forward pass, each node sent forward a value for its hidden output and its cell state output. In the backward pass, therefore, each node will receive _gradients_ for its hidden outputs and cell state outputs that tell us how much these values ultimately impacted the loss. \n",
    "\n",
    "* In addition, recall that each node corresponds to a time step along the sequence being fed into the LSTM model. Thus, each node will receive the gradient from the actual loss - the softmax prediction over all possible characters compared with the one hot encoded version of the correct character.\n",
    "\n",
    "* Similarly to before, we'll initialize these gradients to zero. Each node will output the gradient to be passed to the node _prior_ to it during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "#             print(\"Forward through Layer\", i)\n",
    "            x_batch_out = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch_out\n",
    "  \n",
    "\n",
    "    def loss(self, prediction, y_batch, kind=\"mse\"):\n",
    "        if kind == \"mse\":\n",
    "            return (prediction - y_batch) ** 2\n",
    "        # TODO: other loss functions\n",
    "\n",
    "\n",
    "    def loss_gradient(self, prediction, y_batch, kind=\"mse\"):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        if kind == \"mse\":\n",
    "            return -1.0 * (y_batch - prediction) # P - y\n",
    "        # TODO: other loss functions \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "\n",
    "        for i, layer in enumerate(list(reversed(self.layers))):\n",
    "#             print(\"Backward through layer\", i)\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x_batch, y_batch):\n",
    "        prediction = self.forward(x_batch)\n",
    "        softmax_pred = softmax(prediction)\n",
    "        loss_gradient = self.loss_gradient(softmax_pred, y_batch)\n",
    "        loss = np.sum(self.loss(softmax_pred, y_batch))\n",
    "        self.backward(loss_gradient)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.update_params(self.learning_rate)\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "        \n",
    "    def _initialize_seq_array(self):\n",
    "        return np.array([[]])\n",
    "\n",
    "    \n",
    "    def _append_to_output_seq(self, output_seq, new_x_output):\n",
    "        if output_seq.shape[1] == 0:\n",
    "            output_seq = np.append(output_seq, new_x_output, axis=1)\n",
    "        else:\n",
    "            output_seq = np.append(output_seq, new_x_output, axis=0) \n",
    "        return output_seq\n",
    "\n",
    "    \n",
    "    def forward(self, x_seq_in):\n",
    "        '''\n",
    "        Takes in a vector, outputs a vector of Xs\n",
    "        '''\n",
    "\n",
    "        x_seq_out = self._initialize_seq_array()\n",
    "\n",
    "        H_in = self.start_H\n",
    "        C_in = self.start_C        \n",
    "        \n",
    "        for i in range(x_seq_in.shape[0]):\n",
    "#             print(\"In layer, forward through sequence element\", i)\n",
    "            x_in = np.array(x_seq_in[i], ndmin=2)\n",
    "\n",
    "            x_out, H_in, C_in = self.nodes[i].forward(x_in, H_in, C_in, self.params)\n",
    "            \n",
    "            # TODO\n",
    "            x_seq_out = self._append_to_output_seq(x_seq_out, x_out)\n",
    "            \n",
    "        self.start_H, self.start_C = H_in, C_in\n",
    "\n",
    "        return x_seq_out\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        H_grad = np.zeros((1, self.hidden_size))\n",
    "        C_grad = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        Y_seq_out = self._initialize_seq_array()\n",
    "\n",
    "        for t in range(loss_grad.shape[0]-1, -1, -1):\n",
    "#             print(\"In layer, backward through sequence element\", t)            \n",
    "            Y_grad_back = np.array(loss_grad[t], ndmin=2)\n",
    "            Y_grad_out, H_grad, C_grad = self.nodes[t].backward(Y_grad_back, H_grad, C_grad, self.params)\n",
    "            \n",
    "            Y_seq_out = self._append_to_output_seq(Y_seq_out, Y_grad_out)\n",
    "        \n",
    "        return Y_seq_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: break out `LSTM_Node` into separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, h_prev, C_prev, LSTM_Params):\n",
    "\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.v = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.v, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        \n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        \n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        \n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dh_prev = dz[:, self.vocab_size:]\n",
    "\n",
    "        dC_prev = self.f * dC\n",
    "\n",
    "        return dx_prev, dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    \n",
    "    def __init__(self, text_file, model):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.model.sequence_length\n",
    "\n",
    "\n",
    "    def generate_sequences(self, start_pos, seq_length):\n",
    "        input_sequence = ([self.char_to_idx[ch] \n",
    "                           for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        target_sequence = ([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos+1:start_pos + seq_length+1]])\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "\n",
    "    def sequence_to_model_input(self, sequence, vocab_size):\n",
    "        out_batch = np.zeros((len(sequence), vocab_size))\n",
    "        for i, el in enumerate(sequence):\n",
    "            out_batch[i, el] = 1        \n",
    "        return out_batch\n",
    "\n",
    "    \n",
    "    def generate_batch(self, start_pos):\n",
    "        input_sequence, target_sequence = self.generate_sequences(start_pos, self.model.sequence_length)\n",
    "        return self.sequence_to_model_input(input_sequence, self.vocab_size), \\\n",
    "            self.sequence_to_model_input(target_sequence, self.vocab_size) \n",
    "\n",
    "    def sample_output(self, x_batch, sample_length):\n",
    "        next_char = np.array(x_batch[0], ndmin=2)\n",
    "        \n",
    "        indices = []\n",
    "        sample_model = deepcopy(self.model)\n",
    "        for char in range(sample_length): \n",
    "            x_batch_out = sample_model.forward(next_char)\n",
    "            softmax_probs = softmax(x_batch_out)\n",
    "            \n",
    "            idx = np.random.choice(range(self.vocab_size), p=softmax_probs.ravel())\n",
    "            next_char = np.zeros((1, self.vocab_size))\n",
    "            next_char[0][idx] = 1.0\n",
    "            indices.append(idx)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "        \n",
    "\n",
    "    def train(self, steps, check_every, sample_every, update_method=\"ada\"):\n",
    "        # TODO: break this up into multiple functions\n",
    "        start_pos = 0\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        while True:\n",
    "            if start_pos + self.model.sequence_length >= len(self.data) or self.iterations == 0:\n",
    "                pointer = 0\n",
    "\n",
    "            x_batch, y_batch = self.generate_batch(start_pos)\n",
    "\n",
    "            ## Sampling code here            \n",
    "            \n",
    "            loss = self.model.single_step(x_batch, y_batch)\n",
    "\n",
    "            \n",
    "#             if self.iterations % sample_every == 0:\n",
    "#                 sampled_text = self.sample_output(x_batch, 100)\n",
    "#                 print(sampled_text)\n",
    "    \n",
    "            \n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [self.iterations])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "            \n",
    "            if self.iterations % check_every == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                print(\"Loss\", loss)\n",
    "\n",
    "            start_pos += self.model.sequence_length\n",
    "            self.iterations += 1\n",
    "            if start_pos + self.model.sequence_length > len(self.data):\n",
    "                start_pos = 0\n",
    "                \n",
    "            if self.iterations > steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = LSTM_Model(sequence_length=25, vocab_size=62, hidden_size=100, learning_rate=0.1, layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.train(10000, check_every=100, sample_every=1000, update_method=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No layers, just:\n",
    "\n",
    "* Initialize LSTM Params\n",
    "* Write LSTM Model - sticking close to working example\n",
    "* Write LSTM Layer - sticking close to working example - use `t` to index time and use \"forward-backward\" functions.\n",
    "* Write LSTM node\n",
    "* Write \"sample\" fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, h_prev, C_prev, LSTM_Params):\n",
    "\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.v = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        self.y = np.exp(self.v) / np.sum(np.exp(self.v))\n",
    "        \n",
    "        return self.y, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "\n",
    "        assert self.z.shape == (1, self.vocab_size + self.hidden_size)\n",
    "        assert self.v.shape == (1, self.vocab_size)\n",
    "        assert self.y.shape == (1, self.vocab_size)\n",
    "    \n",
    "#         for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "#             assert param.shape == (1, self.hidden_size)\n",
    "\n",
    "\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_batch_in):\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_batch_in.shape[0]\n",
    "        \n",
    "        x_batch_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "            x_in = np.array(x_batch_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_batch_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_batch_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "            # Backward pass\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.num_layers = layers\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(self.num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        num_chars = len(inputs)\n",
    "        \n",
    "        x_batch_in = np.zeros((num_chars, self.vocab_size))\n",
    "        for t in range(num_chars):\n",
    "            x_batch_in[t, inputs[t]] = 1 # Input character\n",
    "        \n",
    "        for layer in self.layers:\n",
    "#             print(\"Forward through Layer\", i)\n",
    "            x_batch_in = layer.forward(x_batch_in)\n",
    "                \n",
    "        return x_batch_in\n",
    "\n",
    "    def loss(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        y_batch = np.zeros((len(inputs), self.vocab_size))\n",
    "        for t in range(len(inputs)):\n",
    "            y_batch[t, targets[t]] = 1\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "    \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            # Backward pass\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        x_batch_out = self.forward(inputs)\n",
    "        \n",
    "        loss = self.loss(x_batch_out, inputs, targets)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_batch_out, inputs, targets)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_out = sample_model.forward([input_char])\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_batch_out.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    def single_step(self):\n",
    "   \n",
    "        inputs, targets = self._generate_inputs_targets(self.start_pos)\n",
    "        \n",
    "        loss = self.model.single_step(inputs, targets)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_loop(self, num_iterations):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if self.start_pos + self.sequence_length > len(self.data):\n",
    "                self.start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            loss = self.single_step()\n",
    "            ##\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            self.start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            self.start_pos += self.sequence_length\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD0CAYAAABdAQdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XeYVNX9x/H3bKfsgoB0ZEHg0BEREJCOiGBEY4z+gjWWaCxgi1RRkWILMWqwYiESjQU1UpVeBUEREA7SO9JZysK23x9TdoadLcDOzp3dz+t5eJy5987c747wmbPnnnuOKysrCxERcbaocBcgIiL5U1iLiEQAhbWISARQWIuIRACFtYhIBIgJxZsaY+KB1sBuICMU5xARKYaigWrAMmvtKf8dIQlr3EE9P0TvLSJS3HUEFvhvCFVY7wb46KOPqFq1aohOISJSvOzZs4d+/fqBJ0P9hSqsMwCqVq1KzZo1Q3QKEZFiK0f3sS4wiohEAIW1iEgEUFiLiEQAhbWISARQWIuIRACFtYhIBHBcWC/auJ/kgZPZcehEuEsREXEMx4X1f5dtB2DZloNhrkRExDkcF9ZRLhcAGZlhLkRExEGcF9ZR7rDO1HJjIiI+jgtrl+e/WhtSRCSb48La2w2SqawWEfFxXlirG0REJAfHhbWnYa2WtYiIH8eFtadhrT5rERE/jgvraG+ftZrWIiI+jgtrlyes0xXWIiI+jgvraE8/iHpBRESyOS6sYzxhnaG0FhHxcVxYe1vWGeoGERHxUViLiEQAx4V19kROCmsRES/HhXWMWtYiIjk4Lqy9t5tr6J6ISDbHhXW05gYREcnBcWGtbhARkZwcF9YaDSIiklNMXjuNMbHAeCAZiAeeA5YAbwMXANHAbdbajYVVULSvz1rreomIeOXXsr4FOGCt7Qj0Al4DXgA+stZ2AoYCDQu1IK3BKCKSQ54ta+BT4DPPYxeQDnQAfjbGfAdsAfoXakFRmnVPRORMebasrbXHrLUpxphE3KE9FHeXyCFrbQ9gG/BkoRakoXsiIjnke4HRGFMLmA1MsNZOBA4AX3t2/w+4rDAL8s1nraF7IiI+eYa1MaYKMAN40lo73rN5AdDb87gTsKYwC9JoEBGRnPLrsx6Me9THMGPMMM+224F3jDH3A0eAPxVmQd41GBXWIiLZ8gxra21/gl9AvDI05WTT0D0RkWyOuynG21WtoXsiItkcF9ZeusAoIpLNsWGtoXsiItkcF9ZZuENaN8WIiGRzXFh7aTSIiEg2hbWISARwbFhr6J6ISDbHhbVv6J4a1iIiPo4Lay9dYBQRyebYsNbQPRGRbI4La283iFrWIiLZHBfWXhm6g1FExMe5Ya2WtYiIj8JaRCQCOC6svRGtsBYRyea4sPZSWIuIZHNsWGvonohINseFdZZnFIjmsxYRyea4sPZSN4iISDaFtYhIBFBYi4hEAMeFtYbuiYjk5Liw9tLt5iIi2Zwb1mpZi4j4ODasRUQkm/PCWg1qEZEcnBfWIiKSg8JaRCQCOC6ss9QPIiKSg+PCWkREclJYi4hEgJj8DjDGxALjgWQgHngO2A58A/zqOWyctfaTENUoIlLi5RvWwC3AAWvtrcaYCsBPwLPA3621Lxd2Qf43LmZmZhEV5SrsU4iIRJyChPWnwGeexy4gHWgFGGNMX9yt6wHW2pTCLi49M4s4hbWISP591tbaY9baFGNMIu7QHgosBZ6w1nYCNgHDQ1GcFiAQEXEr0AVGY0wtYDYwwVo7EZhkrV3u2T0JaFlYBfnHs5b2EhFxyzesjTFVgBnAk9ba8Z7N040xbTyPuwPLg774PGVkKKxFRKBgfdaDgQuAYcaYYZ5tjwJjjTFpwB7g3lAUp2lSRUTc8g1ra21/oH+QXR0Kv5xA6ZmZoT6FiEhEcNxNMf6Nac1pLSLi5riw9peuPmsREcDpYa2WtYgI4MCw9p9178Tp9DBWIiLiHI4La3+paRnhLkFExBEcHdYnTiusRURAYS0iEhEcF9b+Q/dOKqxFRAAHhrU/taxFRNwcF9ZHTqb5Hms0iIiIm+PC+o05G32P1Q0iIuLmuLA+7teaPqmheyIigAPD2p/6rEVE3BwX1t7BIAmxUeoGERHxcFxYe5WNj+GEukFERACHh/VJjQYREQEcHNZl4mPUZy0i4uHcsI5TWIuIeDkurL23m5dNiNGseyIiHo4Lay91g4iIZHNcWF9etwIAZeOjFdYiIh75rm5e1N65vTU7Dp3g0x92aDSIiIiH41rWZeNjaFg1idJx0ZxIyyArS+swiog4Lqy9SsVFk5UFp9Izw12KiEjYOTasS8dGA5ofREQEHBzWlRLjAfgtJTXMlYiIhJ9jw7pCmTgADh1Py+dIEZHiz7FhXa5ULBC4coyISEnl2LAuX9rdsj5y8nSYKxERCT/HhrVa1iIi2fK8KcYYEwuMB5KBeOA5a+3Xnn1/Ah6y1rYLRWFl4qKJjnJx+ITCWkQkv5b1LcABa21HoBfwGoAxpiVwF+AKVWEul4uy8THsOarRICIi+YX1p8Awz2MXkG6MqQiMAgaEsjBwd4F8sWJnqE8jIuJ4eYa1tfaYtTbFGJMIfIY7uN8FHgVSiqA+AJ6ftq6oTiUi4kj5XmA0xtQCZgMTgF+B+sA44GOgsTHmHyGtEBg3Z2OoTyEi4mj5XWCsAswAHrTWzvRsbuLZlwx8bK0NWXfI/L91peMLswHIysrC5QpZF7mIiKPl17IeDFwADDPGzPH8KVUEdQFQq0Jpnu3bBICeY+cV1WlFRBwnz5a1tbY/0D+XfVuAy0NQU4Am1ZMA+PW3Ywz4+EcG9W5ElaSEUJ9WRMRRHHtTjFejakm+x1/+tIu2o2ZqjmsRKXEcH9al42LYMqZPwLYV2w6HqRoRkfBwfFh7rRh2JZ/c6+51uWHcIv6zdFuYKxIRKToRE9YVysTRtm5FejSqDMAk3SwjIiVIxIS11zu3t6aruZDjWkxXREqQiAtrcPdjp6ZpuS8RKTkiMqwTYqNJTXMvpJuekanRISJS7EVkWJeKi+JkWgYnTqdTb8hUXphuw12SiEhIRWZYx0Zz8PhpGj81HXDPHXI6PTPMVYmIhE5EhvWpIMHcYOhUUlK1UIGIFE8RGdb3dqoLQFJCDLMe6+zb/smy7eEqSUQkpPKcG8Spal5QOuCuxpdubMHjn67kuclrqVQ2nmtbVCcqSjP0iUjxEZEt6zP9oVVNHu5eH4ABn/xE3cFTWL3zSJirEhEpPMUirAEevbIBVzet6nt+zasLwliNiEjhKjZhDTDy+mZcmBjve974qWkagy0ixUKxCusKZeJYNqQHK4f3BODE6QzGTD239RuzsrIYN2cjWw8cL8wSRUTOSbEKa69ypWJ55eZLAHh7/qazbl2fTs/k65W7eH7aOjq/OIeMTLXORSS8imVYA/S9pAZ/6VyXzCyoM2gKHyzaUuDXjpm6jv4f/+R7vvXAcfalnApBlSIiBRORQ/cK6q4r6vDm3E0ADP96Db2bVQvo0z5TSmoaWcCUVbsDtnd7ea7vcbMa5bikVnme7dtEC/iKSJEpti1rgMqJCbxxy6W+57eNX5rrsd/+spdmT8+g+dMz2HM0FYDnb2iW47hVO48wYclW6gyawrz1+/jqp53sPnKy8IsXEfFTrMMaoFfTar4baNbuPkrywMk5jtmXcop7PvwhYNuf2l7ETa0vYvUzV/GvfpfyF89dk/5uG7+U/h//RLvRszh5WlO2ikjoFPuw9pp4T1vf437vLPE9fmveRlqP/C7g2Ldvu4xR17tb1WXjY+jdrBqDejdi3hNdee/O1lxet0KO97/z/Zyt9hemraPOoMkaUSIi561Y91n7a39xJd67ozV3vr+MhRsOkJmZxRvzNvLCtOzpVd+9/TK6N6qS63tcVLE0F1UsTVfjXlrst6OprNl1lDvfX8aSTQf522cref6G5jz26Uq+8Ft2rPOLcwBYNLAb1cuXCs0PKCLFWolpWQN0bViZYdc0BuD9RVt8QT2ibxPG/L4ZXTwhXFCVkxLo2rAy93e5GID//rCDOoOmBAS1v/ZjZjHpxx3qMhGRs1aiwhrgklrlAHj2m18AcLmgX9va3NzmIqLPcfKnJ3s19K28fqY2dQK7TB75ZCWNnpqmZclE5KyUmG4Qr1a1K3Bl4yp8+8teACb8uW2hzNDXtm5FbmtXmw8Xb+WRHg24vX1tysbHcDItg60HTlA5MZ42o2b6jv/Hd78y8OqG531eESkZXKGYO8MYkwxsnjlzJjVr1iz09y8Mo6eu5VhqOiP6Ni2y6VTTMzLp8Pws9h5132Az/29dqVWhdJGcW0Scb8eOHXTv3h2gjrV2i/++EtcN4jXo6kaMvL5Zkc57HRMdxfeDe1DDc5Fx8KRVRXZuEYlsJTasw+ljT//28VPpYa5ERCKFwjoMalUozcPd67Ni22FenflruMsRkQiQ5wVGY0wsMB5IBuKB54ANwFuAC/gVuNtaqybiWWqT7B4l8vK367m5zUV5zlkiIpJfy/oW4IC1tiPQC3gNGAUMttZ28BzzuxDWV2y1u7giF5SOBeDHbYc4cVrfdyKSu/zC+lNgmOexC0gHbrDWzjPGxAFVAS12eA6io1x896h7ZfZ7Jyyn0wuzycrK0so2IhJUnt0g1tpjAMaYROAzYKi1NsMYUxv4DndQrwx5lcVUxbLZXR/7j52mzqApAGwe3VvTr4pIgHwvMBpjagGzgQnW2okA1tqt1tr6wBvA30NbYslTZ9AUDp84He4yRMRB8gxrY0wVYAbwpLV2vGfb18aY+p5DUoDM0JZYvHnvYux7SfWA7Us3H+S7X/aSPHAyd3/wA/N/3ReO8kTEIfK8g9EY8wpwE+C/6uwQ4AXgNHAC92iQ3We8LhmH38HoRGkZmdQfMjXX/Zq1T6R4y+sOxvz6rPsD/YPs6hBkm5yn2OgotozpE3SBBIAvVuzgwW71g+4TkeJNN8U4UJ/m1YJuf2nG+iKuREScQmHtQGP/eInv8bh+l7J0SHff85XbD4ejJBEJM4W1A8XFRLF5dG8WDezG1c2qUTkxgceubABAv3e+5+TpDDIys5iwZCu/eRb3FZHircTNZx0pXC5XwMXEh7rX5+Vv13PsVDqNnprm2z7sy9W+BYFFpPhSWBcD+1JO5ZhbJCU1jWZPzyA6ysXn97dn56GTxMVEcWXj3NeYFBHnUjdIBNkw8mrKlYrNsb31yO/4ydOXnZaRyX9/2E6zp2cAkJGZxXWvL+SBiSu458MfGDn5Fy0pJhKB1LKOIDHRUawc3hOA3/9rIUmlYplj3TfLXPf6Qt6/szV3vLcsz/d4e/5mSsVG82hPE/J6RaTwqGUdob74awfev7MN79x2mW/bmUHdp1k1/vl/LXO89p+zNrDr8MmQ1ygihUct6wjXo3EV+nevzyt+ixi8dWsrejapGnDcw//5MeD5H99czIInuxVJjSJy/tSyLgYeubIBE+9pC0DpuOgcQX1ti+psGdOHj++9nM/uawfAjkMn+feSrUVeq4icG7Wsi4n2F1fino516N4o99Eel9etGPB86Jerubl1LWKi9Z0t4nT6V1qMDOnTOEcgB/Ofey73PZ64dFsoSxKRQqKwLoHaXVyRz+9vD8D2gyfCXI2IFITCuoRqVfsCapQvxcZ9x8NdiogUgPqsS7Cdh0+y8/BJNu07Rt0Ly4a7HBHJg1rWJdgfWrkXhhg1ZV0+R4pIuCmsS7CXbmwBwHdr94a5EhHJj8JaAFiy6UC4SxCRPCisS7gXbmgOwM1vLSGv9ThFJLwU1iXc7y+t4Xu89YCG8Yk4lcK6hIuJjmLCXW0AuHfCD2GuRkRyo7AWrqhXCYD1e4+RmXnuXSGrdhxh95GTtB31HckDJzP22/XMW++ewvW3lFSSB05mtv2tUGoWKWkU1oLL5aKGZwmxuoOnMOccA/V3ry2g3ehZ7D16CoBXZv7KbeOXAtBm5EwA/q4V2kXOicJaABhxXRPf4zveW8byrQfzfc3qnUe47vWFfLBoC8u3Hsr1OP+5s1ftPMJTX61mz5HIW+g3PSNTF2ElbBTWAkD50nEBz28Yt5iJ3+ec5Mm/m+SaVxfw0/bDDP96DTeMWxRwXJ9m1XxLkN3uaV17fbh4K5ePnklKalphlR9yh0+cpt6QqQz/ek24S5ESSrebCwAta5XniavcS329ON0CMHjSKgZPWkXfS6qzdvdR1u89BsDSId0pExf8r86UhztS44JSlCsVy28pqbQZOZNff3O/rlG1JNbuPuo7dunmgzSvWZ7rXl/I1AEdSUrIub6kU3i7cz5cvJWuDSvT1VQOc0VS0iisBXD3Wz/QtR4AzWuW49Z3s1vDX/20K+BYb/8zQOXEeMbc0IxH/7uS0dc3o3H1JL99CQGvu84T+l53fZA9+qT50zPYMqZP4fwwhWjv0VQSYqL5eccR37Y731vGrMc6az4VKVIKa8mhY/0LC3zs5/e3p1aF0vz0VM98j+3gGXWSm7HfrueRKxsU+Nyh9ubcjYyeGnzelO2HTvrCeufhk6SlZ5JcqQy/HXX3xVdOSgj6OpFzpT5rCeqNWy6lT7NqrBvRi24N3b/yT7y7Lbe3qx1wXK0KpfN8n/s6XwzA9AGdaFI9iZHXN2XFsCuDHuu/jqQTnBnU5UrFMvYm93wqt49fSvLAyazYdogOY2bR5aU57Dx8kjajZtJm1Mxgb+eTlZXFv+Zs4PPlOxj+1WrSMzJD9jNI8eEKxdVtY0wysHnmzJnUrFmz0N9fwis1LYMNvx2jaY1y5/wex0+lk5mVRWJCLN/+spd7PnR3iax6uieJefRdL9ywn3KlYs/r3AWVPHBywPOvHuhAw2qJmKHT8n3t5tG9cblcQfdtPXCczi/O8T3/+sEONK9Z/rxqleJhx44ddO/eHaCOtXaL/z61rOWsJcRGn3dYlomP8YXylY2r8MYtlwKwZf+JPG/M6ffO91zz6gKmrd7Dht9SzquGvOxLORXw/PK6FWhRqzzxMdFMvLttvq/3XlQNZta6wHHs27RajxRAnn3WxphYYDyQDMQDzwHbgFeBDOAUcJu1VnNsynmpeYG7O2XRxv387rUFvHRjCx7/dCXg7kIxVRMDliC779/LAUJ2UbL1yO8A92rxJ05n8H9tLvLta1+vElvG9GHC4i0M+yr4UL6eY+cx87HOVC9XipdnWB7uUd832mX7wZMBx27II9hDZdKPO+hmqlCutHNH4HhlZGaxZteREv/bR34t61uAA9bajkAv4DXgFeAha20X4AvgyZBWKCWC9w5Kbz+xN6gBPly8BSBgRIbX9DV7Cr0W//Cc/7euvHVrK65pXj3Hcf3aZvffP3NtEz6/vx0/DO3h29b95bk0emoa7yzYTPOnZzD8q9UcPH6a8Qs3A3D3FXUA2HkoMLxDbdfhkzzyyUpaPDsj6P4Xp6+j1z/mFWlNwZxKz+CZ/63h9/9ayLWvLeTh//zI0Qgam1/Y8hsN8inwmeexC0gHbrbW7vZ7feTdiiaOUz6PFl4WMGHxFkZ8szbHvr9MWF7ores+/5wPQGJ8DBXLxtOzSdWgx0VFufh+cHcSE2Ioncu4c38fLN7Khn3ZXwRDr2nMhCVb+XT5DtrUqcAL0y1LB3fPta+7sOQ2P8svu47S2/Ozg/tGoDNvlipKPcfOC5gJ8uuVu/h65S5WDLuSCmUKr65tB06QVComrD9rQeTZsrbWHrPWphhjEnGH9lBvUBtj2gMPAmNDX6YUd7kFVN1KZbB7Uhj21RpOe0ZNdGtYmbqVyviOSc/IZNSUtWwrpCleW17k/nV7/pNd8z22SlJCjqB+8Q/Ncz1+4Qb3Ig9D+zQC4FS6+2d64rOf2Zdyimf+9wvJAyfn6DMvTEMmrfY9/s/SbRw7lU5aRmZAUEPglLnHTqWzcvvhkNV0pn0pp3KdsnfVzpy/YZ2PTi/OpuPzs0keOJnkgZM5cCz7s09JTeONuRtJTcso1HOei3wvMBpjagGzgQnW2omebTcBbwB9rLX7QluilBRRnrx+vGf2WOsO9SrlmHdk/B2tmfV4F9/x9YZM5a15mxg55Zd8z/HFih2szuUf+yfLtpE8cDJLNh2kfOnYc25p3XhZLd/doI2rJfHNQ1fkOOZWzxDI9+9sHbD9/UVbAHef+drdR/OdBXH3kZPsPnJ23SgxUdlfjIO+WEWvf8yj/pCpOY7b4dc903T4dPq+vpBDx08X+DyZmVlsPXD8rGrbl3KKpZsP+q4ZBHP7+KXsOFQ4X8xHTrq7VVJOpfu2fbp8h+9xs6dnMGbqOt5dsLlQznc+8gxrY0wVYAbwpLV2vGfbLbhb1F2stZtCX6KUFD8/fRUrn+rJg93qU7ui+4LjJbVyv6j0bN+mAc8X/LofcA8t9N6c4u9oahqP/ncl17y6IOj7jf02e5z3+d76fmHZeADKxEfTpHoSQ3o3olOD7JuN4mOiAeiSx23rV78yn7s+WIbdk3PUy8IN+/nm5120Gz2LdqNnsXl/8FA8cTo7hI6mprF291HSM7Po06yab/uOM/rMm3lG+nzz8y6aDp/OQb+AHjxpVa71numdBZvo/OIc1u05mv/BwB/fXEzrkd/xxzcX56jlTFc8P/ucJ9XaezSVCUu2kpWVxbg5G3PsHzN1XY4W9ikHtKzz62gbDFwADDPGDAOigabAVuALYwzAXGvt8JBWKSVC2fjsv47fPtKZ1PQMUlKzw+ad2y6juudCJMAtl9dm6JfZv9IfP53B6p1HfGH81QMdaFGrPAs37Kd1cgU278sOtNS0DBJio0keOJmmNZL45qGOARNLta1T4bx+lopl3a3ymKgoXC4X93Sqyz2d6pKVlZWjy2fFsCvp88/57A4yE+Fsu4/Znl9e143oRUJsNMdOpdPvne8Djnvlu/X84+aWvudHTqT5LiA2r1mOlrXK88Hirb79LS8qzxX1KzHoi8Dw3TiqN2kZmTQcNo2pq90Xb1+Yln1z0NTVe9i8/zh1/LqhMjOzSE3PYNvBEzSsmsQz/1vDewu3+PZP/H5bji/WYJZuzjnT48R72tLsaffPUaFMHK2TL2D6Gvfgs/3HTnNhYny+73umsd+u5+Nl26ldoTRvzM0Z1l7+n/GXP+3i0Z7mrM9VmPIMa2ttf6B/EdUi4hMXE0VcTFRAC7dH4yo5jpt4d1vKl47z9bf6t5r7vr6Qu6+owzsLNvOXznU5nZ59p+ArM3/loW7uuVBW7zyK3ZPC8dMZ1K1Uhosrl2X4tU04HzHR7l9ao6MCgzlY33yFMnEsHtTd9/yRT35i0o87cxw39tv1XJgYz3OTc15oTfPrLklNywgY6fHzjiM5RtLc0T6ZmOgoXMBAT2AnJsQQHeUiOio64NiPl20PeN7/4x/5+sErmLl2L6OmrGWj35fguhG9AoIa3JNf5RfWGWd09ywa2M33xfxg13p0anAhbTxfoN6blV6abnk+j+sDXnuOpLL/2Cma1ijHtgMnfD/PbX6zQW4YeTUAc+w+nvhsJYdOpLHO7zeabQdPMHrqWu5sX4eq5cIzlYBuihHHe//O1nx6X7ug+9rXq0Tj6knc07FO0P2LPau2vzl3Ez9sye77HjdnI42fmu57fpVnqNpdHevw9m2XBbTyz0XbOhXo3awqI67Lv0V5phtbBb/r9815m4IGNcDkn3dTb/AUbh+/lIbD8r7DskpSvO/L5Ga/8ePTB3TyPb6qSc4vxoZVEwF3+A+ZtIq7PvghIKghcO7ys/G/ldmThVUsExfwG9TjVxlfUAN8+4i7ztMFuE3/5x2HuXz0TK55dQFLNh2g04uzcxxTOi6amOgoYqKj6NG4Cj/mMs/Nm3M3cfnomWGb01xhLY7XxVSmdXLe3RJD+jTm4gvdv5rf1q42d7RPBmDNruz+0lU7j9Amn/epdUHec50UVEJsNP/q1yqgu6CgvDfdeP9MebhjgV6XnpnF3PV5X+//x02X8P3gHgHb1j7bi19HXh0QkCOua8oHf24TcNyAHtkXfj8KMtc5wN1+Myk2rJpIYoL7S+97z5dmbrz94lMe7sicJ7rkeWz9Ku4vjWC/fZzp2tcW+h7f/NaSoMfMfSLvUT/3dqob8Dy36wOhprCWYmPmY13YMqYPz/Ztyj1n/APzWrrlIK1qX5Dre3iH7TlJ4+pJPNmrYcC2pYO7894drfn56dxnO1w6uDubR/f2PZ/9eBeua1kjx3Gl4qKJjQ6MgsqJCXRucCGrn7nKt61Ho/zn8N7kCbKvH+zAtAGdeOlG98RX6/O5S/OHrQcpVyqWRtUS85wbJpiMzCymrd4TMHImIzOLiwdPCXr8qOubsWlUbwb0qM+tl9cO2u89fUAnXv2/lmwa1ZvScYHdQvdOWB6WybcU1lIs1ShfiqpJCZSJi+bnp3tS2fMPcuDVDXnOr2vij5fVZETfJiQlxLBpVO+zDoqicn+Xi1n5VE/WjejFljF9qJyUQNeGlUlKiOWNWy6lRc1yAV0u3mNcLpev+6KGX8u5oMrGx7DmmauY9VhnYqKjcnQ3LRvSg3UjenHDpYFdN02ru0dx9PRcZxj25WqOeYbHjZuzkWtenR8w2mLF1sO0vKh8gW8I8t5E1XbUdzz99Rru+/dyPl+RPeSu/pApAf3gVZKyA/lPbS8iKsrFgB4Ncu2mMlUT+V2L6kRFuUhNCwzmDb8do96QqUUe2JrPWoqteX/rSpTLfbFv0cBubNx3HFM1kbSMTPo0q8YDXev5Fku4tV1yeIstgNzm8ejVtBq9mrqH4k1dtZubWtcK2P/vu9uyef9x4mLOrW1WJj7GN3f37y+tydvzs8ccVywTR1SUi5f/2MIXliOua0qU58Kqf/g+8/WagDHMrZ77zt3Ns2o3e46mntWFu8/ua0+Pv89l79FTTFjiHuXyxGc/s3HfcZrXLIf/9cr37mxNV1OZb37eFTB8sqD+2vVilmw6wDPXNqHv69ndKvWGTGXxoG5UK3f2X4LnQlOkishZ2Xs0lbaeObv9b/VfvzeFRRv2c0eHwNb3uDkbeX5a8EUc/P2lU10G9W5U4DrOnMI2mAVPdvVNElYYnvnfGmas2ctOvwup/76rLVfUz3thjYLSFKkiUmiqJCXwzUNi/ukTAAAFp0lEQVRXMOfxLgHbG1RJzBHU4L7gWxBnE9TgHhOel5dvbFGoQQ0w/HdNWDiwW8AiHLe8+30eryg8CmsROWtNa5QjuYAjXcrEx3Cp34XbD/7chrlnjPjI66JvbqKjXL550L0XMr36NK/GDbkMgSwMz/RtmusQy1BRWItIyH3x1w6+oYidG1xI7YplmPN4F+pV9qxjeY7TxF7VpCoLnuzKH1rVDOiSCe28hW4v/KG5b9m61LQMJv24g9+OpnLLO98Hne7gfCmsRSQskiuV4SPPqjudGpxbn6/L5Qra1XFmSzsUXC4XF3nWIB03ZyOPfLKSW99dyoIN+wP6tAuLRoOISNhUSUrgi7+2x3hudDlfY29qwZb9J0iIjc7/4ELgnQPGu9iz3eu+Rf1chknmR2EtImF16UVn31+dm+tbFm0/cqWywSeSym37+VA3iIjIOapUNvic51FRhd9rrrAWETlH/req973EvU5nCHIaUDeIiMg581/SbcR1TXnFb07xwqawFhE5D3UqlSElNf28VxfKj8JaROQ8TO1fsClsz5fCWkTkPBTVMEFdYBQRiQAKaxGRCKCwFhGJAAprEZEIoLAWEYkACmsRkQgQqqF70QB79uwJ0duLiBQ/fpmZYzxgqMK6GkC/fv1C9PYiIsVaNWCj/4ZQhfUyoCOwG8gI0TlERIqbaNxBvezMHSFZ3VxERAqXLjCKiEQAR80NYoyJAv4FtABOAXdbazeEtyo3Y8wK4Kjn6WbgTeAVIB2YYa19Jtz1G2PaAs9ba7sYY+oB7wNZwGrgAWttpjFmONDHU/cAa+3S3I4t4npbAt8Av3p2j7PWfhLueo0xscB4IBmIB54Dfgl2/nDXmke923HmZxsNvA0Yz/nuA1KDnT/cteZRbyxF9Nk6rWV9HZBgrW0HDAReDnM9ABhjEgCXtbaL58+dwBvAn4ArgLaesAlb/caYvwHvAAmeTX8HhlprO+Je7LmvMeZSoDPQFrgZeD23Y8NQbyvg736f8ScOqfcW4IDnXL2A14Kd3yG15lavUz/b3wFYazsAQ4GRwc7vkFpzq7fIPlunhfUVwDQAa+0S4LLwluPTAihtjJlhjJlljOkExFtrN1prs4DpQA/CW/9G4Pd+z1sBcz2Pp/rVN8Nam2Wt3QbEGGMuzOXYcNTbxxgzzxjzrjEm0SH1fgoM8zx24W4pOfmzza1ex3221tovgXs9T2sDh3M5f9hrzafeIvlsnRbWScARv+cZxhgndNWcAF4CrsL9q897nm1eKUA5wli/tfZzIM1vk8vzRZJXfd7twY4NqSD1LgWesNZ2AjYBw51Qr7X2mLU2xfOP8DPcLSrHfra51OvIz9ZTb7ox5gPgVeCjXM7viFpzqbfIPlunhfVRwH9N+ihrbXq4ivGzHvi355tyPe7/ERX89ifi/pZ1Uv3+fWG51efdHuzYojbJWrvc+xhoiUPqNcbUAmYDE6y1E3M5vyNqhaD1OvazBbDW3g40wN0fXCrI+R1TK+Sod0ZRfbZOC+uFQG8AY8zlwKrwluPzZzz9z8aY6kBp4Lgx5mJjjAt3i3s+zqr/R2NMF8/jq8mu7ypjTJQx5iLcXyb7czm2qE03xrTxPO4OLMcB9RpjqgAzgCetteM9mx372eZSr1M/21uNMYM8T0/gDrMfHPzZBqv3i6L6bJ3QxeBvEnClMWYR7v62O8Ncj9e7wPvGmAW4r+T+Gff/qI9wD2KfYa393hizDOfU/xjwtjEmDlgLfGatzTDGzAcW4/6ifiC3Y8NQ7/3Aq8aYNGAPcK+19qgD6h0MXAAMM8Z4+4L7A/906GcbrN5HgbEO/Gy/AN4zxszDPapigOecTv17G6ze7RTR31vdFCMiEgGc1g0iIiJBKKxFRCKAwlpEJAIorEVEIoDCWkQkAiisRUQigMJaRCQCKKxFRCLA/wNkwdZG+aidygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1070cb3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EDwaEhUq EeDU:\n",
      "It\n",
      "uubylrh eatte awp, Tevul tiwhee, vhar lotle\n",
      "weblh neet ouoKo\n",
      "iOh ous ri fake ticr \n",
      "s beer, ous pey, yop koe dtey oryhsh sheeer ho srfutf\n",
      "thor a eevhenr co rid: po noimf 'av wo mrtt\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=100, learning_rate=0.1, sequence_length=25, layers=2)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.training_loop(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement LSTM_Step using forward_backward from other code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No layers, just:\n",
    "\n",
    "* Initialize LSTM Params\n",
    "* Write LSTM Model - sticking close to working example\n",
    "* Write LSTM Layer - sticking close to working example - use `t` to index time and use \"forward-backward\" functions.\n",
    "* Write LSTM node\n",
    "* Write \"sample\" fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, h_prev, C_prev):\n",
    "\n",
    "        self.z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(z, self.params.W_f.value) + self.params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(z, self.params.W_i.value) + self.params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(z, self.params.W_c.value) + self.params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * C_bar\n",
    "        o = sigmoid(np.dot(z, self.params.W_o.value) + self.params.B_o.value)\n",
    "        h = o * tanh(C)\n",
    "\n",
    "        v = np.dot(h, self.params.W_v.value) + self.params.B_v.value\n",
    "        y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "        return z, f, i, C_bar, C, o, h, v, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        \n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        \n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        \n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dh_prev = dz[:, self.vocab_size:]\n",
    "\n",
    "        dC_prev = self.f * dC\n",
    "\n",
    "        return dx_prev, dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, learning_rate):\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward_backward(self, inputs, targets):\n",
    "\n",
    "        # To store the values for each time step\n",
    "        x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "        C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "        v_s, y_s =  {}, {}\n",
    "\n",
    "        # Values at t - 1\n",
    "        h_s[-1] = np.copy(self.start_H)\n",
    "        C_s[-1] = np.copy(self.start_C)\n",
    "\n",
    "        loss = 0\n",
    "        # Loop through time steps\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            x_s[t] = np.zeros((1, self.vocab_size))\n",
    "            \n",
    "            x_s[t][0, inputs[t]] = 1 # Input character\n",
    "\n",
    "            (z_s[t], f_s[t], i_s[t],\n",
    "            C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "            v_s[t], y_s[t]) = \\\n",
    "                self.forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "\n",
    "            # The 0 included only because y_s is 2 dimensional (since we are using batch size 1)\n",
    "            loss += -np.log(y_s[t][0, targets[t]]) # Loss for at t\n",
    "\n",
    "        self.params.clear_gradients()\n",
    "\n",
    "        dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "        dC_next = np.zeros_like(C_s[0]) #dc from the next character\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            # Backward pass\n",
    "            dh_next, dC_next = \\\n",
    "                self.backward(target = targets[t], dh_next = dh_next,\n",
    "                         dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                         z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                         C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                         y = y_s[t])\n",
    "\n",
    "        self.params.clip_gradients()\n",
    "\n",
    "        self.start_H = h_s[len(inputs) - 1]\n",
    "        self.start_C = C_s[len(inputs) - 1]\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x, h_prev, C_prev):\n",
    "        assert x.shape == (1, self.vocab_size)\n",
    "        assert h_prev.shape == (1, self.hidden_size)\n",
    "        assert C_prev.shape == (1, self.hidden_size)\n",
    "\n",
    "        z = np.column_stack((x, h_prev))\n",
    "        \n",
    "        f = sigmoid(np.dot(z, self.params.W_f.value) + self.params.B_f.value)\n",
    "        i = sigmoid(np.dot(z, self.params.W_i.value) + self.params.B_i.value)\n",
    "        C_bar = tanh(np.dot(z, self.params.W_c.value) + self.params.B_c.value)\n",
    "\n",
    "        C = f * C_prev + i * C_bar\n",
    "        o = sigmoid(np.dot(z, self.params.W_o.value) + self.params.B_o.value)\n",
    "        h = o * tanh(C)\n",
    "\n",
    "        v = np.dot(h, self.params.W_v.value) + self.params.B_v.value\n",
    "        y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "        return z, f, i, C_bar, C, o, h, v, y\n",
    "\n",
    "\n",
    "    def backward(self, target, dh_next, dC_next, C_prev,\n",
    "                 z, f, i, C_bar, C, o, h, v, y):\n",
    "\n",
    "        assert z.shape == (1, self.vocab_size + self.hidden_size)\n",
    "        assert v.shape == (1, self.vocab_size)\n",
    "        assert y.shape == (1, self.vocab_size)\n",
    "    \n",
    "        for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "            assert param.shape == (1, self.hidden_size)\n",
    "\n",
    "        dv = np.copy(y)\n",
    "        dv[0, target] -= 1\n",
    "\n",
    "        self.params.W_v.deriv += np.dot(h.T, dv)\n",
    "        self.params.B_v.deriv += dv\n",
    "\n",
    "        dh = np.dot(dv, self.params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C)\n",
    "        do = dsigmoid(o) * do\n",
    "        self.params.W_o.deriv += np.dot(z.T, do)\n",
    "        self.params.B_o.deriv += do\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o * dtanh(tanh(C))\n",
    "        dC_bar = dC * i\n",
    "        dC_bar = dtanh(C_bar) * dC_bar\n",
    "        self.params.W_c.deriv += np.dot(z.T, dC_bar)\n",
    "        self.params.B_c.deriv += dC_bar\n",
    "\n",
    "        di = dC * C_bar\n",
    "        di = dsigmoid(i) * di\n",
    "        self.params.W_i.deriv += np.dot(z.T, di)\n",
    "        self.params.B_i.deriv += di\n",
    "\n",
    "        df = dC * C_prev\n",
    "        df = dsigmoid(f) * df\n",
    "        self.params.W_f.deriv += np.dot(z.T, df)\n",
    "        self.params.B_f.deriv += df\n",
    "\n",
    "        dz = (np.dot(df, self.params.W_f.value.T)\n",
    "             + np.dot(di, self.params.W_i.value.T)\n",
    "             + np.dot(dC_bar, self.params.W_c.value.T)\n",
    "             + np.dot(do, self.params.W_o.value.T))\n",
    "        dh_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = f * dC\n",
    "        \n",
    "        return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    def __init__(self, text_file, LSTM_Model, sequence_length):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.iterations = 0\n",
    "        self.start_pos = 0\n",
    "        self.sequence_length = sequence_length\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        \n",
    "        x = np.zeros((1, self.vocab_size))\n",
    "        x[0, input_char] = 1\n",
    "        \n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        indices = []\n",
    "        for char in range(sample_length): \n",
    "            _, _, _, _, _, _, _, _, p = sample_model.forward(x, \n",
    "                                                             sample_model.start_H, \n",
    "                                                             sample_model.start_C)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            \n",
    "            x = np.zeros((1, self.vocab_size))\n",
    "            x[0, idx] = 1.0\n",
    "            indices.append(idx)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    def single_step(self):\n",
    "   \n",
    "        inputs, targets = self._generate_inputs_targets(self.start_pos)\n",
    "        \n",
    "        loss = self.model.forward_backward(inputs, targets)\n",
    "            \n",
    "        self.model.params.update_params(self.model.learning_rate)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_loop(self, num_iterations, ):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        \n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if self.start_pos + self.sequence_length > len(self.data):\n",
    "                self.start_pos = 0\n",
    "            \n",
    "            loss = self.single_step()\n",
    "            \n",
    "            self.start_pos += self.sequence_length\n",
    "                \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            self.start_pos += self.sequence_length\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115de05c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oroth aleatr s fotoouts inot y fatost fo owoy  gat we e wat ot.\n",
      "But wit st dot cot it wat aqoutir st t wew; fo y ss outo hatave idato t at w t cait avot sak tanoait ese d ocgre ige ainay fof at aros d\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-121e848a12ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcharacter_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacter_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcharacter_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-176-4df8adcf3c8e>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, num_iterations)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-4df8adcf3c8e>\u001b[0m in \u001b[0;36msingle_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_inputs_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-05b9c0dcdf4a>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdC_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dc from the next character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             dh_next, dC_next =                 self.backward(target = targets[t], dh_next = dh_next,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=100, learning_rate=0.1)\n",
    "character_generator = Character_generator('input.txt', mod, sequence_length=25)\n",
    "character_generator.training_loop(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement LSTM_Step using forward_backward from other code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Brief intro section on what LSTMs are\n",
    "2. Go through coding up the math, step by step\n",
    "3. Go through the theory of how data must flow through them\n",
    "4. Go through the classes\n",
    "\n",
    "5. Bonus: switch to autograd and go over variants such as GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: provide a tutorial that contains, side-by-side:\n",
    "\n",
    "* Visual explanations of what is going on in an LSTM-based network\n",
    "* Math explaining why what is going on is working\n",
    "* Code implementing the math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What LSTMs are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTMs are used to model sequences, one sequence element at a time\n",
    "* Any time the input is a sequence or the output is a sequence, an LSTM can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "* Generating the next character in text\n",
    "    * Input is a corpus of text and (optionally) the sequence of text seen so far.\n",
    "    * Output is the predicted next character\n",
    "* An important component of sequence-to-sequence modeling, used in language translation\n",
    "    * Input is a sequence of text, and the output is the sequence in another language.\n",
    "* Image captioning\n",
    "    * Input is a an image, output is a sequence of text describing the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll cover how LSTMs model sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the forward pass of LSTMs, check out beautiful, comprehensive, and very clear blog post from Chris Olah: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, to truly understand what is going on, it is necessary to see **visuals, math, and code** (or at least pseudo-code) all side by side - when I was first learning about neural nets, I remember reading many blog posts like Chris' that contained beautiful visuals, but it wasn't always clear to me how you'd translate those into code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs, step by detailed step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a vocabulary of 60 characters, and an LSTM layer with 100 nodes. What actually goes on in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: the input vector of length 60 is passed in, along with the cell hidden state of length 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_t$ and $h_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code:** Since a natural way to represent our data is as rows, `X` will be a 1x60 vector, and `H` will be a 1x100 vector. We'll use `np.column_stack` to combine these into a 1x160 vector we'll denote `Z`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`z = np.column_stack((x,h))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, these are concatenated into a vector of length 160, and multiplied together and fed through a sigmoid to create a forget gate $f_t$.\n",
    "\n",
    "$f_t = \\sigma(W_f * [X_t, h_{t-1}])$\n",
    "\n",
    "[insert image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code:** `f = sigmoid(np.dot(W_f, z))`, where $W_f$ is a 160x100 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "\n",
    "Similarly, an input gate is used to decide what of the current input to use: \n",
    "\n",
    "$i_t = \\sigma(W_i * [X_t, h_{t-1}])$.\n",
    "\n",
    "[visual here]\n",
    "\n",
    "**Code:** `i = sigmoid(np.dot(W_i, z))`, where $W_i$ is a 160x100 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:\n",
    "\n",
    "Cell state computation\n",
    "\n",
    "$\\tilde{C_t} = tanh(W_c * [X_t, h_{t-1}])$.\n",
    "\n",
    "[visual here]\n",
    "\n",
    "**Code:** `C_tilde = tanh(np.dot(W_C, z))`, where $W_c$ is a 160x100 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "\n",
    "Compute $ C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} $\n",
    "\n",
    "[visual here]\n",
    "\n",
    "`C = f * C_prev + i * C_tilde`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "\n",
    "Compute $o_t = \\sigma(W_o * [X_t, h_{t-1}])$\n",
    "\n",
    "[visual here]\n",
    "\n",
    "`o = sigmoid(np.dot(W_o, z))`\n",
    "\n",
    "Note: $W_o$ has dimension 160 x 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "\n",
    "Compute $h_t = o_t * tanh(C_t)$\n",
    "\n",
    "`h = o * tanh(C)`\n",
    "\n",
    "[visual here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8:\n",
    "\n",
    "Compute $ V_t = W_V * h_t $ \n",
    "\n",
    "[visual here]\n",
    "\n",
    "**Code:** `v = np.dot(W_v, h) + b_v`\n",
    "\n",
    "Note: $W_v$ has dimension 100 x 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `v` is the output of the LSTM cell - a vector the same size as the input that contains predictions of what the next character will be. \n",
    "\n",
    "These are then compared with some other vector `y` that contains the \"right answer\". \n",
    "\n",
    "The loss is computed as: $L = (y-P) ^ 2$ - in code this would simply be `L = (y-P) ** 2`. \n",
    "\n",
    "The gradient of the loss with respect to the prediction therefore would be $-(y-P)$ - in code `-1.0 * (y-P)` - which is enough to let us begin the backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intro to backwards pass]\n",
    "\n",
    "We'll call the three quantities we pass in `loss_grad`, `dh_next`, and `dC_next`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in our basic neural net, when we did $ W * B = C $, and we were calculating $ \\frac{\\partial{L}}{\\partial{W}} $ and $ \\frac{\\partial{L}}{\\partial{B}} $, using $ \\frac{\\partial{L}}{\\partial{C}} $, it turned out that:\n",
    "\n",
    "$$ \\frac{\\partial{C}}{\\partial{W}} = B^T $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial{C}}{\\partial{B}} = W^T $$\n",
    "\n",
    "So that, for example, because of the chain rule,\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{C}} * \\frac{\\partial{C}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{C}} * B^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, since $ V_t = W_v * h_t $, \n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{V}} * H^T $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{H}} = \\frac{\\partial{L}}{\\partial{V}} * W_v^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, we write:\n",
    "\n",
    "`W_v_deriv += np.dot(self.H.T, loss_grad)`\n",
    "\n",
    "and \n",
    "\n",
    "`dh = np.dot(loss_grad, W_v.T)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the current value of $ \\frac{\\partial{L}}{\\partial{H}} $, we add the gradient from the next time step.\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{H}} = \\frac{\\partial{L}}{\\partial{H}} + H_{grad} $$\n",
    "\n",
    "**Code:** `dh += dh_next`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Next we want to compute\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{O}} $$\n",
    "\n",
    "We have both $h_t = o_t * tanh(C_t)$\n",
    "\n",
    "and by the logic presented for step 2, we have:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{O}} = \\frac{\\partial{L}}{\\partial{H}} * tanh(C_t) $$\n",
    "\n",
    "**Code:** `do = dh * tanh(self.C)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5 \n",
    "\n",
    "Call $ W_o * [X_t, h_{t-1}] = p$. To continue \"moving down the chain\", we want to compute\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{P}} $$ \n",
    "\n",
    "We know that since $ o_t = \\sigma(p) $, so the derivative of this function is $ \\sigma(p) * (1 - \\sigma(p)) $. Thus\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{P}} = \\frac{\\partial{L}}{\\partial{O}} * \\sigma(p) * (1 - \\sigma(p)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Similarly to step 2, we have\n",
    "\n",
    "$ W_o * [X_t, h_{t-1}] = p $\n",
    "\n",
    "We'll see this quantity $[X_t, h_{t-1}]$ quite a bit, so we'll call it $z_t$.\n",
    "\n",
    "Since \n",
    "\n",
    "$ W_o * z_t = p $\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{z_t}} = \\frac{\\partial{L}}{\\partial{P}} * W_o^T $$\n",
    "\n",
    "(though as we shall see, this is just one component of $\\frac{\\partial{L}}{\\partial{z_t}}$) - and also:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_o}} = \\frac{\\partial{L}}{\\partial{P}} * z^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "\n",
    "Next we want to compute $$ \\frac{\\partial{L}}{\\partial{C_t}} $$, and we have \n",
    "\n",
    "$h_t = o_t * tanh(C_t)$\n",
    "\n",
    "First, we receive $C_{grad}$ from the layer above, so we can start by initializing\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_t}} = C_{grad} $$\n",
    "\n",
    "First, set $ tanh(C_t) = D_t $. Then, using exactly the same logic as in Step 2:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{D_t}} = \\frac{\\partial{L}}{\\partial{H_t}} * o_t $$.\n",
    "\n",
    "Then, since $ D_t = tanh(C_t) $, the derivative of this will be the derivative of the `tanh` function evaluated at $tanh(C_t)$. So, applying the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_t}} = \\frac{\\partial{L}}{\\partial{D_t}} * \\frac{\\partial{D_t}}{\\partial{C_t}} = \\frac{\\partial{L}}{\\partial{H_t}} * o_t * tanh'(tanh(C_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Next we want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{C_t}}} $$\n",
    "\n",
    "Since $C_t = \\tilde{C_t} * i_t$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$ \\frac{\\partial{L}}{\\partial{\\tilde{C_t}}} = \\frac{\\partial{L}}{\\partial{C}} * i_t $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.5\n",
    "\n",
    "Next we want to ultimately compute $$ \\frac{\\partial{L}}{\\partial{W_c}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ E_t = W_c * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{E_t}} $.\n",
    "\n",
    "We have $ \\tilde{C_t} = tanh(E_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{E_t}} = \\frac{\\partial{L}}{\\partial{C_t}} * tanh'(tanh(E_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "Next, we'll compute $$ \\frac{\\partial{L}}{\\partial{W_c}} $$. Similarly to Step 5, since:\n",
    "\n",
    "$$ E_t = W_c * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_c}} = \\frac{\\partial{L}}{\\partial{E_t}} * z^T $$\n",
    "\n",
    "(multiplying by the transpose of $z$ since these are again matrix multiplications), and \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z}} = \\frac{\\partial{L}}{\\partial{E_t}} * W_c^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "\n",
    "Step 9 follows Step 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{i_t}}} $$\n",
    "\n",
    "Since $C_t = \\tilde{C_t} * i_t$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$ \\frac{\\partial{L}}{\\partial{\\tilde{i_t}}} = \\frac{\\partial{L}}{\\partial{C}} * \\tilde{C_t} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9.5\n",
    "\n",
    "Similarly, step 9.5 follows Step 7.5. We ultimately want to compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ J_t = W_i * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{J_t}} $.\n",
    "\n",
    "We have $i_t  = \\sigma(J_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{J_t}} = \\frac{\\partial{L}}{\\partial{i_t}} * \\sigma(J_t) * (1-\\sigma(J_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10\n",
    "\n",
    "Step 10 is identical to Step 8: we can now compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "Similarly to Step 5, since:\n",
    "\n",
    "$$ J_t = W_i * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_i}} = \\frac{\\partial{L}}{\\partial{J_t}} * z^T $$\n",
    "\n",
    "and, adding to rolling sum of $ \\frac{\\partial{L}}{\\partial{Z}} $, \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{J_t}} * W_i^t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11\n",
    "\n",
    "Step 11 follows Step 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{\\tilde{f_t}}} $$\n",
    "\n",
    "Since $ C_t = \\tilde{f_t} * C_{prev}$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{f_t}} = \\frac{\\partial{L}}{\\partial{C_t}} * C_{prev} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11.5\n",
    "\n",
    "Similarly, step 11.5 follows step 7.5. We want to ultimately compute $$ \\frac{\\partial{L}}{\\partial{W_f}} $$. \n",
    "\n",
    "Following Step 4, we'll first define an intermediate quantity $ G_t = W_f * [X_t, h_{t-1}]$, and compute $ \\frac{\\partial{L}}{\\partial{G_t}} $.\n",
    "\n",
    "We have $ f_t = \\sigma(G_t)$, so \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{G_t}} = \\frac{\\partial{L}}{\\partial{f_t}} * \\sigma(G_t) * (1-\\sigma(G_t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12\n",
    "\n",
    "Step 12 is identical to Steps 8 and 10: we can now compute $$ \\frac{\\partial{L}}{\\partial{W_i}} $$. \n",
    "\n",
    "Similarly to Step 5, since:\n",
    "\n",
    "$$ G_t = W_f * z_t $$\n",
    "\n",
    "We have both:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W_f}} = \\frac{\\partial{L}}{\\partial{G_t}} * z^T $$\n",
    "\n",
    "and, adding to rolling sum of $ \\frac{\\partial{L}}{\\partial{Z_t}} $, \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{G_t}} * W_f^t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13\n",
    "\n",
    "Step 13 follows steps 11 and 7. We want to compute $$ \\frac{\\partial{L}}{\\partial{C_{prev}}} $$\n",
    "\n",
    "Since $ C_t = f_t * C_{prev}$, where this represents an element-wise multiplication, we have simply:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{C_{prev}}} = \\frac{\\partial{L}}{\\partial{C_t}} * f_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14\n",
    "\n",
    "Luckily, this isn't really a step, since we've alread computed the four components of \n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{Z_t}} = \\frac{\\partial{L}}{\\partial{G_t}} * W_f^t + \\frac{\\partial{L}}{\\partial{J_t}} * W_i^t + \\frac{\\partial{L}}{\\partial{E_t}} * W_c^T + \\frac{\\partial{L}}{\\partial{P}} * W_o^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15\n",
    "\n",
    "On the forward pass, $Z_t$ was simply made up of $X_t$ and $h_{t-1}$ concatenated. So, we can select the appropriate elements of $ \\frac{\\partial{L}}{\\partial{Z_t}} $ to make up $ \\frac{\\partial{L}}{\\partial{X_t}} $ and $ \\frac{\\partial{L}}{\\partial{h_{t-1}}} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Flows Through Them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That explains how data flows through individual LSTM cells. Again, we feed in:\n",
    "\n",
    "* A vector of size (`vocab_size`)\n",
    "* The hidden state vector from the prior time step\n",
    "* The cell state vector from the prior time step\n",
    "\n",
    "We get out:\n",
    "\n",
    "* A vector of size (`vocab_size`)\n",
    "* The hidden state vector to be fed back into this node at the next time step\n",
    "* The cell state vector to be fed back into this node at the next time step\n",
    "\n",
    "Here's a visual of how this works in individual cells:\n",
    "\n",
    "![](img/LSTM_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can this be combined into a neural network architecture? First, here's a visual of how data is passed between just two cells in the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on sequences of length 5\n",
    "\n",
    "How would this work if we have multiple layers though? Let's look at a simple example below: suppose our goal was to model sequences of length 5. We could think of \n",
    "\n",
    "First, the first element of the sequence would be fed into the LSTM cell. As described above, this cell would send the (finish this)\n",
    "\n",
    "![](img/lstm_data_flow_1.png)\n",
    "\n",
    "The data could also flow \"vertically\", with each character flowing through all the layers before flowing through to the next character (see visual below)\n",
    "\n",
    "![](img/lstm_data_flow_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding this should easily help you see how data should flow _backwards_ through an LSTM-based network. This is often explained as a separate algorithm called the \"Backpropagation Through Time Algorithm (BPTT)\". Whether or not you think it worthy of that moniker, what is happening is simply this:\n",
    "\n",
    "![](img/lstm_backward_1.png)\n",
    "\n",
    "Again, we could equivalently flow the data through like this:\n",
    "\n",
    "![](img/lstm_backward_2.png)\n",
    "\n",
    "But, these are equivalent, and coding up the first one turns out to be easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * ya\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)) #softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

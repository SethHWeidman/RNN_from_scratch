{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ([char_to_idx[ch] \n",
    "           for ch in data[pointer: pointer + seq_len]])\n",
    "targets = ([char_to_idx[ch] \n",
    "            for ch in data[pointer + 1: pointer + seq_len + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29, 42]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_model_input(sequence, vocab_size):\n",
    "    x = {}\n",
    "    for t in range(len(sequence)):\n",
    "        x[t] = np.zeros((1, vocab_size))\n",
    "        x[t][0, inputs[t]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sequence_to_model_input(inputs, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will output a 62 x 1 output. We _want_ it to output 0s for everything, but 1s for the correct target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_H = np.zeros(hidden_size)\n",
    "        self.start_C = np.zeros(hidden_size)\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.forward_step = 0\n",
    "        \n",
    "    def forward(self, x_batch, first_iter=False):\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "#         import pdb; pdb.set_trace()\n",
    "        batch_size = x_batch.shape[0]\n",
    "        x_out = x_batch\n",
    "        if first_iter:\n",
    "            h_in = np.zeros((batch_size, self.hidden_size))\n",
    "            c_in = np.zeros((batch_size, self.hidden_size))\n",
    "        else:\n",
    "            h_in = self.nodes[0].H\n",
    "            c_in = self.nodes[0].C            \n",
    "        for node in self.nodes:\n",
    "            x_out, h_in, c_in = node.forward(x_out, h_in, c_in, self.params)\n",
    "        self.forward_step += 1\n",
    "        return x_out\n",
    "\n",
    "    \n",
    "    def loss(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return (y_batch - prediction) ** 2\n",
    "\n",
    "    \n",
    "    def loss_gradient(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return -1.0 * (y_batch - prediction)\n",
    "            \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        Implements the \"Backpropagation Through Time\" algorithm.\n",
    "        For each step in the sequence T moving backwards, backpropagate through some number\n",
    "        K nodes\n",
    "        \n",
    "        '''\n",
    "        # Initialize h_grad and c_grad\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        batch_size = loss_grad.shape[0]\n",
    "        H_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        C_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        Y_grad = loss_grad        \n",
    "        T = self.sequence_length - 1\n",
    "        K = 5 # BPTT length\n",
    "        \n",
    "        # BPTT\n",
    "        num_iterations = T - K\n",
    "        for n in range(num_iterations):\n",
    "            for t in range(T-n, T-K-n, -1):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                Y_grad, H_grad, C_grad = \\\n",
    "                    self.nodes[t-n].backward(Y_grad, H_grad, C_grad, self.params)\n",
    "        \n",
    "#         self.start_H = H_grad\n",
    "#         self.start_C = C_grad\n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x, y):\n",
    "        prediction = self.forward(x, first_iter=True)\n",
    "        loss_gradient = self.loss_gradient(x, y)\n",
    "        self.backward(loss_gradient)\n",
    "        self.params.update_params(self.learning_rate)\n",
    "        return\n",
    "\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, epochs):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, X_input, H_input, C_input, LSTM_params):\n",
    "\n",
    "        self.Z = np.column_stack((X_input, H_input))\n",
    "\n",
    "        # Step 2\n",
    "        self.F_int = np.dot(self.Z, LSTM_params.W_F)\n",
    "        self.F = sigmoid(self.F_int)\n",
    "\n",
    "        # Step 3\n",
    "        self.I_int = np.dot(self.Z, LSTM_params.W_I)\n",
    "        self.I = sigmoid(self.I_int)\n",
    "\n",
    "        # Step 4\n",
    "        self.C_prop_int = np.dot(self.Z, LSTM_params.W_C)\n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "\n",
    "        # Step 5 \n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.C_out = self.F * C_input + self.I * self.C_prop\n",
    "\n",
    "        # Step 6\n",
    "        self.O_int = np.dot(self.Z, LSTM_params.W_O)\n",
    "        self.O = sigmoid(self.O_int)\n",
    "\n",
    "        # Step 7\n",
    "        self.C_act = tanh(self.C_out)\n",
    "        self.H_out = self.O * self.C_act\n",
    "\n",
    "        # Step 8\n",
    "        self.X_out = np.dot(self.H_out, LSTM_params.W_V)\n",
    "\n",
    "        return self.X_out, self.H_out, self.C_out\n",
    "\n",
    "\n",
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(self.H_out.T, Y_grad)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(Y_grad, LSTM_params.W_V_diff.T)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.Z.T, self.O_int_diff)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(self.C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.Z.T, self.C_prop_int_diff)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.Z.T, self.I_int_diff)\n",
    "        self.Z_diff += np.dot(self.I_int, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C_out\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.Z.T, self.F_int)        \n",
    "        self.Z_diff += np.dot(self.F_int, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z[:, :self.vocab_size]\n",
    "        H_grad = self.Z[:, self.vocab_size:,]        \n",
    "\n",
    "        return X_grad, H_grad, C_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_F = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_I = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_C = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_O = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_V = np.random.normal(size=(self.hidden_size, self.vocab_size))\n",
    "        \n",
    "        self.W_F_diff = np.zeros_like(self.W_F)\n",
    "        self.W_I_diff = np.zeros_like(self.W_I)\n",
    "        self.W_C_diff = np.zeros_like(self.W_C)\n",
    "        self.W_O_diff = np.zeros_like(self.W_O)\n",
    "        self.W_V_diff = np.zeros_like(self.W_V)\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.W_F -= learning_rate * self.W_F_diff\n",
    "        self.W_I -= learning_rate * self.W_I_diff\n",
    "        self.W_C -= learning_rate * self.W_C_diff\n",
    "        self.W_O -= learning_rate * self.W_O_diff\n",
    "        self.W_V -= learning_rate * self.W_V_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - x * x\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LSTM_Model(sequence_length=20, vocab_size=62, hidden_size=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29, 42]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text_data):\n",
    "    '''\n",
    "    Returns data ready to feed\n",
    "    '''\n",
    "    chars = list(set(text_data))\n",
    "    data_size, X_size = len(text_data), len(chars)\n",
    "    print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "    char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "    idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "    \n",
    "    inputs = ([char_to_idx[ch] \n",
    "           for ch in data[pointer: pointer + seq_len]])\n",
    "    targets = ([char_to_idx[ch] \n",
    "                for ch in data[pointer + 1: pointer + seq_len + 1]])\n",
    "    \n",
    "    # TODO: yield inputs and targets\n",
    "    # https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_model_input(sequence, vocab_size):\n",
    "    x = {}\n",
    "    for t in range(len(sequence)):\n",
    "        x[t] = np.zeros((1, vocab_size))\n",
    "        x[t][0, inputs[t]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sequence_to_model_input(inputs, 62)\n",
    "y = sequence_to_model_input(targets, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.single_step(t[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

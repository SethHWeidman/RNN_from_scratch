{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ([char_to_idx[ch] \n",
    "           for ch in data[pointer: pointer + seq_len]])\n",
    "targets = ([char_to_idx[ch] \n",
    "            for ch in data[pointer + 1: pointer + seq_len + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_model_input(sequence, vocab_size):\n",
    "    x = {}\n",
    "    for t in range(len(sequence)):\n",
    "        x[t] = np.zeros((1, vocab_size))\n",
    "        x[t][0, inputs[t]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sequence_to_model_input(inputs, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will output a 62 x 1 output. We _want_ it to output 0s for everything, but 1s for the correct target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    sum_exp = np.sum(np.exp(x), axis=1)\n",
    "    new_matrix = exp / sum_exp[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(prediction, y):\n",
    "    return 0.5 * (prediction - y) ** 2\n",
    "\n",
    "\n",
    "def cross_entropy(prediction, y):\n",
    "    return 0.5 * (prediction - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_H = np.zeros(hidden_size)\n",
    "        self.start_C = np.zeros(hidden_size)\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.forward_step = 0\n",
    "        \n",
    "    def forward(self, x_batch, first_iter=False):\n",
    "\n",
    "        batch_size = x_batch.shape[0]\n",
    "        x_out = x_batch\n",
    "        if first_iter:\n",
    "            h_in = np.zeros((batch_size, self.hidden_size))\n",
    "            c_in = np.zeros((batch_size, self.hidden_size))\n",
    "        else:\n",
    "            h_in = self.nodes[0].H_out\n",
    "            c_in = self.nodes[0].C_out            \n",
    "        for node in self.nodes:\n",
    "            x_out, h_in, c_in = node.forward(x_out, h_in, c_in, self.params)\n",
    "        self.forward_step += 1\n",
    "        return x_out\n",
    "    \n",
    "    def loss(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return 0.5 * (y_batch - prediction) ** 2\n",
    "\n",
    "    \n",
    "    def loss_gradient(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return -1.0 * (y_batch - prediction)\n",
    "            \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        Implements the \"Backpropagation Through Time\" algorithm.\n",
    "        For each step in the sequence T moving backwards, backpropagate through some number\n",
    "        K nodes\n",
    "        \n",
    "        '''\n",
    "        # Initialize h_grad and c_grad\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        batch_size = loss_grad.shape[0]\n",
    "        H_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        C_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        Y_grad = loss_grad        \n",
    "        T = self.sequence_length - 1\n",
    "        K = 10 # BPTT length\n",
    "        \n",
    "        # BPTT\n",
    "        num_iterations = T - K\n",
    "        for n in range(num_iterations):\n",
    "            for t in range(T-n, T-K-n, -1):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                Y_grad, H_grad, C_grad = \\\n",
    "                    self.nodes[t-n].backward(Y_grad, H_grad, C_grad, self.params)\n",
    "    \n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x, y, first_iter):\n",
    "        prediction = self.forward(x, first_iter=first_iter)\n",
    "        p_softmax = softmax(prediction)\n",
    "        loss_gradient = self.loss_gradient(p_softmax, y)\n",
    "        self.backward(loss_gradient)\n",
    "        self.params.update_params(self.learning_rate)\n",
    "        self.params.clear_gradients()\n",
    "\n",
    "        \n",
    "    def loss_batch(self, x, y):\n",
    "        prediction = self.forward(x, first_iter=False)\n",
    "        p_softmax = softmax(prediction)\n",
    "        return np.sum(self.loss(p_softmax, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, X_input, H_input, C_input, LSTM_params):\n",
    "\n",
    "        self.Z = np.column_stack((X_input, H_input))\n",
    "\n",
    "        # Step 2\n",
    "        self.F_int = np.dot(self.Z, LSTM_params.W_F)\n",
    "        self.F = sigmoid(self.F_int)\n",
    "\n",
    "        # Step 3\n",
    "        self.I_int = np.dot(self.Z, LSTM_params.W_I)\n",
    "        self.I = sigmoid(self.I_int)\n",
    "\n",
    "        # Step 4\n",
    "        self.C_prop_int = np.dot(self.Z, LSTM_params.W_C)\n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "\n",
    "        # Step 5 \n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.C_out = self.F * C_input + self.I * self.C_prop\n",
    "\n",
    "        # Step 6\n",
    "        self.O_int = np.dot(self.Z, LSTM_params.W_O)\n",
    "        self.O = sigmoid(self.O_int)\n",
    "\n",
    "        # Step 7\n",
    "        self.C_act = tanh(self.C_out)\n",
    "        self.H_out = self.O * self.C_act\n",
    "\n",
    "        # Step 8\n",
    "        self.X_out = np.dot(self.H_out, LSTM_params.W_V)\n",
    "\n",
    "        return self.X_out, self.H_out, self.C_out\n",
    "\n",
    "\n",
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(self.H_out.T, Y_grad)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(Y_grad, LSTM_params.W_V_diff.T)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.Z.T, self.O_int_diff)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(self.C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.Z.T, self.C_prop_int_diff)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.Z.T, self.I_int_diff)\n",
    "        self.Z_diff += np.dot(self.I_int, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C_out\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.Z.T, self.F_int)        \n",
    "        self.Z_diff += np.dot(self.F_int, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z_diff[:, :self.vocab_size]\n",
    "        H_grad = self.Z_diff[:, self.vocab_size:,]        \n",
    "\n",
    "        return X_grad, H_grad, C_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_F = np.random.normal(size=(self.stack_size, self.hidden_size), scale=0.1)\n",
    "        self.W_I = np.random.normal(size=(self.stack_size, self.hidden_size), scale=0.1)\n",
    "        self.W_C = np.random.normal(size=(self.stack_size, self.hidden_size), scale=0.1)\n",
    "        self.W_O = np.random.normal(size=(self.stack_size, self.hidden_size), scale=0.1)\n",
    "        self.W_V = np.random.normal(size=(self.hidden_size, self.vocab_size), scale=0.1)\n",
    "        \n",
    "        self.W_F_diff = np.zeros_like(self.W_F)\n",
    "        self.W_I_diff = np.zeros_like(self.W_I)\n",
    "        self.W_C_diff = np.zeros_like(self.W_C)\n",
    "        self.W_O_diff = np.zeros_like(self.W_O)\n",
    "        self.W_V_diff = np.zeros_like(self.W_V)\n",
    "\n",
    "    def clear_gradients(self):\n",
    "        self.W_F_diff = np.zeros_like(self.W_F)\n",
    "        self.W_I_diff = np.zeros_like(self.W_I)\n",
    "        self.W_C_diff = np.zeros_like(self.W_C)\n",
    "        self.W_O_diff = np.zeros_like(self.W_O)\n",
    "        self.W_V_diff = np.zeros_like(self.W_V) \n",
    "        \n",
    "    def clip_gradients(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.W_F_diff = np.clip(self.W_F_diff, -1, 1)\n",
    "        self.W_I_diff = np.clip(self.W_I_diff, -1, 1)\n",
    "        self.W_C_diff = np.clip(self.W_C_diff, -1, 1)\n",
    "        self.W_O_diff = np.clip(self.W_O_diff, -1, 1)\n",
    "        self.W_V_diff = np.clip(self.W_V_diff, -1, 1)        \n",
    "\n",
    "        \n",
    "    def update_params(self, learning_rate):\n",
    "        self.clip_gradients()\n",
    "        self.W_F -= learning_rate * self.W_F_diff\n",
    "        self.W_I -= learning_rate * self.W_I_diff\n",
    "        self.W_C -= learning_rate * self.W_C_diff\n",
    "        self.W_O -= learning_rate * self.W_O_diff\n",
    "        self.W_V -= learning_rate * self.W_V_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - x * x\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    \n",
    "    def __init__(self, text_data, model):\n",
    "        self.data = text_data\n",
    "        self.model = model\n",
    "        self.chars = list(set(data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        \n",
    "    def generate_sequences(self, start_pos, seq_length):\n",
    "        input_sequence = ([self.char_to_idx[ch] \n",
    "                           for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        target_sequence = ([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "    def sequence_to_model_input(self, sequence, vocab_size):\n",
    "        out_batch = np.zeros((len(sequence), vocab_size))\n",
    "        for i, el in enumerate(sequence):\n",
    "            out_batch[i, el] = 1        \n",
    "        return out_batch\n",
    "    \n",
    "    def generate_batch(self, start_pos):\n",
    "        input_sequence, target_sequence = self.generate_sequences(start_pos, self.model.sequence_length)\n",
    "        return self.sequence_to_model_input(input_sequence, self.vocab_size), \\\n",
    "            self.sequence_to_model_input(target_sequence, self.vocab_size) \n",
    "    \n",
    "    def train(self, steps, check_every):\n",
    "        start_pos = 0\n",
    "        iterations = 0\n",
    "        while iterations < steps:\n",
    "#             print(iterations)\n",
    "            x_batch, y_batch = self.generate_batch(start_pos)\n",
    "            first_iter = True if iterations == 0 else False\n",
    "            self.model.single_step(x_batch, y_batch, first_iter)\n",
    "            if iterations % check_every == 0:\n",
    "                print(\"Loss\", self.model.loss_batch(x_batch, y_batch))\n",
    "            \n",
    "            start_pos += self.model.sequence_length\n",
    "            iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 9.838714677035817\n",
      "Loss 9.861503979482947\n",
      "Loss 9.851692886468388\n",
      "Loss 9.870825551389022\n",
      "Loss 9.91548118851625\n",
      "Loss 9.785042160485036\n",
      "Loss 9.879646720258963\n",
      "Loss 9.833550171084518\n",
      "Loss 9.9501021406856\n",
      "Loss 9.993634088249117\n",
      "Loss 9.931728947035257\n",
      "Loss 9.850492445212154\n",
      "Loss 10.200210780873633\n",
      "Loss 9.948110483708186\n",
      "Loss 10.137226196999276\n",
      "Loss 10.368644834289348\n",
      "Loss 10.140780125165707\n",
      "Loss 10.169146463271229\n",
      "Loss 10.291191922965247\n",
      "Loss 10.066157038462649\n",
      "Loss 10.022227017294323\n",
      "Loss 9.875331272596164\n",
      "Loss 10.239049141513501\n",
      "Loss 10.155944657549494\n",
      "Loss 10.64247712026867\n",
      "Loss 10.386408672751411\n",
      "Loss 10.369429733816876\n",
      "Loss 10.788505941604727\n",
      "Loss 10.534249954942588\n",
      "Loss 10.53108930804882\n",
      "Loss 10.441781352029018\n",
      "Loss 10.319054249203006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-418838df2125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcharacter_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacter_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcharacter_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-154-20b9c0ddb7b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps, check_every)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mfirst_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcheck_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-7aaf0bd41ba7>\u001b[0m in \u001b[0;36msingle_step\u001b[0;34m(self, x, y, first_iter)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mp_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mloss_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-7aaf0bd41ba7>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss_grad)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#                 import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mY_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_grad\u001b[0m \u001b[0;34m=\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-61b3f89c5de7>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, Y_grad, H_grad, C_grad, LSTM_params)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mLSTM_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_I_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI_int_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_I\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "mod = LSTM_Model(sequence_length=20, vocab_size=62, hidden_size=100, learning_rate=0.1)\n",
    "character_generator = Character_generator(data, mod)\n",
    "character_generator.train(10000, check_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget Stanford CS231: reproduce the results from the `numpy_LSTM` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

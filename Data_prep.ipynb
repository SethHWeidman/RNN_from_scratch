{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ([char_to_idx[ch] \n",
    "           for ch in data[pointer: pointer + seq_len]])\n",
    "targets = ([char_to_idx[ch] \n",
    "            for ch in data[pointer + 1: pointer + seq_len + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 30, 53, 29, 42, 52, 19, 19, 13, 42, 56, 19, 6, 53, 22, 2, 52, 53, 29, 42]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_model_input(sequence, vocab_size):\n",
    "    x = {}\n",
    "    for t in range(len(sequence)):\n",
    "        x[t] = np.zeros((1, vocab_size))\n",
    "        x[t][0, inputs[t]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sequence_to_model_input(inputs, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will output a 62 x 1 output. We _want_ it to output 0s for everything, but 1s for the correct target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    An LSTM model with one LSTM layer that feeds data through it and generates an output.\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        Initialize list of nodes of length the sequence length\n",
    "        List the vocab size and the hidden size \n",
    "        Initialize the params\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_H = np.zeros(hidden_size)\n",
    "        self.start_C = np.zeros(hidden_size)\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.forward_step = 0\n",
    "        \n",
    "    def forward(self, x_batch, first_iter=False):\n",
    "\n",
    "        batch_size = x_batch.shape[0]\n",
    "        x_out = x_batch\n",
    "        if first_iter:\n",
    "            h_in = np.zeros((batch_size, self.hidden_size))\n",
    "            c_in = np.zeros((batch_size, self.hidden_size))\n",
    "        else:\n",
    "            h_in = self.nodes[0].H_out\n",
    "            c_in = self.nodes[0].C_out            \n",
    "        for node in self.nodes:\n",
    "            x_out, h_in, c_in = node.forward(x_out, h_in, c_in, self.params)\n",
    "        self.forward_step += 1\n",
    "        return x_out\n",
    "\n",
    "    \n",
    "    def loss(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return (y_batch - prediction) ** 2\n",
    "\n",
    "    \n",
    "    def loss_gradient(self, prediction, y_batch):\n",
    "        '''\n",
    "        Return a gradient: how much our prediction influences how much we \"missed\" by.\n",
    "        '''\n",
    "        return -1.0 * (y_batch - prediction)\n",
    "            \n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        Implements the \"Backpropagation Through Time\" algorithm.\n",
    "        For each step in the sequence T moving backwards, backpropagate through some number\n",
    "        K nodes\n",
    "        \n",
    "        '''\n",
    "        # Initialize h_grad and c_grad\n",
    "        # Initialize x_in, h_in, c_in, run em through\n",
    "        batch_size = loss_grad.shape[0]\n",
    "        H_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        C_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        Y_grad = loss_grad        \n",
    "        T = self.sequence_length - 1\n",
    "        K = 5 # BPTT length\n",
    "        \n",
    "        # BPTT\n",
    "        num_iterations = T - K\n",
    "        for n in range(num_iterations):\n",
    "            for t in range(T-n, T-K-n, -1):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                Y_grad, H_grad, C_grad = \\\n",
    "                    self.nodes[t-n].backward(Y_grad, H_grad, C_grad, self.params)\n",
    "        \n",
    "#         self.start_H = H_grad\n",
    "#         self.start_C = C_grad\n",
    "        return \n",
    "\n",
    "\n",
    "    def single_step(self, x, y, first_iter):\n",
    "        prediction = self.forward(x, first_iter=first_iter)\n",
    "        loss_gradient = self.loss_gradient(x, y)\n",
    "        self.backward(loss_gradient)\n",
    "        self.params.update_params(self.learning_rate)\n",
    "\n",
    "        \n",
    "    def loss_batch(self, x, y):\n",
    "        prediction = self.forward(x, first_iter=False)\n",
    "        return np.sum(self.loss(prediction, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node that takes in input and generates output. \n",
    "    Has a size of its hidden layers and a vocabulary size it expects.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, X_input, H_input, C_input, LSTM_params):\n",
    "\n",
    "        self.Z = np.column_stack((X_input, H_input))\n",
    "\n",
    "        # Step 2\n",
    "        self.F_int = np.dot(self.Z, LSTM_params.W_F)\n",
    "        self.F = sigmoid(self.F_int)\n",
    "\n",
    "        # Step 3\n",
    "        self.I_int = np.dot(self.Z, LSTM_params.W_I)\n",
    "        self.I = sigmoid(self.I_int)\n",
    "\n",
    "        # Step 4\n",
    "        self.C_prop_int = np.dot(self.Z, LSTM_params.W_C)\n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "\n",
    "        # Step 5 \n",
    "        self.C_prop = tanh(self.C_prop_int)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.C_out = self.F * C_input + self.I * self.C_prop\n",
    "\n",
    "        # Step 6\n",
    "        self.O_int = np.dot(self.Z, LSTM_params.W_O)\n",
    "        self.O = sigmoid(self.O_int)\n",
    "\n",
    "        # Step 7\n",
    "        self.C_act = tanh(self.C_out)\n",
    "        self.H_out = self.O * self.C_act\n",
    "\n",
    "        # Step 8\n",
    "        self.X_out = np.dot(self.H_out, LSTM_params.W_V)\n",
    "\n",
    "        return self.X_out, self.H_out, self.C_out\n",
    "\n",
    "\n",
    "    def backward(self, Y_grad, H_grad, C_grad, LSTM_params):\n",
    "        # Initialize the gradient for the words and the hidden layers:\n",
    "        self.Z_diff = np.zeros_like(self.Z)\n",
    "        \n",
    "        # 2\n",
    "        LSTM_params.W_V_diff += np.dot(self.H_out.T, Y_grad)\n",
    "\n",
    "        # 3\n",
    "        self.H_diff = np.dot(Y_grad, LSTM_params.W_V_diff.T)\n",
    "        self.H_diff += H_grad\n",
    "\n",
    "        # 4\n",
    "        self.O_diff = self.H_diff * self.C_act\n",
    "\n",
    "        # 4.5\n",
    "        self.O_int_diff = sigmoid(self.O, deriv=True) * self.O\n",
    "        \n",
    "        # 5\n",
    "        LSTM_params.W_O_diff += np.dot(self.Z.T, self.O_int_diff)\n",
    "        self.Z_diff = np.dot(self.O_int_diff, LSTM_params.W_O.T)\n",
    "        \n",
    "        # 6\n",
    "        self.C_diff = C_grad\n",
    "        self.C_diff += self.H_diff * self.O * tanh(self.C_act, deriv=True)\n",
    "\n",
    "        # 7\n",
    "        self.C_prop_diff = self.C_diff * self.I\n",
    "\n",
    "        # 7.5\n",
    "        self.C_prop_int_diff = tanh(self.C_prop, deriv = True) * self.C_prop_diff\n",
    "\n",
    "        # 8\n",
    "        LSTM_params.W_C_diff += np.dot(self.Z.T, self.C_prop_int_diff)\n",
    "        self.Z_diff += np.dot(self.C_prop_int, LSTM_params.W_C.T)\n",
    "        \n",
    "        # 9\n",
    "        self.I_diff = self.C_diff * self.C_prop\n",
    "\n",
    "        # 9.5\n",
    "        self.I_int_diff = sigmoid(self.I, deriv=True) * self.I_diff\n",
    "\n",
    "        # 10\n",
    "        LSTM_params.W_I_diff += np.dot(self.Z.T, self.I_int_diff)\n",
    "        self.Z_diff += np.dot(self.I_int, LSTM_params.W_I.T)\n",
    "        \n",
    "        # 11\n",
    "        self.F_diff = self.C_diff * self.C_out\n",
    "\n",
    "        # 11.5\n",
    "        self.F_int_diff = sigmoid(self.F, deriv=True) * self.F_diff\n",
    "\n",
    "        # 12\n",
    "        LSTM_params.W_F_diff += np.dot(self.Z.T, self.F_int)        \n",
    "        self.Z_diff += np.dot(self.F_int, LSTM_params.W_F.T)\n",
    "        \n",
    "        # 13\n",
    "        C_grad = self.F * self.C_diff\n",
    "\n",
    "        # 14\n",
    "        X_grad = self.Z[:, :self.vocab_size]\n",
    "        H_grad = self.Z[:, self.vocab_size:,]        \n",
    "\n",
    "        return X_grad, H_grad, C_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_F = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_I = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_C = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_O = np.random.normal(size=(self.stack_size, self.hidden_size))\n",
    "        self.W_V = np.random.normal(size=(self.hidden_size, self.vocab_size))\n",
    "        \n",
    "        self.W_F_diff = np.zeros_like(self.W_F)\n",
    "        self.W_I_diff = np.zeros_like(self.W_I)\n",
    "        self.W_C_diff = np.zeros_like(self.W_C)\n",
    "        self.W_O_diff = np.zeros_like(self.W_O)\n",
    "        self.W_V_diff = np.zeros_like(self.W_V)\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.W_F -= learning_rate * self.W_F_diff\n",
    "        self.W_I -= learning_rate * self.W_I_diff\n",
    "        self.W_C -= learning_rate * self.W_C_diff\n",
    "        self.W_O -= learning_rate * self.W_O_diff\n",
    "        self.W_V -= learning_rate * self.W_V_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - x * x\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LSTM_Model(sequence_length=20, vocab_size=62, hidden_size=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 49, 50, 30, 39, 61, 10, 36, 36, 20, 61, 47, 36, 8, 30, 35, 1, 10, 30, 39]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 49, 50, 30, 39, 61, 10, 36, 36, 20, 61, 47, 36, 8, 30, 35, 1, 10, 30, 39]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    \n",
    "    def __init__(self, text_data, model):\n",
    "        self.data = text_data\n",
    "        self.model = model\n",
    "        self.chars = list(set(data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        \n",
    "    def generate_sequences(self, start_pos, seq_length):\n",
    "        input_sequence = ([self.char_to_idx[ch] \n",
    "                           for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        target_sequence = ([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos:start_pos + seq_length]])\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "    def sequence_to_model_input(self, sequence, vocab_size):\n",
    "        out_batch = np.zeros((len(sequence), vocab_size))\n",
    "        for i, el in enumerate(sequence):\n",
    "            out_batch[i, el] = 1        \n",
    "        return out_batch\n",
    "    \n",
    "    def generate_batch(self, start_pos):\n",
    "        input_sequence, target_sequence = self.generate_sequences(start_pos, self.model.sequence_length)\n",
    "        return self.sequence_to_model_input(input_sequence, self.vocab_size), \\\n",
    "            self.sequence_to_model_input(target_sequence, self.vocab_size) \n",
    "    \n",
    "    def train(self, steps):\n",
    "        start_pos = 0\n",
    "        iterations = 0\n",
    "        while iterations < steps:\n",
    "            x_batch, y_batch = self.generate_batch(start_pos)\n",
    "            first_iter = True if iterations == 0 else False\n",
    "            self.model.single_step(x_batch, y_batch, first_iter)\n",
    "            if iterations % 100 == 0:\n",
    "                print(\"Loss\", self.model.loss_batch(x_batch, y_batch))\n",
    "            \n",
    "            start_pos += self.model.sequence_length\n",
    "            iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 18533.891628121077\n",
      "Loss 3598.445927655147\n",
      "Loss 11814.897280359244\n",
      "Loss 1002273.0225738513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:97: RuntimeWarning: overflow encountered in add\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-3a4c4108e261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcharacter_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacter_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcharacter_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-a52b23cf65c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfirst_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-91c0cb50abba>\u001b[0m in \u001b[0;36msingle_step\u001b[0;34m(self, x, y, first_iter)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mloss_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-91c0cb50abba>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss_grad)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#                 import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mY_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_grad\u001b[0m \u001b[0;34m=\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#         self.start_H = H_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-78babee87668>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, Y_grad, H_grad, C_grad, LSTM_params)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# 9.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI_int_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = open('input.txt', 'r').read()\n",
    "mod = LSTM_Model(sequence_length=20, vocab_size=62, hidden_size=100, learning_rate=0.00001)\n",
    "character_generator = Character_generator(data, mod)\n",
    "character_generator.train(2000, check_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "* Explore other optimization algorithms other than straightforward SGD. \n",
    "* Use gradient clipping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

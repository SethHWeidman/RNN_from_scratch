{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lunch and Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your current understanding of RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_next_character.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_unrolling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is actually going on?\n",
    "\n",
    "### Example with sequence length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the backwards pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_in_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep learning?\n",
    "\n",
    "### What would an RNN with multiple layers look like?\n",
    "\n",
    "![](img/rnn_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the backward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_olah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Forward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Backward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I used [this minimal example](https://gist.github.com/karpathy/d4dee566867f8291f086) from Andrej Karpathy as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def row_softmax(input_array):    \n",
    "    a_exp = np.exp(input_array)\n",
    "    row_sums = a_exp.sum(axis=1)\n",
    "    new_matrix = a_exp / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    '''\n",
    "    A class that holds weight matrices along with their derivatives and momentum.\n",
    "    '''\n",
    "    def __init__(self, value):\n",
    "        '''\n",
    "        param value: numpy array, two dimensional, shape of the weight matrix\n",
    "        '''\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        '''\n",
    "        Resets the value of the derivative\n",
    "        '''\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        '''\n",
    "        Clips the derivative, setting its min value to -1 and its max value to 1.\n",
    "        '''\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to AdaGrad rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to stochastic gradient descent rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Params` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        The parameters to be used when updating the values in an LSTM_Layer, which is a layer\n",
    "        of LSTM cells stretched out over time.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the layer. This can be \n",
    "        different for each layer.\n",
    "        param vocab_size: int - the number of characters in the vocabulary that we are predicting\n",
    "        the next character of.\n",
    "        Note: the shape of these weight matrices assumes that the data will be fed in as \"rows\", \n",
    "        meaning that each data point will be represented by a numpy array of shape (1, vocab_size)\n",
    "        '''\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        '''\n",
    "        Biases always have the dimensions of the output of the transformation that they are being added to.\n",
    "        '''\n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        '''\n",
    "        Returns a list of all the parameters for easy iteration\n",
    "        '''\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        '''\n",
    "        Clears all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        '''\n",
    "        Clips all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        '''\n",
    "        Updates all the parameters according to the \"AdaGrad\" rule.\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node performing the role of the \"circles\" in the diagrams above.\n",
    "    It takes has two methods:\n",
    "    1. \"forward\" which takes in: \n",
    "        * An observation, \"X\"\n",
    "        * A hidden state from the prior time step, \"H_prev\"\n",
    "        * A cell state from the prior time step, \"C_prev\"\n",
    "        And returns:\n",
    "        * The output \"X_out\" to be passed on to the next layer.\n",
    "        * The output \"H\" to be passed into the next node as the hidden state\n",
    "        * The output \"C\" to be passed into the next node as the cell state\n",
    "    2. \"backward\" which takes in: \n",
    "        * The gradient of the output with respect to the loss, \"out_grad\"\n",
    "        * The gradient of the hidden state with respect to the loss, \"h_grad\"\n",
    "        * The gradient of the cell state with respect to the loss, \"c_grad\"\n",
    "        And returns:\n",
    "        * The gradient of the output of the same time step in the prior layer - the circle \"below\" in the \n",
    "        drawings above - \"dx_grad\"\n",
    "        * The gradient of the hidden state from the prior time step, \"dh_grad\" \n",
    "        * The gradient of the cell state from the prior time step, \"dc_grad\" \n",
    "    \n",
    "    It uses as parameters, not its own parameters, but the parameters from the LSTM_Layer that it is a part of.\n",
    "        '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, H_prev, C_prev, LSTM_Params):\n",
    "        '''\n",
    "        param x: numpy array of shape (1, vocab_size)\n",
    "        param H_prev: numpy array of shape (1, hidden_size)\n",
    "        param C_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.x_out: numpy array of shape (1, vocab_size)\n",
    "        return self.H: numpy array of shape (1, hidden_size)\n",
    "        return self.C: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, H_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.x_out = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.x_out, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    '''\n",
    "    Corresponds to a \"row\" of circles in the diagrams above. \n",
    "    We initialize a layer with a number of \"Nodes\" equal to the length of the sequences we passin.\n",
    "    Again, the key methods are \"forward\" and \"backward\".\n",
    "    1. \"forward\":\n",
    "        * Passes data forward through all the nodes in the layer\n",
    "        * Sets the hidden and cell states outputted by the last node to be those that will be passed in when the\n",
    "        next sequence is fed through the layer\n",
    "        * Passes the appropriate data on to the next layer in the network\n",
    "    2. \"backward\":\n",
    "        * Passes gradient backward through all the nodes in the layer\n",
    "        * Sets the hidden state outputted by the last node equal to the node \n",
    "        * Passes the appropriate gradient on to the previous layer in the network\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_seq_in):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_seq_in.shape[0]\n",
    "        \n",
    "        x_seq_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "\n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            x_in = np.array(x_seq_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, num_layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "\n",
    "    def _generate_one_hot_array(self, sequence):\n",
    "        '''\n",
    "        param sequence - sequence of indices of characters of length sequence_length.\n",
    "        return batch - numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.vocab_size))\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "        \n",
    "        return batch\n",
    "        \n",
    "    def loss(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_in = self._generate_one_hot_array(inputs)\n",
    "        \n",
    "        x_batch_out = self.forward(x_batch_in)\n",
    "        \n",
    "        x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "        \n",
    "        loss = self.loss(x_softmax, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_softmax, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Character_generator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        '''\n",
    "        param text_file: path to text file containing characters that we are going to generate text to mimic\n",
    "        param LSTM_Model: the LSTM_Model that we are using to generate the text \n",
    "        '''\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        '''\n",
    "        Given a start position, generates a list of inputs and targets.\n",
    "        param start_pos: int - index of start position in characters \n",
    "        return inputs: list - list of length sequence_length with the indices of the characters forming the \n",
    "        inputs to the model \n",
    "        return targets: list - list of length sequence_length with the indices of the characters forming the \n",
    "        targets of the model\n",
    "        '''\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_in = self.model._generate_one_hot_array([input_char])\n",
    "            \n",
    "            x_batch_out = sample_model.forward(x_batch_in)\n",
    "        \n",
    "            x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, num_iterations, sample_every=100):\n",
    "        '''\n",
    "        Trains the \"character generator\" for a number of iterations. \n",
    "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
    "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
    "        '''\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs, targets = self._generate_inputs_targets(start_pos)\n",
    "            loss = self.model.single_step(inputs, targets)\n",
    "\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b0b8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -ave inpmear'y thot 'ama ftrein,\n",
      "Theur mand harc Ihem cithy thyehirsst eavrfy listry te ti hue sighincBs tuve hent sor pgotntr bfavein beysurw agpnot you, o'd my epaint\n",
      "yguma:\n",
      "Hat ma toleun!\n",
      "\n",
      "YeUon t\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.01, sequence_length=25, num_layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.train(10000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

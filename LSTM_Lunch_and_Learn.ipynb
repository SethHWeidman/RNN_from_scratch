{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lunch and Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your current understanding of RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_next_character.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_unrolling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is actually going on?\n",
    "\n",
    "### Example with sequence length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the backwards pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_in_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep learning?\n",
    "\n",
    "### What would an RNN with multiple layers look like?\n",
    "\n",
    "![](img/rnn_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the backward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_olah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Forward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Backward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I used [this minimal example](https://gist.github.com/karpathy/d4dee566867f8291f086) from Andrej Karpathy as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def row_softmax(input_array):    \n",
    "    a_exp = np.exp(input_array)\n",
    "    row_sums = a_exp.sum(axis=1)\n",
    "    new_matrix = a_exp / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    '''\n",
    "    A class that holds weight matrices along with their derivatives and momentum.\n",
    "    '''\n",
    "    def __init__(self, value):\n",
    "        '''\n",
    "        param value: numpy array, two dimensional, shape of the weight matrix\n",
    "        '''\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        '''\n",
    "        Resets the value of the derivative\n",
    "        '''\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        '''\n",
    "        Clips the derivative, setting its min value to -1 and its max value to 1.\n",
    "        '''\n",
    "        self.deriv = np.clip(self.deriv, -2, 2, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to AdaGrad rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to stochastic gradient descent rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Params` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        The parameters to be used when updating the values in an LSTM_Layer, which is a layer\n",
    "        of LSTM cells stretched out over time.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the layer. This can be \n",
    "        different for each layer.\n",
    "        param vocab_size: int - the number of characters in the vocabulary that we are predicting\n",
    "        the next character of.\n",
    "        Note: the shape of these weight matrices assumes that the data will be fed in as \"rows\", \n",
    "        meaning that each data point will be represented by a numpy array of shape (1, vocab_size)\n",
    "        '''\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        '''\n",
    "        Biases always have the dimensions of the output of the transformation that they are being added to.\n",
    "        '''\n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        '''\n",
    "        Returns a list of all the parameters for easy iteration\n",
    "        '''\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        '''\n",
    "        Clears all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        '''\n",
    "        Clips all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        '''\n",
    "        Updates all the parameters according to the \"AdaGrad\" rule.\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node performing the role of the \"circles\" in the diagrams above.\n",
    "    It takes has two methods:\n",
    "    1. \"forward\" which takes in: \n",
    "        * An observation, \"X\"\n",
    "        * A hidden state from the prior time step, \"H_prev\"\n",
    "        * A cell state from the prior time step, \"C_prev\"\n",
    "        And returns:\n",
    "        * The output \"X_out\" to be passed on to the next layer.\n",
    "        * The output \"H\" to be passed into the next node as the hidden state\n",
    "        * The output \"C\" to be passed into the next node as the cell state\n",
    "    2. \"backward\" which takes in: \n",
    "        * The gradient of the output with respect to the loss, \"out_grad\"\n",
    "        * The gradient of the hidden state with respect to the loss, \"h_grad\"\n",
    "        * The gradient of the cell state with respect to the loss, \"c_grad\"\n",
    "        And returns:\n",
    "        * The gradient of the output of the same time step in the prior layer - the circle \"below\" in the \n",
    "        drawings above - \"dx_grad\"\n",
    "        * The gradient of the hidden state from the prior time step, \"dh_grad\" \n",
    "        * The gradient of the cell state from the prior time step, \"dc_grad\" \n",
    "    \n",
    "    It uses as parameters, not its own parameters, but the parameters from the LSTM_Layer that it is a part of.\n",
    "        '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, H_prev, C_prev, LSTM_Params):\n",
    "        '''\n",
    "        param x: numpy array of shape (1, vocab_size)\n",
    "        param H_prev: numpy array of shape (1, hidden_size)\n",
    "        param C_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.x_out: numpy array of shape (1, vocab_size)\n",
    "        return self.H: numpy array of shape (1, hidden_size)\n",
    "        return self.C: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, H_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.x_out = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.x_out, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    '''\n",
    "    Corresponds to a \"row\" of circles in the diagrams above. \n",
    "    We initialize a layer with a number of \"Nodes\" equal to the length of the sequences we passin.\n",
    "    Again, the key methods are \"forward\" and \"backward\".\n",
    "    1. \"forward\":\n",
    "        * Passes data forward through all the nodes in the layer\n",
    "        * Sets the hidden and cell states outputted by the last node to be those that will be passed in when the\n",
    "        next sequence is fed through the layer\n",
    "        * Passes the appropriate data on to the next layer in the network\n",
    "    2. \"backward\":\n",
    "        * Passes gradient backward through all the nodes in the layer\n",
    "        * Sets the hidden state outputted by the last node equal to the node \n",
    "        * Passes the appropriate gradient on to the previous layer in the network\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_seq_in):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_seq_in.shape[0]\n",
    "        \n",
    "        x_seq_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "\n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            x_in = np.array(x_seq_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, num_layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "\n",
    "    def _generate_one_hot_array(self, sequence):\n",
    "        '''\n",
    "        param sequence - sequence of indices of characters of length sequence_length.\n",
    "        return batch - numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.vocab_size))\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "        \n",
    "        return batch\n",
    "        \n",
    "    def loss(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_in = self._generate_one_hot_array(inputs)\n",
    "        \n",
    "        x_batch_out = self.forward(x_batch_in)\n",
    "        \n",
    "        x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "        \n",
    "        loss = self.loss(x_softmax, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_softmax, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Character_generator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        '''\n",
    "        param text_file: path to text file containing characters that we are going to generate text to mimic\n",
    "        param LSTM_Model: the LSTM_Model that we are using to generate the text \n",
    "        '''\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        '''\n",
    "        Given a start position, generates a list of inputs and targets.\n",
    "        param start_pos: int - index of start position in characters \n",
    "        return inputs: list - list of length sequence_length with the indices of the characters forming the \n",
    "        inputs to the model \n",
    "        return targets: list - list of length sequence_length with the indices of the characters forming the \n",
    "        targets of the model\n",
    "        '''\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_in = self.model._generate_one_hot_array([input_char])\n",
    "            \n",
    "            x_batch_out = sample_model.forward(x_batch_in)\n",
    "        \n",
    "            x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, num_iterations, sample_every=100):\n",
    "        '''\n",
    "        Trains the \"character generator\" for a number of iterations. \n",
    "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
    "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
    "        '''\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs, targets = self._generate_inputs_targets(start_pos)\n",
    "            loss = self.model.single_step(inputs, targets)\n",
    "\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD0CAYAAABQH3cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd4VFX6wPHvJJNCQoDQQcBQDyIKrHSkKKBgL2t3XduirgV19yeIYMW2q+5aVl11kdVVcUFdCyAdpYgUKQpy6EXpoYZAQpL5/XHn3rkzcyeThEym5P08zz47c++dmXMR3jlzznve4/J4PAghhIgPSdFugBBCiLKToC2EEHFEgrYQQsQRCdpCCBFHJGgLIUQccUfiTZVSaUA3YCdQHInPEEKIBJQMNAGWaK0LnC6ISNDGCNjzIvTeQgiR6PoC851ORCpo7wT44IMPaNy4cYQ+QgghEsuuXbu44YYbwBtDnUQqaBcDNG7cmGbNmkXoI4QQImGFHFaWiUghhIgjErSFECKOSNAWQog4IkFbCCHiiARtIYSIIxK0hRAijsRc0H511npyRk6OdjOEECImhc3TVkqlAOOAHCANGAtsB74C1nsve0Nr/XFlNOjFGesq422EECIhlWVxzY1Artb6d0qpusAK4EngJa31ixFtnRBCCD9lCdoTgUnexy6gCDgLUEqpSzF62/drrY9EpolCCCFMYce0tdZ5WusjSqksjOA9GlgM/J/Wuh+wCXgsss0UQggBZZyIVEo1B+YA72utPwQ+01ov857+DOhS2Q2TDYeFECJY2KCtlGoETAdGaK3HeQ9PU0p19z4eCCxzfPFJkJgthBDByjKmPQrIBsYopcZ4jz0I/E0pdQLYBQyr7IZJzBZCiGBhg7bWejgw3OFUn8pvjo8xPOKK5EcIIUTcibnFNSbpaQshRLDYDdoStYUQIkjsBm3pawshRJDYDdoSs4UQIogEbSGEiCOxG7RleEQIIYLEbNAuLpGgLYQQgWI2aJeURLsFQggRe2I2aBdJ1BZCiCAxF7QHqAYAFMtMpBBCBIm5oH3+6Y0BGdMWQggnMRe0k5OMeiMStIUQIljsBW2XBG0hhAgl5oK2O1mCthBChBJzQTtJetpCCBFSzAXtY4XFAOQVFEW5JUIIEXtiLmhPXLYdgC9X7oxyS4QQIvbEXNC+59y2AAzu0CjKLRFCiNgTc0E7IzUZgBJZXCOEEEFiLmibedorth+MckuEECL2lLqxr1IqBRgH5ABpwFit9Rfec9cD92qte1Vmg5pnZwCQmhxz3ydCCBF14SLjjUCu1rovMAR4DUAp1QW4jQhsl167RgoABUXFlf3WQggR98IF7YnAGO9jF1CklKoHPAPcH4kGpbqTcCe5yC+UoC2EEIFKHR7RWucBKKWygEkYAfxfwIPAsUg1qkZqsgRtIYRwEHbgWCnVHJgDvA+sB9oCbwATgA5Kqb9XdqNqpCRbi2yEEEL4hJuIbARMB+7RWs/yHj7dey4HmKC1rvRhkozUZI6dkKAthBCBSg3awCggGxijlDLHtodqrSM2NAJQI9UtwyNCCOEg3Jj2cGB4iHNbgJ4RaBM1UpI4dkJqjwghRKCYTIbOSHXLmLYQQjiIyaAt2SNCCOEsNoN2SjL7jxZGuxlCCBFzYjJoJye52HOkINrNEEKImBOTQbtRrXQAGdcWQogAMRm02zasCcCuw8ej3BIhhIgtMRm0m9Q2etq7DknQFkIIu5gM2g2y0gDYlyfj2kIIYReTQTs7MxWAjxZvi3JLhBAitsRm0M4wgvbCjblRbokQQsSWmAzayUkucuplRLsZQggRc2IyaANsyc0H4MXpOsotEUKI2BGzQfu2s1sC8OrsDVFuiRBCxI6YDdr3D2prPS4u8USxJUIIETtiNmhnpadwbvuGgKT+CSGEKWaDNsAd/VoB8M26vVFuiRBCxIaYDtqdW9QB4KFJq6LcEiGEiA0xHbTT3MnW45GfSOAWQoiYDtoAk+7sBcCEJduj3BIhhIi+mA/aXXPqWo8PyMYIQohqLuaDtt17322NdhOEECKqSt2NXSmVAowDcoA0YCywAXgLcAHrgdu11hHdOn3V4+dx5uPTcSe7IvkxQggR88L1tG8EcrXWfYEhwGvAM8AorXUf7zUXR7B9ANRKTyErzc1e2YJMCFHNldrTBiYCk7yPXUARcKXWulgplQo0Bg5FsH2WRrXT2bY/vyo+SgghYlapQVtrnQeglMrCCN6jvQH7VGAmRsBeGfFWAl1PzWbqT7uq4qOEECJmhZ2IVEo1B+YA72utPwTQWm/VWrcF3gReimwTDTn1Mzl07ARHjp+oio8TQoiYVGrQVko1AqYDI7TW47zHvlBKmdWcjgAlkW2iQfaNFEKI8GPao4BsYIxSaoz32CPAeKVUIZAP3B7B9lkaZhlBe8+RAto2yqqKjxRCiJgTbkx7ODDc4VQfh2MR1bCWsdnvniPS0xZCVF9xs7imoXeH9j2HJe1PCFF9xU3QrpnmJiM1mT2Sqy2EqMbiJmi7XC4aZqVJ0BZCVGtxE7TB2Oz3y5U7ot0MIYSImrgK2qbjJ4qj3QQhhIiKuArar17XBYANe/Ki3BIhhIiOuArazbJrAKB3HYlyS4QQIjriKmjXr2mk/R2T4REhRDUVV0G7gTdX+9AxqT8ihKie4ipop6cYG/3+dZqOckuEECI64ipo23k8nmg3QQghqlzcBe3uLY2NfncdlhokQojqJ+6C9h39WgGwZZ/sYiOEqH7iLmifWi8DgC9XycpIIUT1E3dBu2kdI1f7w++3cfO7i6PcGiGEqFpxF7QzUn0lwOfqvRQVV8nGOUIIERPiLmgDfPbH3tbj56aujWJLhBCiasVl0O7SIpvrujcH4J35m6PcGiGEqDpxGbQB/u/89tFughBCVLm4Ddp1M1Otx19IjW0hRDVR6sa+SqkUYByQA6QBY4FtwKtAMVAA3KS13h3ZZjprkJXG3iMF3PfRci7p1DQaTRBCiCoVrqd9I5Crte4LDAFeA14G7tVaDwA+BUZEtIWlGDnEGCLp1LxOtJoghBBVKlzQngiM8T52AUXAtVrrFd5jbiBq68mvPKsZzbJr0Kp+ZrSaIIQQVarU4RGtdR6AUioLmASM1lrv9B7rDdwD9It0I0tTJyNFSrUKIaqNsBORSqnmwBzgfa31h95j1wBvAhdqrfdGtomla5iVzuodh6LZBCGEqDKlBm2lVCNgOjBCaz3Oe+xGjB72AK31psg3sXSLN+9n9+EC5q2P6neHEEJUiXA97VFANjBGKTVXKTUPI3MkC/jUe+yJSDeyNN1ysgF4Z54sshFCJL5wY9rDgeFV1JYKee7KM+nxzCy+WSc9bSFE4ovbxTWmRrXSARhyeuMot0QIISIv7oO26evVu6LdBCGEiLiECdoAs9dGZWGmEEJUmYQI2td0NSr+3Tp+KSUlsuGvECJxJUTQHta/lfV4+faDUWyJEEJEVkIE7dYNalqPC04UR7ElQggRWQkRtAH+d3cfACYt+yXKLRFCiMhJmKDdICsNgE+X/xrllgghROQkTNA+xbtLO8CBo4VRbIkQQkROwgRtgFv65ACwYW9edBsihBARklBBu5V3QvKGd76PckuEECIyEipon9ehEQCFRSWcKC6JcmuEEKLyJVTQblAzzXrc9pGpUWyJEEJERkIF7aQkV7SbIIQQEZVQQRtgzZPnBx07fqKYLk9O5+ufdkahRUIIUXkSLmhnpPpKhHd7eibfbcxl+/58DuSf4Nmpa6PYMiGEOHmlboIQry7u1JQvV+5g75ECrnt7kXV8a25+FFslhBAnL+F62gD3nNPG8Xiz7BqOx4UQIl4kZNBWjbO4uXdO0PF6tuwSIYSIRwkZtAFa1s8MOiYVAIUQ8S5hg/ZVXZvxu56nWs/bNKzJ2l1H/K4pKi7haEFRVTdNCCEqrNSJSKVUCjAOyAHSgLFa6y+85/4GaK31m5FuZEVkpLp56rKOPHxBe/IKiuj+9CwALnh5HlOG9yVn5GTr2i3PXRitZgohRLmE62nfCORqrfsCQ4DXlFINlFJTgUsi3rpKkJHqpmFWurUl2Zqdh/nv0u1+1yzalBuNpgkhRLmFC9oTgTHexy6gCKgJPA68H7lmVb7nf3um9fihSav8zi3beqCqmyOEEBVSatDWWudprY8opbKAScBorfVmrXVClNEza3D/dZqOckuEEKJswk5EKqWaA3OA97XWH0a+SZHzyV29/Z5/+9A5UWqJEEJUTLiJyEbAdOAerfWsqmlS5Jx1arbf82QpMCWEiDPhetqjgGxgjFJqrvd/cb2s8IrfnOL3vEXdDNwSvIUQcaLUnrbWejgwPMS5xyPRoEh76erOfPqDb/PfQac14uMl26LYIiGEKLuELBgVzsrHzqNGSjIA2RkpHC0sprCohFR3wq41EkIkiGoZpWrXSLECdO2MFACe/1rKtgohYl+1DNp2RcUeAP41fzOz1+4mZ+Rkhr23lGOFUqdECBF7qn3Qdif7JiFvHb8UgOlrdnPBK/Oi1SQhhAip2gftm3rlOB7fvO8oHo+nahsjhBBhVPugDfCP63/jeLzdaNnRXQgRWyRoA+e2b+h4/ESx9LSFELFFgjZQIzU52k0QQogykaDt1al5HQD02CF+9bXnrd8LQEmJh0c//4m1uw5X2mfmjJzsV9dbCCHCkaDt9fGwnswfcQ5pbqPXfdeA1gAcP1ECwPiFW3jvu60M+XvlZ5UUl8gwjBCibCRoe6WnJNMsO8N6fkHHJn7nn/xqjfX4552V19sGmLRse/iLhBACCdohZaUbK/zfmbeJgiL/hTZDXz753rZ944URn/x40u8nhKgeJGiHkO6tTfL95v2o0V8Hnf9m3d6Tev8r31h4Uq8XQlRPErRDaFw7vdTzvx+3uNzveaK4hH15BY7nDh07Ue73E0JUPxK0y+GZy88o9fx9Hy3njbkbueS1+X5ZIR6Ph8mrdnLtW4voOnYmn6/4Nei1/f4yhwNHC5m+eheH8k+wfX9+pbdfCBH/qmVp1oq6vkcLRn3mG38uLvH47X7zxcodfLFyh/XcLPc6b/0+7v7wB+v48Akrgt770LETdHlqht8xe+qhEEKA9LTLbP4IYz/JprZhEzOHO5Q9R44DcPi489DHA4Pa8dLVnUK+XmqfCCECSdAuxbu3dAOgQ5NaVjrgjAf7W+eHvbcMgJlrdjsukjlaYGSdhNrO7MaeLbjiN82s55d2bup3/khBkd/z4yeKWb7tgN+xDXvy2LzvaJnuRwgR/yRol6Jvm/rcN7At79/W3TqWmeZmxJD2ABQWl3DTuMXc/t5Sx9f/vPMw36zby6Ofr3Y8X69mGgBXndWMprXTWbrFPyAfOFro93zUpz9y+esL2bDnCAC5eQUMeukbznlhboXuTwgRfyRol8KdnMSDg9tZwdV0S58c6/G3paT+3f/xCn4/bjF7jjhnjJhKPLDj0HF+PXgM8PXw9+X5B+1PlxsTmINe+pb1u49w1tiZZb4XIURiCDsRqZRKAcYBOUAaMBZYA4wHPMBPwN1a65KItTLGmDnc5dGkdjq9WtWjoLiEyat28uhFHaxzn/zwi9+15v6V5qKeaat30bR2Db9r1gSsyswvLCIjVeaVhUh0Zelp3wjkaq37AkOA14CXgNHeYy7g0sg1MTZNurNXua7feeg4L13Tmdeu68KEYT39euuBSrwTkNe//T0Ad7y/jItfm+93zf6AoZMOj07j+0255IyczC8HJF1QiERVlqA9ERjjfewCioCzgG+8x6YCgyq/abGta07doGNnnFI77OtcLhc9W9XD5fJNTv7nth5+1zSu5ctQWbf7iOP7PPHlmqBj17y1CICzn59DUXH4Hz73T1hOzsjJbMuVIC9EvAj7e1prnQeglMoCJgGjgRe01mY+2hEgfLRKYJ/c1ZuzTs0GjDS/d+ZtJt2dxCuzN5Tp9We3rW89/nhYT1o1qGk9P+9v31aoTWt3HaFjmC+R/60wcso/X/Er9w5sW6HPEUJUrTJNRCqlmgNzgPe11h8C9m5cFnAwAm2LeYseHsj3owZaARugYVY6oy44jQfPU2x85gJa1c8EoF+7BmV6zx6t6oU8N947QWn3yAWnOV570avzHY876dC0VpmvLY97PzJ68pJvLkTlCRu0lVKNgOnACK31OO/h5UqpAd7HQ4FquXV549rpNKoVukZJcpKLTd4c6tKyTAAev7gDT1/esdRrUt3B/7nC9aYB8gqKaDNqCrsPH7eOHSv0VS687d/+KYu/HjzGwXz/MfOK+NK7OrTlw1NC1lwRQpRPWXrao4BsYIxSaq5Sai7GEMkTSqnvgFSMYRNRinB1S27u05IbepxqPW9UKy3omoxUN09derrfscy0ZC7p1DToWruOj02jqMRDj2dmWceGT1ge8vo+z82m85MzQp6viGenrK3U9xOiuirLmPZwYLjDqf4Ox0SABllp7D1SwFVdm4W/2Obhoadx/8f+NUo6NKlF5+Z12LTvKO8u2AIYi31evrazVfNk/dNDGfryPDbsycPj8fhNeNpNX7Pb7/m5L8xl9p8H0PGxaeVqZ1kFlrLdtDePc1/8huVjBpOdmRqRzxQiEcnimgib+UB/Zj7Yj5Tk8v1Rd29pZKf89qxmPDi4HevGDrWGRx4e6hvHbt2gJi6Xi2H9WjGsXytSkpO4vMspgLFVmt7lnH3Su7X/2Lk5jJNnWzpfWFR5qfc39Gjh9/wKbz3x8/9esYlWIaorWY0RYbUzUqidkVLu1zWtU4Nv/m8AzbIz/CoJgjG2veLRwX4rJkfZJiTNWiRjJ6/hg++3Bb33ofwTLNyYG7YNHy/Zxu965ZS77U5enrWeBwa3s543z87gYP6hsKtFhRD+pKcdw06tlxkUsE11MlJp07Cm4zlzKzOngH38RDFPTwnO8QY483H/oZExIWqmlMWOg8fIzkihVYPMoHMej4cffz0E+H5RCCHKRoJ2AjILWjlpP+Zr/rvUt2zeLDkLcPh4UdD1P3mDa3kczC+k93OzOZB/ghZ1fZsl/3fpdjweDzfZdv1ZvHl/ud9fiOpMgnYCalk/uHd7XfcWQccWPTyQZtkZTLmvr9/xq22Tpsu3+1LwTxSXcPO7izlaEBzc7eyZJ3O1bwLyoUmrmLFmN/PW7/O7vqSkbHnc+YVFZb5WiEQlQTsBqcZZQcdGDg3ufZv7YJ6S7V+M6vkrz7Qej/nfT9bjP/13JXP1XjrahlEKioq54/2lVh564PL5nq3qsmy0UeXg1HoZHHDI/84/URx0LND+o4V0eHQaYz7/iaLiEnYeOhb2NbEkZ+Rkx5rrQpSXBO1qYPUT51O7Rgp/u8Z5l5zMVP+qhS6Xi5kPBmd0tvWOoXs8vt7x+99tZdrq3daQx/o9eX6vuWtAG+rVTKNJ7XS65dTFRfAYfZ7DsEyg8Qs2A8Y4fZtHptLr2dnl2gz57W83kTNyst8Co2h4abqO6ueL+CdBO8G1b5xFZpqRJHR5F9+wx4VnNLEeu23piP+4/jcA1iTnqfUyrACdluK7rtWoKQBM/nGndSw3r4B7bHthArRrZLxPZpqbowVFuJONoJ1TL4M7+rcCoOezswinyGFY5HA5gvbTU34G4Fnv/0dLWevRCBGKBO0EteHpodx+dkv+c7t/BcGpw/vStmFNXgyxN2WPVv7ZHFtz83nVG2jM7dNMJ4pLWL7NN+Z91tiZ7PWm8HXLyWbjMxfQxFsHfMOePKb+tMsaD594Z2/W7/b1yotLGaueuWY3r8/dGHQ8L8zYumnJFt9kp1kkqypJ7RVRmSRoJyh3chKjL+pA/YBdd05rUosZD/YPuZFD4PUAP/56kB+2HeCrVf4B761vNwVda2ag/Ovmbo7pinnewJ+V7ubec9tYx3NLqU3y50krHY//e+GWoGMej4fPlv/Ccds4+VVvfhfyvatCYRnK5ApRVhK0BQCf/bE3793a3fHczJ/3cMXrC9m4138D4b9OCz0+mxmwi06DLOPLIK/gBMlJLtLcSVYvHGB/KQWqDuY7D4NMWLKdrblGmw4cLeSqNxcyY81uHvh4Jc9EeRjEzr6y1PxSPJhfyA3vLOLJL9f4fcEIEY6siBQAdGmRHf4ir+Z1a7B9f+nZG4G97NOa1GLvkb0s2JBLcYlRE6VeTV/NkfzCigWutbuOcGq9TLo8ZaQZLtmyDIBfDxjt+2hx8AKjqmavu7Ivr4C8giLOeWEuB/JPsGBDLuMWbGbLcxdGsYUinkhPW4S04tHBjsedAvbsP5VeP8xMCVxhy/tOSU6yapI8560CuPPQsXLlYj80aZXjcfNL4OFPf7SOXdq5qd+uQFXlng/9Kyp2fGwaB0L8eoiUz5b/Qs7IyZyQoZq4J0FbhFQnI7j63m1ntww6tujhgbRqUJMLzmhc7s/o29bYHGLxlv28NGMdvZ6dzetzgzMsuuVks+HpoTw8tD1dbZtO9GvXwHGiz0ztO7d9QwA+uL0HtdJTrM2SI8Xj8bDBm/Z4qByBOXBS9dt1e7nktfl+E7TLth5gfYjt58J54GNjXmCKLdunMpWUeEqdTBaVR4K2KJeHh7bn0z/2tp5fdVYza5GOmS7o5F2HXXcAetl26nll1noAXpi+jtlrd+PxeKzx3gGqIe7kJO7o35pJd/W2fgV8uXIHLR+eEvS+5k5B5s5BvVvXIyM1ucLDMGU1e+0eBr30DZe/voBOT05nrt5jnfvrb32Lli7t7F8DPXAi9qZxi1n1yyHW7DhsHbvyjYUMruD2c6bWDZzr1Zysq//5Ha1HTaHr2JnsORLdXPhEJ0FblIs7OYnW9X3/8O2rGV0uFyseHczyMcHDKv3a+rZbswesUBUQbx2/lGmrd1s90Kx0/+kXp18Bdmbv+535m622packU1BUYg2/5OYVkDNyMh8vqbxx79XeIGumQn76w6/Wuau6Nqdtw5qc16ERxSUemtT2DdUkeeueL9y4z28V6m3/XkJhUQnLtx2olPaFK0FQUUu9Rcr25RUw7L1lEfkMYZCgLUp1Z//W1uNh/YzFMPZAO3mV/8/tOhmpjpsa2CcmNwVkodw/yHlT4ck/7qTr2JkA1EwLP2f+2MUd+NfvuwK+Me1T6vgyVMz3+Jc3kJuBZsQnP1JevxzI5/Dx4OGPwCRHc3MKs8dfu0YKh46d4KtVO0lOcvHqdV0ArGGb69/+nvcXbbVev+dIAat+Ocjlry+0jpW3zrm9t751f77fuZGfrLJ+4VSWeCsxEG8kaItS/aGvbwx7lMMmwqFKx5bmx4DKgUXFzmOh5h6TAE4b8JgBz1SvZhoDT2tEm4Y1rR56x1NqoRoZtVhq1TCC9tNTfuZYYTF3vO/rEZqBvKzOfn4Ol/1jQdBx+6pRuxHe2i9Z6W6+91Y2/OXAMdK8G1tszc1n2VbnioeBQ8UTlmxjx8FjnP38bPr9ZU7Ytl7wim8L18CJ2wlLtvPSjHVh36M8urcMvTm1OHkStEWpatdwHr5Y/cT5DDqtIevGDi3zey31Fo4K3MC4l20XHfsko53Tzj8XndnE77lZw6ROjRQrtzu/sJiMNGMhkf1e/jXff2HQU1851xgvTeAvBoBjhc694L5t6wOwdIv/MIf5pXfbv5dy5RvOi4COBPToH/18NX/5ei2/HDjGtv35/H1mxYKufQK3Mldt2r9sAf6zaCs5Iyfz3FTZJ7QySNAWpXInJ3HPOW2sYQdTZpqbd37vvOoxlPo109j87AV+GxgD9GlT33r8+CWnc32P4DKy558enJkSuP9lkzq+MeLvNhk78xwtKLIW+uw/6gt+9pKxprIGrsBKhgC7Dh3n5ncXhxwayPC24UjAmHJBiKGOSXf2sh4HFsbq0bKu33L8v88s+/DGdd2bW4+H2X5pvDKr4jVRFm0K3gVphm0P0tHeMfo3vwkuRSDKr0xBWynVw7sLO0qp3yilFiul5imlXlVKSeBPcH8+XzHwtEaV8l6hNhpe/MhA/nLlmXQ8pTbD+rbyO9e0dnrIPTZfv8HIWPn7NZ05RxnpfRv3Gil3eQVFRk/bW8XQ7O2Cbzzbbs1O39jv1B93cjC/0DGNzV6DZfm2A0xetZNnpvzMXL2XCUu2O7YzlE7N6zger2mbeJ31sy8DJSvNN7xSVvaNKD5avJ0FG4x65vbAOm5B+YaHTP9ZtJVr31oUdPwP7y21Hjeva8wr2OcXRMWFDbhKqYeAdwCzG/MWcL/Wui9wCLg+cs0T1UXDrHSu7mb0Au09ZoCGpSyIueCMJmx57kIu825mDFgLVzo+No21u45Qwxu0m9fN4LwOob98zI726h2HuOuDH+j85Axaj5rC72077eQXFvHiDN/y/ctfX8jdH/7gt8NPaZOmMx/sZz1OcyeFDGT2MgBmJcVW9TODeuoAjWoF14sxFRWXsC1g8vGGd74Puq48ZW5NhUUlVi8a4NY+La20xks7N2Xl9oPkjJzM4WNGm389WPYJyl2Hjlc473vO2j3kjJwcNKyUKMrSS94IXGF73kxrbU5lLwDOrvRWiWotze1fzMqe31wRn9uGEgIXB9VISbZyyM2hisDNhs1l6B6Phw6PTuO977YSyNzNHowe/qrHz+N3PY1hoOEDfdkxbRr6Nqj4ftTAkG2ukRpc0OuJS093vLZhlu9LrbCohJ9+PWQFvMBJ39LsKWet8R9/Pej3fOKy7VzVtTm10t1kZ6Raq2DL+4Wweschej47i6e+MuqybNkXPHdQmlvGLwFg+IQV5XpdvAgbtLXWnwD2P/VNSilzzfLFQPDeVkJUki4t6tC2UfBOPKW5a0DrkOcCg+GaJ88nw1vx0FzI8/oc5/HdN78JrmoYSq30FK7uavxyuNz2K8DOzDU3i2mZLjqzCfUyU/noDz39jmeG6MGbPcqi4hLajZ7KRa/O55HPjDRGc4K14ym1uKSTLz/e4/GQ5IJ7zvFVWvwhTC74ofwTVgojwCOf/eR3/sHB7ax25hUUkeQw3xFu3uBYYbGV3jh+4Rbaj/maAS/M9Qv8Ho+HRZtyHd9rpa1Mwuy1e4LOJ4KKjEffAjyslJoF7AH2hbleiAq7o1+r8BcFsJd8DXRmM/8xZJfLZU0SmoFhyZbg4FXNc13SAAAPyklEQVRUXMLzX5cv++GMZrXZ8tyF5ATs2fnAoHY8f+UZ1vP2tu3hrunanJev7YLL5fLLqgFj2OWxiztYz9c+NYTre7Qgr8DYO/Nu2wYU5tj6D95FPvee29ZvG7nco4WUeMCd7CKnnjHmfed//DewCKxTcv/Hy7nvo+VM8BbhWrvLf0m9Oaew89Bxpq/eRUpycNC2j887GfHJKsc89F2HfL8Cfvvmd1z71iL+Mk3zjzkbOGZb5XqpQxpmoqlI0L4QuEFrPRCoB8wIc70Q5fbUZR25uFNThnRsEv7iABmpbh4c3I6m3hWHj9sCnd1X9xoje2YGzB8/+MHxOoDfPFX2v+b24RDH84Pack03X4bM7d6J1+yMFJ7/7Zl+GTn1bAuVMtPcfr329JRkstLdHD5exOtzNzBttW9i0WS+1/mnN/b7lWEuWvr3wi08fknwsMs78zbR9pGp/G+5b0WnOSY98tMfeWfeJmuoyfxlU9dWtfHw8SIa1w4er7/dNkHp5IuVzptU/GniCqtnvcw7ifzG3I381Ru4Y8VPvx6K+O5IFQna64FZSqmFwGGtdXDhByFO0u96nhq0eKY87hvYloUPD0SPHcLve+f4nfvbNZ3o2aouHU+pDUBdW2AMVTfjcBn2sTR9Zgt0ZVHfG+xa1g8eabS3vU6NlKBJzrlr91JYVMIL051ztQMn8wLH9Iee0cQq2nWhLe/d3ODiDduOQetsOw2Nnfwzuw4fx53kYsSQ9mx57kJqpRt58HW8K2bN7eD6tq0fVIagvH769bC1K1Igc3ho1S8HHc9HSnGJh8UBmTwXvTqff367iYlLy5dFVB5lCtpa6y1a657ex19qrTtrrXtrrR+JWMuEqARp7uSgNMPLuzRjwjBfHnRjWw2Q7k/79qscc5FzDz2c7i3rhr/I5vSmtfnkrt58GDCGDf5DPZlpbtzJSXxxTx9WP3E+ALqUqn8HHTaWuCEgB37EkPYkJ7momeamkW1C82Lv+Lf5/k6ZHJNX7STVHRxCzOGKpd5t3t6+qSt/HFC2XYpKs3HvUcehk0LvitpLXvMNjZyjGlj7k0bK2/M2cfU/v2PO2j18tWqHX0lh+xZ3lU1yrIVw8ODgdtx2dkumP9Av6NzGZy5g3dihViXBQH8+T5X78846NdtxCziXy8XEO3vx8rWdrWNnNqtjTUqOGNI+5Ht2fjJ4SCcjYEchc5VomjvJr2ytfYjG4/Fw1ZsLceJUNdHMwpm3fh9p7iTSU5I5y7bStTy/WuyenrLGseDVsUL/Yy9e1Yk6Gakci/COQJu86wFuGb+Eez5c7pc/b/+SqmwStIVwYG6W0M4hcyU5yUWqO4m2DYN7ctd2a+7Xc68M3XLqcmln5wyUupnOZQZCcUolBGNi8oPvt1nDQ/YebbvRU60JzbIw/+xyjxZaAdy+Nmr++uDVqGWRmermnBfnBh03s4tu9g4lXdq5KekpyUElBWas2V3uYlulCfwC3Gv7BRE4+VyZJGgLQfBYbx/b6smJ3iXlWelu1jx5vnX8oSGKD//g2+1+0GkNee7Kk8spL6/BHfyX999+dktu7Ok/BGLvpWfYgvZA7wYRdm/M3ci+vAK/BTknQhT0CuWtm84KOtYs27cqc8znqykp8bB531G/tD17Wt/k+3zLP8yJzu8377dqynTL8fXczUBs5oW7k5NIT0miwNbT/u/S7fzhvaUMebn89cg/WfaLtcrWLvALcLO3Fs3d54ROOa0MskekEEBT28rELi3q+K1U7JZTlx8fPw93UpLfP9Q0dzK9W9dn9p/68+6CLTwZYvFLJNknUbvlZDP6og54PB7+s8hXI7xDk1rWY3s5gAfPaxf0fi5cVmaJk0/u6kVyUpJjhUOTPUCbGtVK5/O7+1gpea1GGfkLj1/cgWu6taBGarKVJ39Z56ac3rQ2X9zTh4xUN60bZPpNiIKRwtiyfiZ9/zLH6s3bFzhlpCZztLAIj8fYj/TzFcbksFORr9J4PB7+NHElae4kdEBxtMBt8f7mLdwV6S3tpKctBNC5eW3r8WMXBwffrPSUkEMLrRrU5KnLOoasqxJpd/Q3UgbP8facXS4X80ecwwDVgMu7nBJycZJyOB6uBslvWmTTuXkdlnkrNjoFKHuGi5kDDljZOnaPf7mG0x79mg178qwskP7KmCs4s1kd2jSs6fjnmuRy0bxuBlnpbivY279oM9PclHjg3Be/AWDBBl9Rq+82Bhe4CqXQm6tuL+y190gBz0z52bGeOkDXnPJNRJeX9LSFAM46tS4rHh0cdkecWDRySHvOOKU2Q2yVEJtlZzD+lu6O1z97xRkUl3hw23rdb9/U1a/Ik5M7+7e2Ami9mmmsfWqI43X2jJK+th2LkpNcnNehEdPXBOeTvzhdM/WnXUDwWDEYq0S/sm24YcbxNHeyNYF6SnYNqziV+cUROAQDcN3bi9jy3IWl3qvpmMNE630fLee7TblkhvgSd0rdrEzS0xbCKx4DNhg964vObOoXhEtzXfcW3NjTvzzuYIdCWq0b+Aef9IANHtJTkh0zXgB+38t4/+MBGRxtQ6ThmQEbINXhPp654gy/5+aQz768Aj5avN36rBre9tgLbk1c9kvQ++0/6kuHPHA0ODXSdNQhaFtlf0PsNxrqz6SySNAWQjiaMKwX3W0/9fPKkap3mjeo7g3Iyd6+P3ylP6eaJebCHYBnLj/Db0s7s9edX1hsDWHZg/LPtpK7JjN1cM2Ow3R5agY5IyezYrt/hszK7QcZ57Cjkb2cbv2aVf9FL0FbCOGoQVYa42/tZj1/pxxbsrXy7vreKaDWi33opI1DyiT4Z4bYzXywHx1PqeW3kUOXFnXweIzFLLl5BVYv1z7/sGGPL/Oj4ynGl4mZw/2tLf3wsn8soNMT0wGj7sql/1jgtw3dDe8s4qo3F9LDtnhqX14h68YO9dukOdIkaAshgpgVATNS3ZzrneC8umuzMr++e8u6TBjWM6h4lz218u2bulorL0392zVwHNMGo6ztV/f29ZuYNHe9v+rN7ziQf8IaHrGv/Jy33lfT7oFBRsaMuSho9Q7/XriZdui09+aCDbks2XIgaIl6qjuJ05sak6wnU3qhrCRoCyGCvGILPo9e1IHf9TyVJy7pWMorgvVsVS9onP20JrWssfE6NVK4ubf/2LrTsvjyOOgNui6Xy9rVyPTB7T2sHni+d3gkcD9L085DoWuLm5tsgG+5/6MXdeDc9g0ZeFpw7ntlk6AthAD8d9Wxy6mfyVOXdQyZ8lheU4f3Y+xlHcnOTKVzc/+hELMmd1kFbtc262dfZkpgvZRuOXWthUIvlrID/bkOqy5DMYdeWtTLYNzN3UL+SqhMkvInhACgXqaxGUNGJQXnUFrWz7TS4uw1Tu7s39qawCyrQe0b+m18YM+nDky9S0l2Wbncy7YeQO9yLrZVngU4F3cqf+ngkyU9bSEEYJRUHaAa8NLVnaLy+TvKsYek6Q8Bm2TYA3XgYh6Xy+U3+Xn+38u/pD3o8wM2oa4KErSFEIAR1Mbf0r1CG09Uhs4hdqYvTXpKMh/c3oOnLjPG20dfeJrfeTOIb3zmglLfZ9KdvRyP/zBmMPMeOsfx3O1nt/QrC1BVZHhECBETbg0o2lVWfdrUp0+b+tZGynaf39OHQ/kn/IZhAr17cze65tTl3nPb8Ops/11w6mamUjczlZRkl1/hrIeHtueO/pEtDBWK9LSFEFE18c5ezHCoW14ZaqWn0LyufwGrW/rk+F/jrSkeOAn6/m2+MgCBlQ7thbqqmgRtIURUdcupG7KoVSQEjp2f3tSY/HS5XMz58wBy6mXw3BVn+NVNuS8g33z59qrd2sxOgrYQolqxp+U9denpfrVCWtbPZO7/ncO13f1rkt8/qB0/Pn6e9bwi4++VRYK2EKJaMdP02jasGVQ4K5SkJBdZ6Sn8945e3D+oLVd3bR7+RREiE5FCiGrl3PaNWPX4eX5FqMqqe8u65d64ubKVKWgrpXoAz2utByilOgNvAkXAOuB2rXXlbbwmhBARVpGAHSvCDo8opR4C3gHMMlaPAU9qrc8G0oCyVRMXQghx0soypr0RuML2fDlQVynlArIA5z13hBBCVLqwQVtr/Qn+gXk98ArwM9AImBuRlgkhhAhSkeyRl4G+Wuv2wHvAi5XbJCGEEKFUJGjvB8zK4TsA520mhBBCVLqKpPzdDkxQShUBhcAfKrdJQgghQilT0NZabwF6eh/PB/qEeUkywK5du8JcJoQQwmSLmSGLmkdqcU0TgBtuuCFCby+EEAmtCUbmXpBIBe0lQF9gJ1Acoc8QQohEk4wRsJeEusDl8XhCnRNCCBFjpGCUEELEkZgqGKWUSgJeBzoBBRh1TTaU/qrYppRKAcYBORjL/scCa4DxgAf4Cbhba12ilHoMoyxAEXC/1nqxUqqN07VVfBsVopRqCCwDBmPc03gS/54fBi4BUjH+Ln9DAt+39+/3vzH+fhdjZJMl9H/rgFpMju0vz706XVva58daT/syIF1r3QsYSWIs3LkRyNVa9wWGAK8BLwGjvcdcwKVKqd8A/YEewLXAP7yvD7q2ittfId5/zP8EzIrz1eGeBwC9MbKr+gPNSfz7vgBwa617A08CT5PA9+xQi+mk7rWUa0OKtaB9NvA1gNZ6EdA1us2pFBOBMd7HLoxv07MwemAAU4FBGPc+XWvt0VpvA9xKqQYhro0HL2BUg9zhfV4d7vl84EfgM+BL4CsS/77XYbQ/CaiFUfIike85sBbTyd5rqGtDirWgXQs4ZHterJSKqSGc8tJa52mtjyilsoBJwGjApbU2Z4CPALUJvnfzuNO1MU0pdTOwV2s9zXY4oe/Zqz5GR+Mq4E7gAyApwe87D2NoZC3wNkZdooT9b+1Qi+lk7zXUtSHFWtA+jFE50JSktS6KVmMqi1KqOTAHeF9r/SFgH7PLAg4SfO/mcadrY92twGCl1FygM0aNmoa284l4zwC5wDStdaHWWgPH8f8HmIj3/QDGPbfDmIv6N8Z4vikR79nuZP8th7o2pFgL2gswxshQSvXE+KkZ15RSjYDpwAit9Tjv4eXe8U+AocA8jHs/XymVpJRqgfGFtS/EtTFNa91Pa91faz0AWAHcBExN5Hv2mg8MUUq5lFJNgUxgVoLf9wF8PcX9QAoJ/vc7wMnea6hrQ4q1oYfPMHpoCzHGf2+JcnsqwyiMolpjlFLm2PZw4BWlVCpGidtJWutipdQ84DuML9O7vdf+CXjbfm2Vtr7yBN1Hot2z1vorpVQ/YDG++9lMYt/334Bx3vtJxfj7vpTEvme7k/p7Xcq1IcniGiGEiCOxNjwihBCiFBK0hRAijkjQFkKIOCJBWwgh4ogEbSGEiCMStIUQIo5I0BZCiDgiQVsIIeLI/wPggBEJq97jHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1095f4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fommy so'll.\n",
      "\n",
      "KUCKeIAL:\n",
      "N aawis s Ioke hinc' simk outung jazitcertilo, in, tallsnot mony oins hir on the wimyt.\n",
      "\n",
      "KAnG CAL:\n",
      "Thoush oud mave\n",
      "corly aperele thea comind youldesb grecurpt\n",
      "Teecemsir toub? \n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.01, sequence_length=25, num_layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.train(10000, sample_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

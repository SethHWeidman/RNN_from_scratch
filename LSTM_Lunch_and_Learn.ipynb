{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lunch and Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your current understanding of RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_next_character.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_unrolling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is actually going on?\n",
    "\n",
    "### Example with sequence length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the backwards pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_in_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep learning?\n",
    "\n",
    "### What would an RNN with multiple layers look like?\n",
    "\n",
    "![](img/rnn_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the backward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_olah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Forward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Backward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    '''\n",
    "    A class that holds weight matrices along with their derivatives and momentum.\n",
    "    '''\n",
    "    def __init__(self, value):\n",
    "        '''\n",
    "        param value: numpy array, two dimensional, shape of the weight matrix\n",
    "        '''\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        '''\n",
    "        Resets the value of the derivative\n",
    "        '''\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        '''\n",
    "        Clips the derivative, setting its min value to -1 and its max value to 1.\n",
    "        '''\n",
    "        self.deriv = np.clip(self.deriv, -1, 1, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to AdaGrad rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to stochastic gradient descent rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Params` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        The parameters to be used when updating the values in an LSTM_Layer, which is a layer\n",
    "        of LSTM cells stretched out over time.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the layer. This can be \n",
    "        different for each layer.\n",
    "        param vocab_size: int - the number of characters in the vocabulary that we are predicting\n",
    "        the next character of.\n",
    "        Note: the shape of these weight matrices assumes that the data will be fed in as \"rows\", \n",
    "        meaning that each data point will be represented by a numpy array of shape (1, vocab_size)\n",
    "        '''\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        '''\n",
    "        Biases always have the dimensions of the output of the transformation that they are being added to.\n",
    "        '''\n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        '''\n",
    "        Returns a list of all the parameters for easy iteration\n",
    "        '''\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        '''\n",
    "        Clears all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        '''\n",
    "        Clips all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        '''\n",
    "        Updates all the parameters according to the \"AdaGrad\" rule.\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node performing the role of the \"circles\" in the diagrams above.\n",
    "    It takes has two methods:\n",
    "    1. \"forward\" which takes in: \n",
    "        * An observation, \"X\"\n",
    "        * A hidden state from the prior time step, \"H_prev\"\n",
    "        * A cell state from the prior time step, \"C_prev\"\n",
    "        And returns:\n",
    "        * The output \"X_out\" to be passed on to the next layer.\n",
    "        * The output \"H\" to be passed into the next node as the hidden state\n",
    "        * The output \"C\" to be passed into the next node as the cell state\n",
    "    2. \"backward\" which takes in: \n",
    "        * The gradient of the output with respect to the loss, \"out_grad\"\n",
    "        * The gradient of the hidden state with respect to the loss, \"h_grad\"\n",
    "        * The gradient of the cell state with respect to the loss, \"c_grad\"\n",
    "        And returns:\n",
    "        * The gradient of the output of the same time step in the prior layer - the circle \"below\" in the \n",
    "        drawings above - \"dx_grad\"\n",
    "        * The gradient of the hidden state from the prior time step, \"dh_grad\" \n",
    "        * The gradient of the cell state from the prior time step, \"dc_grad\" \n",
    "    \n",
    "    It uses as parameters, not its own parameters, but the parameters from the LSTM_Layer that it is a part of.\n",
    "        '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, H_prev, C_prev, LSTM_Params):\n",
    "        '''\n",
    "        param x: numpy array of shape (1, vocab_size)\n",
    "        param H_prev: numpy array of shape (1, hidden_size)\n",
    "        param C_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.x_out: numpy array of shape (1, vocab_size)\n",
    "        return self.H: numpy array of shape (1, hidden_size)\n",
    "        return self.C: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, H_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.x_out = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.x_out, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    '''\n",
    "    Corresponds to a \"row\" of circles in the diagrams above. \n",
    "    We initialize a layer with a number of \"Nodes\" equal to the length of the sequences we passin.\n",
    "    Again, the key methods are \"forward\" and \"backward\".\n",
    "    1. \"forward\":\n",
    "        * Passes data forward through all the nodes in the layer\n",
    "        * Sets the hidden and cell states outputted by the last node to be those that will be passed in when the\n",
    "        next sequence is fed through the layer\n",
    "        * Passes the appropriate data on to the next layer in the network\n",
    "    2. \"backward\":\n",
    "        * Passes gradient backward through all the nodes in the layer\n",
    "        * Sets the hidden state outputted by the last node equal to the node \n",
    "        * Passes the appropriate gradient on to the previous layer in the network\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_seq_in):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_seq_in.shape[0]\n",
    "        \n",
    "        x_seq_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "            x_in = np.array(x_seq_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "            # Backward pass\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, num_layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "         '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        x_batch = self._generate_one_hot_array(inputs)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "\n",
    "    def _generate_one_hot_array(self, sequence):\n",
    "        '''\n",
    "        param sequence - sequence of indices of characters of length sequence_length.\n",
    "        return batch - numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.vocab_size))    \n",
    "        return batch\n",
    "        \n",
    "    def loss(self, x_batch_out, targets):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, inputs, targets):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        y_batch = np.zeros(self.sequence_length, self.vocab_size))\n",
    "\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        x_batch_out = self.forward(inputs)\n",
    "        \n",
    "        x_softmax = softmax(x_batch_out)\n",
    "        \n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "        \n",
    "        loss = self.loss(x_softmax, inputs, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_softmax, inputs, y_batch)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Character_generator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        '''\n",
    "        param text_file: path to text file containing characters that we are going to generate text to mimic\n",
    "        param LSTM_Model: the LSTM_Model that we are using to generate the text \n",
    "        '''\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        '''\n",
    "        Given a start position, generates a list of inputs and targets.\n",
    "        param start_pos: int - index of start position in characters \n",
    "        return inputs: list - list of length sequence_length with the indices of the characters forming the \n",
    "        inputs to the model \n",
    "        return targets: list - list of length sequence_length with the indices of the characters forming the \n",
    "        targets of the model\n",
    "        '''\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_out = sample_model.forward([input_char])\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_batch_out.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "    \n",
    "    \n",
    "    def single_step(self):\n",
    "        '''\n",
    "        Generates the data and runs the model for one step\n",
    "        return loss: float - mean squared error loss \n",
    "        '''   \n",
    "        inputs, targets = self._generate_inputs_targets(self.start_pos)\n",
    "        \n",
    "        loss = self.model.single_step(inputs, targets)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_loop(self, num_iterations):\n",
    "        \n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if self.start_pos + self.sequence_length > len(self.data):\n",
    "                self.start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            loss = self.single_step()\n",
    "            ##\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            self.start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[self.start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            self.start_pos += self.sequence_length\n",
    "            num_iter += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e0b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ess, and ,\n",
      "To grost\n",
      "This ose my,\n",
      "but t\n",
      "A all yout beglady that would gide.\n",
      "\n",
      "QUEEL USerave\n",
      "Nay and my lord?\n",
      "\n",
      "TANBALE:\n",
      "How lly lopes praise andreper guy would being offecour:\n",
      "Hease peppiors all the houl\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.1, sequence_length=25, layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.training_loop(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
